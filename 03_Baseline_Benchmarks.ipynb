{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V5E1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"2DVP51NGCHPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyAgp9t99gDY"},"outputs":[],"source":["# @title 1. Imports and Setup\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# Set seeds for reproducibility (Crucial for scientific reporting)\n","np.random.seed(42)\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","\n","print(\"Libraries loaded successfully.\")"]},{"cell_type":"code","source":["# @title 2. Load Data and Initial Preprocessing\n","import pandas as pd\n","import os\n","\n","# Define the data directory relative to this script\n","data_dir = './data'\n","# Update the filename to the .zip version\n","file_name = 'asv_interpretability_dataset_modified.zip'\n","file_path = os.path.join(data_dir, file_name)\n","\n","# Check if file exists\n","if not os.path.exists(file_path):\n","    raise FileNotFoundError(f\"Data file not found at {file_path}. Please ensure the 'data' folder contains the zipped dataset.\")\n","\n","# Load the data directly from the zip file\n","# Pandas automatically detects the zip compression\n","df = pd.read_csv(file_path, dtype={'PatientID': str})\n","print(f\"Successfully loaded data from {file_path}\")\n","\n","# --- Helper: Handle NeutrophilCount with '<0.1' values ---\n","def parse_neutrophil(value):\n","    try:\n","        return float(value)\n","    except:\n","        if isinstance(value, str) and \"<\" in value:\n","            threshold = float(value.replace(\"<\", \"\").strip())\n","            return threshold / 2\n","        return np.nan\n","\n","# --- Helper: Create Proxy Labels (Clinical Dysbiosis) ---\n","def label_dysbiosis(row):\n","    # Proxy definition: High Temp + Low Neutrophils + Liquid Stool\n","    is_temp_abnormal = row['MaxTemperature'] > 38.0\n","    is_neutro_low = row['NeutrophilCount'] < 500\n","    is_consistency_liquid = row.get('Consistency_liquid', 0) == 1\n","    return int(is_temp_abnormal and is_neutro_low and is_consistency_liquid)\n","\n","# 1. Clean Neutrophils\n","df['NeutrophilCount'] = df['NeutrophilCount'].apply(parse_neutrophil)\n","# Impute missing neutrophils with median (optional, depends on your specific logic)\n","# df['NeutrophilCount'].fillna(df['NeutrophilCount'].median(), inplace=True)\n","\n","# 2. One-hot encode stool consistency\n","df = pd.get_dummies(df, columns=['Consistency'])\n","\n","# 3. Log transform Genus-relative abundances (Compositional handling)\n","# Ensure no negative values or zeros break the log\n","df['RelativeAbundance'] = df['RelativeAbundance'].astype(float)\n","df['RelativeAbundance'] = np.log1p(df['RelativeAbundance'])\n","\n","# 4. Generate Labels (Row-wise)\n","df['DysbiosisLabel'] = df.apply(label_dysbiosis, axis=1)\n","\n","# 5. Pivot to Wide Format (Time Series Format)\n","# Keep metadata\n","metadata_cols = ['PatientID', 'SampleID', 'DayRelativeToNearestHCT',\n","                 'MaxTemperature', 'NeutrophilCount'] + \\\n","                [col for col in df.columns if col.startswith('Consistency_')] + \\\n","                ['DysbiosisLabel']\n","\n","# Pivot Genus\n","genus_pivot = df.pivot_table(index=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'],\n","                             columns='Genus', values='RelativeAbundance', fill_value=0).reset_index()\n","\n","# Merge Metadata back\n","metadata = df[metadata_cols].drop_duplicates(subset=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'])\n","merged_df = pd.merge(genus_pivot, metadata, on=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'], how='left')\n","\n","print(f\"Data Processed. Total Samples: {len(merged_df)}\")\n","print(f\"Total Unique Patients: {merged_df['PatientID'].nunique()}\")"],"metadata":{"id":"aqakxWWHBhKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Patient-Level Splitting & Scaling (PREVENT DATA LEAKAGE)\n","\n","# --- STEP A: Split Patients First ---\n","# We split the Patient IDs, NOT the sequences.\n","unique_patients = merged_df['PatientID'].unique()\n","np.random.shuffle(unique_patients) # Randomize patient order\n","\n","n_total = len(unique_patients)\n","n_train = int(0.70 * n_total)\n","n_val = int(0.15 * n_total)\n","\n","train_pids = unique_patients[:n_train]\n","val_pids = unique_patients[n_train : n_train + n_val]\n","test_pids = unique_patients[n_train + n_val:]\n","\n","print(f\"Patients in Train: {len(train_pids)}\")\n","print(f\"Patients in Val:   {len(val_pids)}\")\n","print(f\"Patients in Test:  {len(test_pids)}\")\n","\n","# Create separate DataFrames based on Patient ID\n","df_train = merged_df[merged_df['PatientID'].isin(train_pids)].copy()\n","df_val = merged_df[merged_df['PatientID'].isin(val_pids)].copy()\n","df_test = merged_df[merged_df['PatientID'].isin(test_pids)].copy()\n","\n","# --- STEP B: Feature Selection (Train Only) ---\n","# Identify genus columns\n","all_genus_cols = genus_pivot.columns.drop(['PatientID', 'SampleID', 'DayRelativeToNearestHCT']).tolist()\n","\n","# Calculate variance ONLY on Training data to avoid leakage\n","train_variances = df_train[all_genus_cols].var()\n","# Drop columns with near-zero variance in training set\n","non_zero_var_cols = train_variances[train_variances > 1e-6].index.tolist()\n","\n","# Define final feature list (Microbiome + Stool Consistency)\n","# Exclude Temp/Neutrophils from Input X (as they define the label Y)\n","\n","#Changed on 02 December, 2025\n","# feature_cols = non_zero_var_cols + [col for col in merged_df.columns if 'Consistency' in col]\n","\n","# === CRITICAL FIX ===\n","# Input Features = Microbiome ONLY.\n","# We REMOVE 'Consistency' because it is part of the Label definition (Leakage).\n","feature_cols = non_zero_var_cols\n","# feature_cols += [col for col in merged_df.columns if 'Consistency' in col] <--- REMOVED THIS LINE\n","\n","print(f\"Selected {len(feature_cols)} features (Microbiome Genus Only).\")\n","\n","#print(f\"Selected {len(feature_cols)} features based on Training Set variance.\")\n","\n","# --- STEP C: Scaling (Fit on Train Only) ---\n","scaler = MinMaxScaler()\n","\n","# 1. FIT and TRANSFORM on Training\n","df_train[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n","\n","# 2. TRANSFORM Only on Val/Test (using Train statistics)\n","df_val[feature_cols] = scaler.transform(df_val[feature_cols])\n","df_test[feature_cols] = scaler.transform(df_test[feature_cols])\n","\n","print(\"Scaling complete. Data leakage prevented.\")"],"metadata":{"id":"cCJu8UgOBkh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Sequence Generation (Sliding Window)\n","\n","def build_sequences(df, feature_cols, label_col='DysbiosisLabel', seq_len=14):\n","    \"\"\"\n","    Generates sequences strictly within patient groups.\n","    \"\"\"\n","    X_sequences = []\n","    y_labels = []\n","\n","    # Group by patient to ensure window never crosses patient boundaries\n","    for pid, group in df.groupby('PatientID'):\n","        # Sort by time\n","        group = group.sort_values('DayRelativeToNearestHCT')\n","\n","        values = group[feature_cols].values\n","        labels = group[label_col].values\n","\n","        # Sliding window\n","        if len(values) >= seq_len:\n","            for i in range(len(values) - seq_len + 1):\n","                seq = values[i:i+seq_len]\n","                label_window = labels[i:i+seq_len]\n","\n","                # Label Logic: If ANY point in window is dysbiotic, label=1\n","                # (Or use label_window[-1] for \"next step prediction\")\n","                label = int(label_window.max())\n","\n","                X_sequences.append(seq)\n","                y_labels.append(label)\n","\n","    return np.array(X_sequences), np.array(y_labels)\n","\n","# Build sequences for each split independently\n","SEQ_LEN = 14\n","\n","X_train, y_train = build_sequences(df_train, feature_cols, seq_len=SEQ_LEN)\n","X_val, y_val = build_sequences(df_val, feature_cols, seq_len=SEQ_LEN)\n","X_test, y_test = build_sequences(df_test, feature_cols, seq_len=SEQ_LEN)\n","\n","print(f\"Training Sequences: {X_train.shape}\")\n","print(f\"Validation Sequences: {X_val.shape}\")\n","print(f\"Testing Sequences: {X_test.shape}\")"],"metadata":{"id":"dVlZAmPBBnYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import os\n","\n","# Define the path relative to the 'models' folder\n","model_path = \"./models/DynaBiome_PatientSplit_Model.keras\"\n","\n","# Check if file exists (Good practice for public code)\n","if not os.path.exists(model_path):\n","    raise FileNotFoundError(f\"Model file not found at {model_path}. Did you upload it to the 'models' folder?\")\n","\n","# Load the trained model\n","autoencoder = tf.keras.models.load_model(model_path)\n","print(f\"Model loaded successfully from {model_path}\")"],"metadata":{"id":"yWFoaU0tgcva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 6. Generate Reconstruction Errors (Features)\n","\n","import numpy as np\n","\n","def get_mae_features(model, X_data):\n","    \"\"\"\n","    Passes data through the frozen Autoencoder and calculates MAE.\n","    Returns a vector of shape (n_samples, ).\n","    \"\"\"\n","    # 1. Reconstruct sequences\n","    reconstructions = model.predict(X_data, verbose=0)\n","\n","    # 2. Calculate Mean Absolute Error (averaged over time and features)\n","    # Axis 1 = Timesteps, Axis 2 = Features\n","    mae_loss = np.mean(np.abs(X_data - reconstructions), axis=(1, 2))\n","    return mae_loss\n","\n","print(\"Generating features using the frozen LSTM Autoencoder...\")\n","\n","# 1. Generate Errors for the TRAINING set (Both Normal and Dysbiotic)\n","# [cite_start]We use this to TRAIN the downstream classifiers [cite: 1, 2, 3]\n","train_mae = get_mae_features(autoencoder, X_train)\n","\n","# 2. Generate Errors for the VALIDATION set\n","# [cite_start]We use this to TUNE thresholds (Youden's Index) [cite: 1, 2, 3]\n","val_mae = get_mae_features(autoencoder, X_val)\n","\n","# 3. Generate Errors for the TEST set\n","# [cite_start]We use this ONLY for final reporting [cite: 1, 2, 3]\n","test_mae = get_mae_features(autoencoder, X_test)\n","\n","# Reshape for Scikit-Learn (Must be 2D array: n_samples x n_features)\n","X_train_feat = train_mae.reshape(-1, 1)\n","X_val_feat   = val_mae.reshape(-1, 1)\n","X_test_feat  = test_mae.reshape(-1, 1)\n","\n","print(f\"Feature Extraction Complete.\")\n","print(f\"Training Features Shape:   {X_train_feat.shape} (Labels: {len(y_train)})\")\n","print(f\"Validation Features Shape: {X_val_feat.shape}   (Labels: {len(y_val)})\")\n","print(f\"Test Features Shape:       {X_test_feat.shape}  (Labels: {len(y_test)})\")"],"metadata":{"id":"rFC29E5Ghw-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install statsmodels\n","!pip install xgboost"],"metadata":{"id":"Filwa_ruUzZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import OneClassSVM\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, average_precision_score, accuracy_score\n","import numpy as np\n","from sklearn.metrics import precision_recall_curve # Added for ensemble F1 tuning\n","\n","# --- 1. DATA PREPARATION ---\n","# Ensure features are 2D arrays (N_samples, 1)\n","X_train_feat = train_mae.reshape(-1, 1)\n","X_val_feat   = val_mae.reshape(-1, 1)\n","X_test_feat  = test_mae.reshape(-1, 1)\n","\n","print(f\"Training Data Shape: {X_train_feat.shape}\")\n","\n","# Define list of ensemble names for later iteration\n","ensemble_names_list = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","\n","# --- 2. DEFINE PARAMETER GRIDS ---\n","param_grids = {\n","    \"Logistic Regression\": {\n","        'model': LogisticRegression(random_state=42, solver='liblinear'),\n","        'params': {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n","    },\n","    \"KNN\": {\n","        'model': KNeighborsClassifier(),\n","        'params': {'n_neighbors': [5, 20, 50], 'weights': ['uniform', 'distance']}\n","    },\n","    \"Random Forest\": {\n","        'model': RandomForestClassifier(random_state=42),\n","        'params': {'n_estimators': [100, 200], 'max_depth': [3, 5, 10], 'min_samples_split': [5, 10]}\n","    },\n","    \"XGBoost\": {\n","        'model': XGBClassifier(eval_metric='logloss', random_state=42),\n","        'params': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}\n","    },\n","    \"MLP\": {\n","        'model': MLPClassifier(max_iter=500, random_state=42),\n","        'params': {'hidden_layer_sizes': [(32, 16), (64, 32)], 'activation': ['relu'], 'alpha': [0.001, 0.01]}\n","    }\n","}\n","\n","best_models = {}\n","\n","print(\"--- STARTING HYPERPARAMETER TUNING (GridSearchCV) ---\")\n","print(\"Note: Tuning is performed on Training Set with 5-Fold CV.\\n\")\n","\n","# --- 3. SUPERVISED MODELS LOOP ---\n","for name, config in param_grids.items():\n","    print(f\"Tuning {name}...\")\n","    grid = GridSearchCV(estimator=config['model'], param_grid=config['params'], cv=5, scoring='roc_auc', n_jobs=-1)\n","    grid.fit(X_train_feat, y_train)\n","    best_models[name] = grid.best_estimator_\n","    print(f\"  Best CV AUC: {grid.best_score_:.4f}\")\n","\n","# --- 4. ONE-CLASS SVM SPECIAL HANDLING ---\n","print(\"\\nTuning One-Class SVM (Special Unsupervised Handling)...\")\n","best_ocsvm = None\n","best_ocsvm_f1 = -1\n","X_train_normal_feat = X_train_feat[y_train == 0]\n","\n","for nu in [0.01, 0.05, 0.1, 0.2, 0.3]:\n","    ocsvm = OneClassSVM(kernel='rbf', gamma='scale', nu=nu)\n","    ocsvm.fit(X_train_normal_feat)\n","    preds = ocsvm.predict(X_val_feat)\n","    binary_preds = np.where(preds == -1, 1, 0)\n","    f1 = f1_score(y_val, binary_preds)\n","    if f1 > best_ocsvm_f1:\n","        best_ocsvm_f1 = f1\n","        best_ocsvm = ocsvm\n","\n","best_models[\"One-Class SVM\"] = best_ocsvm\n","print(f\"  Best OCSVM Val F1: {best_ocsvm_f1:.4f}\")\n","\n","# --- 5. COLLECT PREDICTIONS FOR ENSEMBLING & TUNING THRESHOLDS ---\n","# We need Validation Predictions (to train Meta-Learners and tune F1 thresholds)\n","# and Test Predictions (for Final Evaluation)\n","val_probs_dict = {}\n","test_probs_dict = {}\n","val_aucs = {} # For Weighted Ensemble\n","best_thresholds = {}\n","\n","print(\"\\n--- Generating Predictions for Ensembling & Tuning Threshholds ---\")\n","\n","for name, clf in best_models.items():\n","    if name == \"One-Class SVM\": continue # OCSVM excluded from standard ensembles for now\n","\n","    # Predict on Validation\n","    val_probs = clf.predict_proba(X_val_feat)[:, 1]\n","    val_probs_dict[name] = val_probs\n","\n","    # Store Val AUC for weighting\n","    val_aucs[name] = roc_auc_score(y_val, val_probs)\n","\n","    # Threshold Tuning for Individual Models (Youden's Index for ROC)\n","    fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n","    optimal_idx = np.argmax(tpr - fpr)\n","    # Ensure optimal_idx is within bounds of thresholds array\n","    if optimal_idx < len(thresholds):\n","        best_thresholds[name] = thresholds[optimal_idx]\n","    else:\n","        best_thresholds[name] = 0.5 # Default if threshold array is empty/problematic\n","\n","    # Predict on Test\n","    test_probs = clf.predict_proba(X_test_feat)[:, 1]\n","    test_probs_dict[name] = test_probs\n","\n","# ==========================================================\n","# --- 6. ADVANCED ENSEMBLE METHODS (CALCULATE PROBABILITIES) ---\n","# ==========================================================\n","\n","# A. Averaged Ensemble - Calculate Test Probs\n","avg_probs = np.mean(list(test_probs_dict.values()), axis=0)\n","test_probs_dict['Averaged Ensemble'] = avg_probs # Store test probs\n","\n","# B. Weighted Ensemble - Calculate Test Probs\n","total_auc = sum(val_aucs.values())\n","weights = {k: v / total_auc for k, v in val_aucs.items()} # Ensure weights sum to 1\n","weighted_probs = np.zeros_like(avg_probs)\n","for name, prob in test_probs_dict.items():\n","    if name in weights: # Ensure we only use models that contributed to weights\n","        weighted_probs += prob * weights[name]\n","test_probs_dict['Weighted Ensemble'] = weighted_probs # Store test probs\n","\n","\n","# Prepare Data for Stacking Meta-Learners\n","X_meta_train_stack = np.column_stack(list(val_probs_dict.values())) # Base model predictions on validation set\n","y_meta_train_stack = y_val\n","X_meta_test_stack = np.column_stack(list(test_probs_dict.values())[:-2]) # Base model predictions on test set (excluding current ensembles)\n","\n","# C. Stacked Ensemble (Logistic Regression Meta) - Train & Calc Test Probs\n","meta_lr = LogisticRegression(random_state=42)\n","meta_lr.fit(X_meta_train_stack, y_meta_train_stack)\n","stack_lr_probs = meta_lr.predict_proba(X_meta_test_stack)[:, 1]\n","test_probs_dict['Stacked (LR)'] = stack_lr_probs # Store test probs\n","\n","# D. Stacked Ensemble (XGBoost Meta) - Train & Calc Test Probs\n","meta_xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n","meta_xgb.fit(X_meta_train_stack, y_meta_train_stack)\n","stack_xgb_probs = meta_xgb.predict_proba(X_meta_test_stack)[:, 1]\n","test_probs_dict['Stacked (XGB)'] = stack_xgb_probs # Store test probs\n","\n","# --- E. TUNE ENSEMBLE THRESHOLDS (MAX F1 on Validation Set) ---\n","print(\"\\n--- Tuning Ensemble Thresholds for F1-score (on Validation Set) ---\")\n","\n","# Calculate validation probabilities for ensembles for tuning\n","val_avg_probs = np.mean(list(val_probs_dict.values()), axis=0)\n","val_weighted_probs = np.zeros_like(val_avg_probs)\n","for name, prob in val_probs_dict.items(): # Use val_probs_dict for averaging\n","    if name in weights: # Ensure weights are applied correctly\n","        val_weighted_probs += prob * weights[name]\n","\n","# Stacked meta-learners already trained on X_meta_train_stack (validation predictions)\n","val_stack_lr_probs = meta_lr.predict_proba(X_meta_train_stack)[:, 1]\n","val_stack_xgb_probs = meta_xgb.predict_proba(X_meta_train_stack)[:, 1]\n","\n","ensemble_val_probs_for_tuning = {\n","    'Averaged Ensemble': val_avg_probs,\n","    'Weighted Ensemble': val_weighted_probs,\n","    'Stacked (LR)': val_stack_lr_probs,\n","    'Stacked (XGB)': val_stack_xgb_probs\n","}\n","\n","for name in ensemble_names_list:\n","    probs = ensemble_val_probs_for_tuning[name]\n","    precision, recall, thresholds = precision_recall_curve(y_val, probs)\n","    # Handle division by zero for fscore calculation, especially for cases with no positive predictions\n","    fscore = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(precision), where=(precision + recall != 0))\n","\n","    # Check if fscore array is empty or contains only NaNs (e.g., if no positive predictions or recall=0)\n","    if fscore.size > 0 and not np.all(np.isnan(fscore)):\n","        best_thresh_idx = np.nanargmax(fscore) # Use nanargmax to ignore NaNs\n","        # Ensure best_thresh_idx is within thresholds array bounds\n","        if best_thresh_idx < len(thresholds):\n","            best_thresholds[name] = thresholds[best_thresh_idx]\n","        else:\n","            best_thresholds[name] = 0.5 # Default if index is out of bounds\n","    else:\n","        best_thresholds[name] = 0.5 # Default threshold if no meaningful F1 can be calculated\n","    print(f\"  {name:<20}: Optimal F1 Threshold = {best_thresholds[name]:.4f}\")\n","\n","\n","# ==========================================================\n","# --- FINAL RESULTS TABLE (Individual Models + Ensembles) ---\n","# ==========================================================\n","print(\"\\n--- INDIVIDUAL & ENSEMBLE MODEL RESULTS ---\")\n","print(f\"{'Model':<20} | {'ROC AUC':<10} | {'PR AUC':<10} | {'F1-Score':<10} | {'Accuracy':<10} | {'W-F1':<10} | {'M-F1':<10} | {'Optimal Threshold':<19}\") # Updated header\n","print(\"-\" * 118)\n","\n","# Individual Models\n","for name in best_models.keys():\n","    optimal_thresh_str = \"N/A\"\n","    if name == \"One-Class SVM\":\n","        # OCSVM Logic\n","        scores = -best_models[name].decision_function(X_test_feat)\n","        test_preds = np.where(best_models[name].predict(X_test_feat) == -1, 1, 0)\n","        roc = roc_auc_score(y_test, scores)\n","        pr = average_precision_score(y_test, scores)\n","        f1 = f1_score(y_test, test_preds)\n","        acc = accuracy_score(y_test, test_preds)\n","        f1_w = f1_score(y_test, test_preds, average='weighted')\n","        f1_m = f1_score(y_test, test_preds, average='macro')\n","    else:\n","        probs = test_probs_dict[name]\n","        # Apply the tuned threshold to get binary predictions for F1, Accuracy\n","        thresh = best_thresholds.get(name, 0.5)\n","        binary_preds = (probs >= thresh).astype(int)\n","        optimal_thresh_str = f\"{thresh:.4f}\"\n","\n","        roc = roc_auc_score(y_test, probs)\n","        pr = average_precision_score(y_test, probs)\n","        f1 = f1_score(y_test, binary_preds)\n","        acc = accuracy_score(y_test, binary_preds)\n","        f1_w = f1_score(y_test, binary_preds, average='weighted')\n","        f1_m = f1_score(y_test, binary_preds, average='macro')\n","\n","    print(f\"{name:<20} | {roc:.4f}     | {pr:.4f}     | {f1:.4f}     | {acc:.4f}     | {f1_w:.4f}     | {f1_m:.4f}     | {optimal_thresh_str:<19}\")\n","\n","# Ensembles\n","for name in ensemble_names_list:\n","    probs = test_probs_dict[name]\n","    # Apply the tuned F1 threshold for ensembles to get binary predictions\n","    thresh = best_thresholds.get(name, 0.5)\n","    binary_preds = (probs >= thresh).astype(int)\n","    optimal_thresh_str = f\"{thresh:.4f}\"\n","\n","    roc = roc_auc_score(y_test, probs)\n","    pr = average_precision_score(y_test, probs)\n","    f1 = f1_score(y_test, binary_preds)\n","    acc = accuracy_score(y_test, binary_preds)\n","    f1_w = f1_score(y_test, binary_preds, average='weighted')\n","    f1_m = f1_score(y_test, binary_preds, average='macro')\n","\n","    print(f\"{name:<20} | {roc:.4f}     | {pr:.4f}     | {f1:.4f}     | {acc:.4f}     | {f1_w:.4f}     | {f1_m:.4f}     | {optimal_thresh_str:<19}\")\n","\n","print(\"-\" * 118)"],"metadata":{"id":"TG4lfZc7R8g6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 9. Generate Final ROC & PR Curves (All Models + Advanced Ensembles)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n","\n","# Setup Plot (1 Row, 2 Columns)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9)) # Slightly larger figure for better spacing\n","plt.rcParams.update({'font.size': 14, 'font.family': 'serif'}) # Increased overall font size\n","\n","# --- DEFINE COLORS & STYLES ---\n","colors = {\n","    'Logistic Regression': '#1f77b4',  # Blue\n","    'Random Forest':       '#ff7f0e',  # Orange\n","    'XGBoost':             '#2ca02c',  # Green\n","    'MLP':                 '#d62728',  # Red\n","    'KNN':                 '#9467bd',  # Purple\n","    'One-Class SVM':       '#e377c2',  # Pink\n","    'Averaged Ensemble':   'black',\n","    'Weighted Ensemble':   '#8c564b',  # Brown\n","    'Stacked (LR)':        '#7f7f7f',  # Gray\n","    'Stacked (XGB)':       '#bcbd22'   # Olive\n","}\n","\n","# Styles for differentiating Ensembles\n","styles = {\n","    'Averaged Ensemble': '-',\n","    'Weighted Ensemble': '--',\n","    'Stacked (LR)':      '-.',\n","    'Stacked (XGB)':     ':'\n","}\n","\n","# ==========================================\n","# PLOT 1: ROC CURVES (Left Panel)\n","# ==========================================\n","\n","# 1. Baseline (LSTM-AE)\n","fpr_ae, tpr_ae, _ = roc_curve(y_test, test_mae)\n","auc_ae = roc_auc_score(y_test, test_mae)\n","ax1.plot(fpr_ae, tpr_ae, label=f'LSTM-AE Baseline (AUC = {auc_ae:.3f})',\n","         linestyle='--', color='gray', linewidth=2.5, alpha=0.7) # Increased linewidth\n","\n","# 2. Individual Tuned Classifiers\n","for name, clf in best_models.items():\n","    if name == \"One-Class SVM\":\n","        # OCSVM Logic\n","        scores = -clf.decision_function(X_test_feat)\n","        fpr, tpr, _ = roc_curve(y_test, scores)\n","        roc_val = roc_auc_score(y_test, scores)\n","        ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_val:.3f})',\n","                 color=colors.get(name, 'black'), linewidth=1.5, linestyle=':', alpha=0.6) # Increased linewidth\n","        continue\n","\n","    # Standard Models\n","    probs = clf.predict_proba(X_test_feat)[:, 1]\n","    fpr, tpr, _ = roc_curve(y_test, probs)\n","    roc_val = roc_auc_score(y_test, probs)\n","\n","    # Highlight RF/XGB slightly\n","    lw = 2.5 if name in ['Random Forest', 'XGBoost'] else 1.5 # Increased linewidth\n","    ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_val:.3f})',\n","             color=colors.get(name, 'black'), linewidth=lw, alpha=0.8)\n","\n","# 3. Advanced Ensembles (From test_probs_dict)\n","ensemble_names = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","\n","for name in ensemble_names:\n","    if name in test_probs_dict:\n","        probs = test_probs_dict[name]\n","        fpr, tpr, _ = roc_curve(y_test, probs)\n","        roc_val = roc_auc_score(y_test, probs)\n","\n","        # Make Averaged Ensemble thickest\n","        lw = 4 if name == 'Averaged Ensemble' else 3 # Increased linewidth\n","        ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_val:.3f})',\n","                 color=colors.get(name, 'black'), linestyle=styles.get(name, '-'),\n","                 linewidth=lw, zorder=10)\n","\n","# Formatting ROC\n","ax1.plot([0, 1], [0, 1], 'k:', alpha=0.4)\n","ax1.set_xlim([0.0, 1.0])\n","ax1.set_ylim([0.0, 1.02])\n","ax1.set_xlabel('False Positive Rate (1 - Specificity)', fontweight='bold')\n","ax1.set_ylabel('True Positive Rate (Sensitivity)', fontweight='bold')\n","ax1.set_title('Receiver Operating Characteristic (ROC)', fontweight='bold', pad=10)\n","ax1.legend(loc=\"lower right\", fontsize=11, ncol=1) # Increased legend fontsize\n","ax1.grid(True, alpha=0.3)\n","\n","\n","# ==========================================\n","# PLOT 2: PRECISION-RECALL CURVES (Right Panel)\n","# ==========================================\n","\n","# 1. Baseline\n","precision_ae, recall_ae, _ = precision_recall_curve(y_test, test_mae)\n","pr_auc_ae = average_precision_score(y_test, test_mae)\n","ax2.plot(recall_ae, precision_ae, label=f'LSTM-AE Baseline (AP = {pr_auc_ae:.3f})',\n","         linestyle='--', color='gray', linewidth=2.5, alpha=0.7) # Increased linewidth\n","\n","# 2. Individual Classifiers\n","for name, clf in best_models.items():\n","    if name == \"One-Class SVM\":\n","        scores = -clf.decision_function(X_test_feat)\n","        precision, recall, _ = precision_recall_curve(y_test, scores)\n","        pr_val = average_precision_score(y_test, scores)\n","        ax2.plot(recall, precision, label=f'{name} (AP = {pr_val:.3f})',\n","                 color=colors.get(name, 'black'), linewidth=1.5, linestyle=':', alpha=0.6) # Increased linewidth\n","        continue\n","\n","    probs = clf.predict_proba(X_test_feat)[:, 1]\n","    precision, recall, _ = precision_recall_curve(y_test, probs)\n","    pr_val = average_precision_score(y_test, probs)\n","\n","    lw = 2.5 if name in ['Random Forest', 'XGBoost'] else 1.5 # Increased linewidth\n","    ax2.plot(recall, precision, label=f'{name} (AP = {pr_val:.3f})',\n","             color=colors.get(name, 'black'), linewidth=lw, alpha=0.8)\n","\n","# 3. Advanced Ensembles\n","for name in ensemble_names:\n","    if name in test_probs_dict:\n","        probs = test_probs_dict[name]\n","        precision, recall, _ = precision_recall_curve(y_test, probs)\n","        pr_val = average_precision_score(y_test, probs)\n","\n","        lw = 4 if name == 'Averaged Ensemble' else 3 # Increased linewidth\n","        ax2.plot(recall, precision, label=f'{name} (AP = {pr_val:.3f})',\n","                 color=colors.get(name, 'black'), linestyle=styles.get(name, '-'),\n","                 linewidth=lw, zorder=10)\n","\n","# Formatting PR\n","ax2.set_xlim([0.0, 1.0])\n","ax2.set_ylim([0.0, 1.02])\n","ax2.set_xlabel('Recall (Sensitivity)', fontweight='bold')\n","ax2.set_ylabel('Precision (Positive Predictive Value)', fontweight='bold')\n","ax2.set_title('Precision-Recall (PR) Curves', fontweight='bold', pad=10)\n","ax2.legend(loc=\"lower left\", fontsize=11, ncol=1) # Increased legend fontsize\n","ax2.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('Figure3b_Complete_Ensemble_Analysis.pdf', dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"JQWRPjY9qTi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 16. Benchmark Evaluation: Isolation Forest & LOF (Unsupervised)\n","from sklearn.ensemble import IsolationForest\n","from sklearn.neighbors import LocalOutlierFactor\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","import numpy as np\n","\n","print(\"--- RE-EVALUATING UNSUPERVISED BENCHMARKS (On Patient-Split Data) ---\")\n","\n","# 1. PREPARE DATA (Flatten Time Dimension)\n","# LSTM uses (N, 14, Features). IF/LOF need (N, 14*Features).\n","n_samples_train, n_steps, n_feats = X_train.shape\n","n_samples_test = X_test.shape[0]\n","\n","# Flatten: Each sample becomes a long vector of 14*Features\n","X_train_flat = X_train.reshape(n_samples_train, n_steps * n_feats)\n","X_test_flat = X_test.reshape(n_samples_test, n_steps * n_feats)\n","\n","# Filter Normal Training Data for Isolation Forest (Standard Anomaly Detection setup)\n","X_train_flat_normal = X_train_flat[y_train == 0]\n","\n","print(f\"Flattened Train Shape: {X_train_flat.shape}\")\n","print(f\"Flattened Test Shape:  {X_test_flat.shape}\")\n","\n","# Store results\n","benchmark_results = []\n","\n","# ==========================================\n","# A. ISOLATION FOREST\n","# ==========================================\n","print(\"\\nRunning Isolation Forest...\")\n","iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42, n_jobs=-1)\n","\n","# Train on Normal Data Only\n","iso_forest.fit(X_train_flat_normal)\n","\n","# Predict (Returns -1 for outlier, 1 for inlier)\n","iso_scores = -iso_forest.decision_function(X_test_flat)\n","\n","# Calculate Metrics\n","iso_roc = roc_auc_score(y_test, iso_scores)\n","iso_pr = average_precision_score(y_test, iso_scores)\n","\n","# Map predictions: -1 -> 1 (Anomaly), 1 -> 0 (Normal)\n","iso_preds_raw = iso_forest.predict(X_test_flat)\n","iso_preds = np.where(iso_preds_raw == -1, 1, 0)\n","\n","# EXTENDED METRICS\n","iso_acc = accuracy_score(y_test, iso_preds)\n","iso_f1_macro = f1_score(y_test, iso_preds, average='macro')\n","iso_f1_weighted = f1_score(y_test, iso_preds, average='weighted')\n","\n","print(f\"Isolation Forest | ROC: {iso_roc:.4f} | PR: {iso_pr:.4f} | Acc: {iso_acc:.4f}\")\n","benchmark_results.append(['Isolation Forest', iso_acc, iso_f1_macro, iso_f1_weighted, iso_roc, iso_pr])\n","\n","\n","# ==========================================\n","# B. LOCAL OUTLIER FACTOR (LOF)\n","# ==========================================\n","print(\"Running Local Outlier Factor (LOF)...\")\n","lof = LocalOutlierFactor(n_neighbors=20, novelty=True, n_jobs=-1)\n","\n","# Train on Normal Data\n","lof.fit(X_train_flat_normal)\n","\n","# Predict\n","lof_scores = -lof.decision_function(X_test_flat)\n","\n","# Metrics\n","lof_roc = roc_auc_score(y_test, lof_scores)\n","lof_pr = average_precision_score(y_test, lof_scores)\n","\n","# Map predictions\n","lof_preds_raw = lof.predict(X_test_flat)\n","lof_preds = np.where(lof_preds_raw == -1, 1, 0)\n","\n","# EXTENDED METRICS\n","lof_acc = accuracy_score(y_test, lof_preds)\n","lof_f1_macro = f1_score(y_test, lof_preds, average='macro')\n","lof_f1_weighted = f1_score(y_test, lof_preds, average='weighted')\n","\n","print(f\"Local Outlier Factor | ROC: {lof_roc:.4f} | PR: {lof_pr:.4f} | Acc: {lof_acc:.4f}\")\n","benchmark_results.append(['Local Outlier Factor', lof_acc, lof_f1_macro, lof_f1_weighted, lof_roc, lof_pr])\n","\n","# ==========================================\n","# COMPARE WITH DYNABIOME (Stacked LR)\n","# ==========================================\n","try:\n","    # Retrieve scores and threshold\n","    dyna_roc = roc_auc_score(y_test, test_probs_dict['Stacked (LR)'])\n","    dyna_pr = average_precision_score(y_test, test_probs_dict['Stacked (LR)'])\n","\n","    dyna_thresh = best_thresholds['Stacked (LR)']\n","    dyna_preds = (test_probs_dict['Stacked (LR)'] >= dyna_thresh).astype(int)\n","\n","    # EXTENDED METRICS\n","    dyna_acc = accuracy_score(y_test, dyna_preds)\n","    dyna_f1_macro = f1_score(y_test, dyna_preds, average='macro')\n","    dyna_f1_weighted = f1_score(y_test, dyna_preds, average='weighted')\n","\n","    benchmark_results.append(['DynaBiome (Stacked LR)', dyna_acc, dyna_f1_macro, dyna_f1_weighted, dyna_roc, dyna_pr])\n","\n","except (NameError, KeyError) as e:\n","    print(f\"DynaBiome scores not found (skipping comparison row): {e}\")\n","\n","# Print Summary Table\n","df_bench = pd.DataFrame(benchmark_results, columns=['Model', 'Accuracy', 'F1 (Macro)', 'F1 (Weighted)', 'ROC AUC', 'PR AUC'])\n","print(\"\\n--- FINAL BENCHMARK COMPARISON ---\")\n","print(df_bench)"],"metadata":{"id":"xfxg6PiZNRc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 17. Plot Benchmark Comparison\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Data from your results\n","data = {\n","    'Model': ['Isolation Forest', 'LOF', 'DynaBiome (Proposed)'],\n","    'ROC AUC': [0.8555, 0.8407, 0.8908],\n","    'PR AUC':  [0.5877, 0.5710, 0.6479]\n","}\n","df_plot = pd.DataFrame(data)\n","\n","# Melt for Seaborn\n","df_melted = df_plot.melt(id_vars='Model', var_name='Metric', value_name='Score')\n","\n","# Plot\n","plt.figure(figsize=(10, 6))\n","sns.set_style(\"whitegrid\")\n","# Custom palette: Grey for baselines, Red for DynaBiome to make it pop\n","ax = sns.barplot(x='Metric', y='Score', hue='Model', data=df_melted,\n","                 palette=['#95a5a6', '#7f8c8d', '#d62728'])\n","\n","# Annotate bars\n","for container in ax.containers:\n","    ax.bar_label(container, fmt='%.3f', padding=3, fontsize=10)\n","\n","plt.title('Benchmarking Static vs. Temporal Anomaly Detection', fontsize=14, pad=20)\n","plt.ylim(0.5, 1.0)  # Zoom in to show differences\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.tight_layout()\n","plt.savefig('Figure_5_Benchmarks.pdf', dpi=600)\n","plt.show()"],"metadata":{"id":"6hT5H-zdQJVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 17. Generate Benchmark Comparison Figure (ROC & PR Curves)\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, precision_recall_curve, auc\n","\n","# Setup Plot\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n","plt.rcParams.update({'font.size': 11, 'font.family': 'serif'})\n","\n","# Colors & Styles\n","colors = {\n","    'Isolation Forest': '#7f7f7f',  # Gray\n","    'Local Outlier Factor': '#c7c7c7', # Light Gray\n","    'DynaBiome (Stacked LR)': 'black'   # Bold Black for your model\n","}\n","styles = {\n","    'Isolation Forest': '--',\n","    'Local Outlier Factor': ':',\n","    'DynaBiome (Stacked LR)': '-'\n","}\n","widths = {\n","    'Isolation Forest': 2,\n","    'Local Outlier Factor': 2,\n","    'DynaBiome (Stacked LR)': 3\n","}\n","\n","# Define models to plot\n","# Key: (Scores, Label)\n","models_to_plot = {\n","    'Isolation Forest': iso_scores,\n","    'Local Outlier Factor': lof_scores,\n","    'DynaBiome (Stacked LR)': test_probs_dict['Stacked (LR)']\n","}\n","\n","# ==========================================\n","# PLOT 1: ROC CURVES (Left)\n","# ==========================================\n","for name, scores in models_to_plot.items():\n","    fpr, tpr, _ = roc_curve(y_test, scores)\n","    roc_val = auc(fpr, tpr)\n","\n","    ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_val:.3f})',\n","             color=colors[name], linestyle=styles[name],\n","             linewidth=widths[name], alpha=0.8)\n","\n","ax1.plot([0, 1], [0, 1], 'k:', alpha=0.3)\n","ax1.set_xlim([0.0, 1.0])\n","ax1.set_ylim([0.0, 1.02])\n","ax1.set_xlabel('False Positive Rate', fontweight='bold')\n","ax1.set_ylabel('True Positive Rate', fontweight='bold')\n","ax1.set_title('ROC Curves: Proposed vs. Benchmarks', fontweight='bold')\n","ax1.legend(loc=\"lower right\", fontsize=10)\n","ax1.grid(True, alpha=0.3)\n","\n","# ==========================================\n","# PLOT 2: PRECISION-RECALL CURVES (Right)\n","# ==========================================\n","for name, scores in models_to_plot.items():\n","    precision, recall, _ = precision_recall_curve(y_test, scores)\n","    pr_val = auc(recall, precision)\n","\n","    ax2.plot(recall, precision, label=f'{name} (AP = {pr_val:.3f})',\n","             color=colors[name], linestyle=styles[name],\n","             linewidth=widths[name], alpha=0.8)\n","\n","ax2.set_xlim([0.0, 1.0])\n","ax2.set_ylim([0.0, 1.02])\n","ax2.set_xlabel('Recall', fontweight='bold')\n","ax2.set_ylabel('Precision', fontweight='bold')\n","ax2.set_title('Precision-Recall Curves: Proposed vs. Benchmarks', fontweight='bold')\n","ax2.legend(loc=\"lower left\", fontsize=10)\n","ax2.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('Figure_Benchmark_Comparison.pdf', dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"HQZdRMsJVucy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 18. Train State-of-the-Art Baseline: CNN-LSTM with Self-Knowledge Distillation\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, LSTM, Dense,\n","                                     Dropout, GlobalAveragePooling1D, Lambda)\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import backend as K\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n","from sklearn.utils import class_weight # Import class_weight\n","\n","# Clear session to avoid clutter\n","tf.keras.backend.clear_session()\n","\n","# ----- Custom Layer to compute KL divergence and attach loss -----\n","class KLDivergenceLayer(tf.keras.layers.Layer):\n","    def __init__(self, weight=0.1, **kwargs):\n","        super(KLDivergenceLayer, self).__init__(**kwargs)\n","        self.weight = weight\n","\n","    def call(self, inputs):\n","        main_pred, branch_pred = inputs\n","        # Clip to avoid log(0) errors\n","        epsilon = 1e-7\n","        main_pred = tf.clip_by_value(main_pred, epsilon, 1.0)\n","        branch_pred = tf.clip_by_value(branch_pred, epsilon, 1.0)\n","\n","        # Calculate KL Divergence\n","        kl_loss = tf.reduce_mean(tf.keras.losses.kld(main_pred, branch_pred))\n","        self.add_loss(self.weight * kl_loss)\n","        return inputs\n","\n","# ----- Setup Input Shapes based on your Sequence Data -----\n","timesteps = X_train.shape[1]   # Should be 14 (weeks)\n","n_features = X_train.shape[2]  # Should be ~118 (bacteria)\n","\n","# ----- Model Definition -----\n","inp = Input(shape=(timesteps, n_features), name='input_layer')\n","\n","# --- CNN Module ---\n","cnn_out = Conv1D(filters=64,\n","                 kernel_size=3,\n","                 activation='relu',\n","                 padding='same',\n","                 name='conv1d')(inp)\n","\n","# --- Self-Distillation Branch (Shallow Classifier) ---\n","branch_features = GlobalAveragePooling1D(name='global_avg_pool')(cnn_out)\n","branch_output = Dense(1, activation='sigmoid', name='branch_output')(branch_features)\n","\n","# --- Main Pipeline (CNN + LSTM) ---\n","pool_out = MaxPooling1D(pool_size=2, padding='same', name='max_pool')(cnn_out)\n","lstm_out = LSTM(100, return_sequences=False, name='lstm')(pool_out)\n","dense_out = Dense(64, activation='relu', name='dense')(lstm_out)\n","drop = Dropout(0.3, name='dropout')(dense_out)\n","main_output = Dense(1, activation='sigmoid', name='main_output')(drop)\n","\n","# --- Attach KL Divergence Loss ---\n","_ = KLDivergenceLayer(weight=0.1, name='kl_layer')([main_output, branch_output])\n","\n","# Build Model\n","model_skd = Model(inputs=inp, outputs=[main_output, branch_output], name='CNN_LSTM_SKD')\n","\n","# ----- Compile -----\n","# Using dictionary to target specific outputs\n","model_skd.compile(\n","    optimizer=Adam(learning_rate=0.001),\n","    loss={'main_output': 'binary_crossentropy', 'branch_output': 'binary_crossentropy'},\n","    loss_weights={'main_output': 1.0, 'branch_output': 0.5}, # Branch is an auxiliary loss\n","    metrics={'main_output': 'accuracy', 'branch_output': 'accuracy'}\n",")\n","\n","model_skd.summary()\n","\n","# ----- Training -----\n","# Calculate class weights for imbalanced dataset\n","class_weights = class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}\n","\n","# Create dictionary targets for multi-output\n","y_train_targets = {'main_output': y_train, 'branch_output': y_train}\n","y_val_targets = {'main_output': y_val, 'branch_output': y_val}\n","\n","# Added mode='min' to EarlyStopping for loss monitoring\n","early_stopping = EarlyStopping(monitor='val_main_output_loss', patience=10, restore_best_weights=True, mode='min')\n","\n","print(\"\\n--- Training CNN-LSTM SKD Model ---\")\n","history_skd = model_skd.fit(\n","    X_train,\n","    y_train_targets,\n","    validation_data=(X_val, y_val_targets),\n","    epochs=100, # Increased epochs slightly as early stopping will handle it\n","    batch_size=64,\n","    callbacks=[early_stopping],\n","    # We apply the class weights calculated earlier to handle imbalance\n","    # Removed class_weight=class_weights_dict because it's not supported for multi-output models\n","    verbose=1\n",")\n","\n","# ----- Evaluation on Test Set -----\n","print(\"\\n--- Evaluating on Test Set ---\")\n","# Predict returns a list: [main_predictions, branch_predictions]\n","preds_skd = model_skd.predict(X_test)\n","main_preds = preds_skd[0] # We only care about the Main branch for final evaluation\n","\n","# Calculate Metrics\n","roc_val = roc_auc_score(y_test, main_preds)\n","pr_val = average_precision_score(y_test, main_preds)\n","preds_binary = (main_preds > 0.5).astype(int)\n","acc = accuracy_score(y_test, preds_binary)\n","f1_macro = f1_score(y_test, preds_binary, average='macro')\n","f1_weighted = f1_score(y_test, preds_binary, average='weighted')\n","\n","print(\"-\" * 30)\n","print(f\"CNN-LSTM (SKD) Results:\")\n","print(f\"ROC AUC:       {roc_val:.4f}\")\n","print(f\"PR AUC:        {pr_val:.4f}\")\n","print(f\"Accuracy:      {acc:.4f}\")\n","print(f\"F1 (Macro):    {f1_macro:.4f}\")\n","print(f\"F1 (Weighted): {f1_weighted:.4f}\")\n","print(\"-\" * 30)\n"],"metadata":{"id":"BBJ6qeBB5Upn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15xiAha_GuPr"},"source":["# @title Generate Final ROC & PR Curves (All Models + Advanced Ensembles)\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","\n","from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n","\n","\n","\n","# Setup Plot (1 Row, 2 Columns)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9)) # Slightly larger figure for better spacing\n","\n","plt.rcParams.update({'font.size': 14, 'font.family': 'serif'}) # Increased overall font size\n","\n","\n","\n","# --- DEFINE COLORS & STYLES ---\n","\n","colors = {\n","\n","    'LSTM-AE Baseline':    'gray',\n","\n","    'Logistic Regression': '#1f77b4',  # Blue\n","\n","    'Random Forest':       '#ff7f0e',  # Orange\n","\n","    'XGBoost':             '#2ca02c',  # Green\n","\n","    'MLP':                 '#d62728',  # Red\n","\n","    'KNN':                 '#9467bd',  # Purple\n","\n","    'One-Class SVM':       '#e377c2',  # Pink\n","\n","    'Averaged Ensemble':   'black',\n","\n","    'Weighted Ensemble':   '#8c564b',  # Brown\n","\n","    'Stacked (LR)':        '#7f7f7f',  # Gray\n","\n","    'Stacked (XGB)':       '#bcbd22',\n","\n","    'CNN-LSTM (SKD)':      '#17becf'   # Teal for CNN-LSTM SKD\n","\n","}\n","\n","\n","\n","# Styles for differentiating Ensembles and SKD\n","\n","styles = {\n","\n","    'LSTM-AE Baseline':    '--',\n","\n","    'Logistic Regression': '-',\n","\n","    'Random Forest':       '-',\n","\n","    'XGBoost':             '-',\n","\n","    'MLP':                 '-',\n","\n","    'KNN':                 '-',\n","\n","    'One-Class SVM':       ':',\n","\n","    'Averaged Ensemble':   '-',\n","\n","    'Weighted Ensemble':   '--',\n","\n","    'Stacked (LR)':        '-.',\n","\n","    'Stacked (XGB)':       ':',\n","\n","    'CNN-LSTM (SKD)':      '-.' # Dotted line for CNN-LSTM SKD\n","\n","}\n","\n","\n","\n","# Line widths for highlighting\n","\n","line_widths = {\n","\n","    'LSTM-AE Baseline':    2.5,\n","\n","    'Averaged Ensemble':   4,\n","\n","    'CNN-LSTM (SKD)':      3.5\n","\n","}\n","\n","\n","\n","# Add CNN-LSTM (SKD) predictions to test_probs_dict\n","\n","# 'main_preds' and 'y_test' are from the executed cell 'BBJ6qeBB5Upn'\n","\n","roc_skd = roc_auc_score(y_test, main_preds)\n","\n","pr_skd = average_precision_score(y_test, main_preds)\n","\n","test_probs_dict['CNN-LSTM (SKD)'] = main_preds.flatten()\n","\n","\n","\n","# Compile all models to plot\n","\n","all_models_to_plot = [\n","\n","    'LSTM-AE Baseline'\n","\n","] + list(best_models.keys()) + [\n","\n","    'Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)',\n","\n","    'CNN-LSTM (SKD)' # Add CNN-LSTM SKD here\n","\n","]\n","\n","\n","\n","# ==========================================\n","\n","# PLOT 1: ROC CURVES (Left Panel)\n","\n","# ==========================================\n","\n","\n","\n","for name in all_models_to_plot:\n","\n","    if name == 'LSTM-AE Baseline':\n","\n","        fpr, tpr, _ = roc_curve(y_test, test_mae)\n","\n","        auc_val = roc_auc_score(y_test, test_mae)\n","\n","    elif name == 'One-Class SVM':\n","\n","        scores = -best_models[name].decision_function(X_test_feat)\n","\n","        fpr, tpr, _ = roc_curve(y_test, scores)\n","\n","        auc_val = roc_auc_score(y_test, scores)\n","\n","    elif name == 'CNN-LSTM (SKD)':\n","\n","        fpr, tpr, _ = roc_curve(y_test, test_probs_dict[name])\n","\n","        auc_val = roc_auc_score(y_test, test_probs_dict[name])\n","\n","    elif name in best_models:\n","\n","        probs = best_models[name].predict_proba(X_test_feat)[:, 1]\n","\n","        fpr, tpr, _ = roc_curve(y_test, probs)\n","\n","        auc_val = roc_auc_score(y_test, probs)\n","\n","    else: # Ensembles (Averaged, Weighted, Stacked) which are in test_probs_dict\n","\n","        probs = test_probs_dict[name]\n","\n","        fpr, tpr, _ = roc_curve(y_test, probs)\n","\n","        auc_val = roc_auc_score(y_test, probs)\n","\n","\n","\n","    lw = line_widths.get(name, 1.5) # Default linewidth\n","\n","    ax1.plot(fpr, tpr, label=f'{name} (AUC = {auc_val:.3f})',\n","\n","             color=colors.get(name, 'black'), linestyle=styles.get(name, '-'),\n","\n","             linewidth=lw, alpha=0.8)\n","\n","\n","\n","# Formatting ROC\n","\n","ax1.plot([0, 1], [0, 1], 'k:', alpha=0.4)\n","\n","ax1.set_xlim([0.0, 1.0])\n","\n","ax1.set_ylim([0.0, 1.02])\n","\n","ax1.set_xlabel('False Positive Rate (1 - Specificity)', fontweight='bold')\n","\n","ax1.set_ylabel('True Positive Rate (Sensitivity)', fontweight='bold')\n","\n","ax1.set_title('Receiver Operating Characteristic (ROC)', fontweight='bold', pad=10)\n","\n","ax1.legend(loc=\"lower right\", fontsize=11, ncol=1)\n","\n","ax1.grid(True, alpha=0.3)\n","\n","\n","\n","\n","\n","# ==========================================\n","\n","# PLOT 2: PRECISION-RECALL CURVES (Right Panel)\n","\n","# ==========================================\n","\n","\n","\n","for name in all_models_to_plot:\n","\n","    if name == 'LSTM-AE Baseline':\n","\n","        precision, recall, _ = precision_recall_curve(y_test, test_mae)\n","\n","        pr_auc_val = average_precision_score(y_test, test_mae)\n","\n","    elif name == 'One-Class SVM':\n","\n","        scores = -best_models[name].decision_function(X_test_feat)\n","\n","        precision, recall, _ = precision_recall_curve(y_test, scores)\n","\n","        pr_auc_val = average_precision_score(y_test, scores)\n","\n","    elif name == 'CNN-LSTM (SKD)':\n","\n","        precision, recall, _ = precision_recall_curve(y_test, test_probs_dict[name])\n","\n","        pr_auc_val = average_precision_score(y_test, test_probs_dict[name])\n","\n","    elif name in best_models:\n","\n","        probs = best_models[name].predict_proba(X_test_feat)[:, 1]\n","\n","        precision, recall, _ = precision_recall_curve(y_test, probs)\n","\n","        pr_auc_val = average_precision_score(y_test, probs)\n","\n","    else: # Ensembles\n","\n","        probs = test_probs_dict[name]\n","\n","        precision, recall, _ = precision_recall_curve(y_test, probs)\n","\n","        pr_auc_val = average_precision_score(y_test, probs)\n","\n","\n","\n","    lw = line_widths.get(name, 1.5)\n","\n","    ax2.plot(recall, precision, label=f'{name} (AP = {pr_auc_val:.3f})',\n","\n","             color=colors.get(name, 'black'), linestyle=styles.get(name, '-'),\n","\n","             linewidth=lw, alpha=0.8)\n","\n","\n","\n","# Formatting PR\n","\n","ax2.set_xlim([0.0, 1.0])\n","\n","ax2.set_ylim([0.0, 1.02])\n","\n","ax2.set_xlabel('Recall (Sensitivity)', fontweight='bold')\n","\n","ax2.set_ylabel('Precision (Positive Predictive Value)', fontweight='bold')\n","\n","ax2.set_title('Precision-Recall (PR) Curves', fontweight='bold', pad=10)\n","\n","ax2.legend(loc=\"lower left\", fontsize=11, ncol=1)\n","\n","ax2.grid(True, alpha=0.3)\n","\n","\n","\n","plt.tight_layout()\n","\n","plt.savefig('Figure_Combined_ROCPR_Analysis.pdf', dpi=600, bbox_inches='tight')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, LSTM, Dropout\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras import regularizers, backend as K\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.feature_selection import SelectKBest, mutual_info_classif\n","from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n","import pandas as pd\n","\n","# Clear session\n","tf.keras.backend.clear_session()\n","\n","print(\"--- BENCHMARKING: Sparse AE + LSTM & SelectKBest + LSTM ---\")\n","\n","# --- CONFIGURATION (Matched to your notebook) ---\n","n_timesteps = X_train.shape[1]\n","n_features = X_train.shape[2]\n","encoding_dim = 64  # Size of sparse layer\n","k_best = 50        # Number of features for SelectKBest\n","rho = 0.01         # Sparsity parameter\n","beta = 3.0         # Sparsity weight\n","\n","# Common Early Stopping\n","es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# ==============================================================================\n","# 1. Sparse Autoencoder + LSTM\n","# ==============================================================================\n","print(\"\\n--- 1. Running Sparse Autoencoder + LSTM ---\")\n","\n","# Custom KL Divergence Regularizer\n","class KLDivergenceRegularizer(regularizers.Regularizer):\n","    def __init__(self, rho=0.01, beta=3.0):\n","        self.rho = rho\n","        self.beta = beta\n","\n","    def __call__(self, x):\n","        rho_hat = K.mean(x, axis=0)\n","        epsilon = 1e-6\n","        rho_hat = K.clip(rho_hat, epsilon, 1.0 - epsilon)\n","        loss = self.rho * K.log(self.rho / rho_hat) + \\\n","               (1 - self.rho) * K.log((1 - self.rho) / (1 - rho_hat))\n","        return self.beta * K.sum(loss)\n","\n","    def get_config(self):\n","        return {'rho': self.rho, 'beta': self.beta}\n","\n","# A. Prepare Flattened Data for AE\n","X_train_flat = X_train.reshape(-1, n_features)\n","X_val_flat = X_val.reshape(-1, n_features)\n","\n","# B. Build & Train Sparse AE\n","input_layer = Input(shape=(n_features,))\n","sparse_encoded = Dense(encoding_dim, activation='relu',\n","                       activity_regularizer=KLDivergenceRegularizer(rho, beta),\n","                       name='sparse_layer')(input_layer)\n","decoded = Dense(n_features, activation='sigmoid')(sparse_encoded)\n","\n","autoencoder_sparse = Model(input_layer, decoded)\n","autoencoder_sparse.compile(optimizer='adam', loss='mse')\n","\n","print(\"Training Sparse Autoencoder...\")\n","autoencoder_sparse.fit(X_train_flat, X_train_flat,\n","                       epochs=50, batch_size=256,\n","                       validation_data=(X_val_flat, X_val_flat),\n","                       callbacks=[es], verbose=0)\n","\n","# C. Extract Features\n","encoder_model = Model(inputs=autoencoder_sparse.input, outputs=autoencoder_sparse.get_layer('sparse_layer').output)\n","\n","def encode_sequence(data, enc_model):\n","    N, T, F = data.shape\n","    flat = data.reshape(-1, F)\n","    encoded_flat = enc_model.predict(flat, verbose=0)\n","    return encoded_flat.reshape(N, T, -1)\n","\n","X_train_enc = encode_sequence(X_train, encoder_model)\n","X_val_enc = encode_sequence(X_val, encoder_model)\n","X_test_enc = encode_sequence(X_test, encoder_model)\n","\n","# D. Train LSTM on Sparse Features\n","lstm_input = Input(shape=(n_timesteps, encoding_dim))\n","l = LSTM(64, return_sequences=False)(lstm_input)\n","l = Dense(32, activation='relu')(l)\n","l = Dropout(0.3)(l)\n","out = Dense(1, activation='sigmoid')(l)\n","\n","lstm_sparse = Model(lstm_input, out)\n","lstm_sparse.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","print(\"Training LSTM on Sparse Features...\")\n","lstm_sparse.fit(X_train_enc, y_train,\n","                validation_data=(X_val_enc, y_val),\n","                epochs=50, batch_size=64,\n","                callbacks=[es], class_weight=class_weights_dict, verbose=0)\n","\n","# Evaluate\n","pred_sparse = lstm_sparse.predict(X_test_enc, verbose=0).ravel()\n","roc_sparse = roc_auc_score(y_test, pred_sparse)\n","pr_sparse = average_precision_score(y_test, pred_sparse)\n","f1_sparse = f1_score(y_test, (pred_sparse > 0.5).astype(int), average='macro')\n","f1_sparse_weighted = f1_score(y_test, (pred_sparse > 0.5).astype(int), average='weighted')\n","acc_sparse = accuracy_score(y_test, (pred_sparse > 0.5).astype(int))\n","\n","print(f\"Result: ROC={roc_sparse:.4f} | PR={pr_sparse:.4f}\")\n","\n","\n","# ==============================================================================\n","# 2. SelectKBest + LSTM\n","# ==============================================================================\n","print(\"\\n--- 2. Running SelectKBest + LSTM ---\")\n","\n","# A. Feature Selection (on Time-Averaged Data)\n","X_train_avg = np.mean(X_train, axis=1)\n","selector = SelectKBest(score_func=mutual_info_classif, k=k_best)\n","selector.fit(X_train_avg, y_train)\n","selected_indices = selector.get_support(indices=True)\n","\n","# B. Subset Data\n","X_train_sel = X_train[:, :, selected_indices]\n","X_val_sel = X_val[:, :, selected_indices]\n","X_test_sel = X_test[:, :, selected_indices]\n","\n","# C. Train LSTM on Selected Features\n","lstm_sel_input = Input(shape=(n_timesteps, k_best))\n","l2 = LSTM(64, return_sequences=False)(lstm_sel_input)\n","l2 = Dense(32, activation='relu')(l2)\n","l2 = Dropout(0.3)(l2)\n","out2 = Dense(1, activation='sigmoid')(l2)\n","\n","lstm_select = Model(lstm_sel_input, out2)\n","lstm_select.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","print(\"Training LSTM on SelectKBest Features...\")\n","lstm_select.fit(X_train_sel, y_train,\n","                validation_data=(X_val_sel, y_val),\n","                epochs=50, batch_size=64,\n","                callbacks=[es], class_weight=class_weights_dict, verbose=0)\n","\n","# Evaluate\n","pred_sel = lstm_select.predict(X_test_sel, verbose=0).ravel()\n","roc_sel = roc_auc_score(y_test, pred_sel)\n","pr_sel = average_precision_score(y_test, pred_sel)\n","f1_sel = f1_score(y_test, (pred_sel > 0.5).astype(int), average='macro')\n","f1_sel_weighted = f1_score(y_test, (pred_sel > 0.5).astype(int), average='weighted')\n","acc_sel = accuracy_score(y_test, (pred_sel > 0.5).astype(int))\n","\n","print(f\"Result: ROC={roc_sel:.4f} | PR={pr_sel:.4f}\")\n","\n","# ==============================================================================\n","# 3. Final Comparison Table\n","# ==============================================================================\n","results_data = [\n","    ['Sparse AE + LSTM', roc_sparse, pr_sparse, acc_sparse, f1_sparse, f1_sparse_weighted],\n","    ['SelectKBest + LSTM', roc_sel, pr_sel, acc_sel, f1_sel, f1_sel_weighted]\n","]\n","df_feat_bench = pd.DataFrame(results_data, columns=['Model', 'ROC AUC', 'PR AUC', 'Accuracy', 'F1 (Macro)', 'F1 (Weighted)'])\n","print(\"\\n--- FEATURE SELECTION BENCHMARKS ---\")\n","print(df_feat_bench.to_string())"],"metadata":{"id":"7uBWAvaaIuC_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_probs_dict['Sparse AE + LSTM'] = pred_sparse\n","test_probs_dict['SelectKBest + LSTM'] = pred_sel\n","\n","# Add Isolation Forest and LOF scores, inverting them as anomaly detection models\n","# higher scores indicate higher anomaly, which aligns with higher probability of dysbiosis (label 1)\n","test_probs_dict['Isolation Forest'] = iso_scores\n","test_probs_dict['Local Outlier Factor'] = lof_scores"],"metadata":{"id":"bm5lQcoVRcD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 15. Feature Importance Analysis (Reconstruction Attribution)\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# --- 1. CALCULATE ERROR PER FEATURE ---\n","# Reconstruct the Test Set\n","reconstructions = autoencoder.predict(X_test, verbose=0)\n","\n","# Calculate MAE for each feature (averaged over time only)\n","# Result Shape: (n_samples, n_features)\n","mae_per_feature = np.mean(np.abs(X_test - reconstructions), axis=1)\n","\n","# --- 2. IDENTIFY DYSBIOTIC SAMPLES ---\n","# We focus on samples that were TRULY Dysbiotic (y_test == 1)\n","# to see what characterizes the disease state.\n","dysbiosis_indices = np.where(y_test == 1)[0]\n","dysbiosis_errors = mae_per_feature[dysbiosis_indices]\n","\n","# Calculate Mean Error per feature across all Dysbiotic samples\n","mean_error_per_feature = np.mean(dysbiosis_errors, axis=0)\n","\n","# --- 3. MAP TO NAMES ---\n","# Create a DataFrame\n","# Note: 'feature_cols' comes from your Step 3 (Patient Split) block\n","importance_df = pd.DataFrame({\n","    'Feature': feature_cols,\n","    'Mean_MAE': mean_error_per_feature\n","})\n","\n","# Sort by Error (High Error = High Importance)\n","importance_df = importance_df.sort_values(by='Mean_MAE', ascending=False).head(20)\n","\n","# --- 4. PLOT ---\n","plt.figure(figsize=(10, 8))\n","plt.rcParams.update({'font.size': 12, 'font.family': 'serif'})\n","\n","sns.barplot(data=importance_df, x='Mean_MAE', y='Feature', palette='viridis')\n","\n","plt.title('Top 20 Bacteria Driving Dysbiosis Classification\\n(Based on Reconstruction Error Contribution)', fontweight='bold')\n","plt.xlabel('Mean Reconstruction Error (MAE)', fontweight='bold')\n","plt.ylabel('Bacterial Genus', fontweight='bold')\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('Figure4_Feature_Importance.pdf', dpi=600)\n","plt.show()\n","\n","print(\"Top 5 Drivers:\")\n","print(importance_df[['Feature', 'Mean_MAE']].head(5))"],"metadata":{"id":"T2MdvfhVfx4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install shap\n","import shap\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# ==========================================\n","# 1. SETUP: Define the \"Score Function\"\n","# ==========================================\n","# SHAP needs a function that takes data -> outputs a single number (Anomaly Score)\n","# We calculate the Mean Squared Error (MSE) for each sample.\n","\n","def model_loss_function(data):\n","    # data shape: (samples, features) or (samples, timesteps, features)\n","\n","    # If your model expects 3D input (LSTM) but SHAP passes 2D, reshape it:\n","    timesteps = SEQ_LEN  # Use the globally defined sequence length\n","    num_features = data.shape[1] // timesteps\n","\n","    # Reshape for the model\n","    data_3d = data.reshape(-1, timesteps, num_features)\n","\n","    # Get reconstruction from the autoencoder\n","    reconstruction = autoencoder.predict(data_3d, verbose=0)\n","\n","    # Calculate MSE (Anomaly Score) per sample\n","    # We flatten to calculate total error per sample\n","    mse = np.mean(np.square(data_3d - reconstruction), axis=(1,2))\n","    return mse\n","\n","# ==========================================\n","# 2. PREPARE DATA FOR SHAP\n","# ==========================================\n","# SHAP creates a \"background\" to compare against.\n","# We use a summary of the training set (Normal data) to speed this up.\n","\n","# Flatten your 3D X_train and X_test to 2D for SHAP (samples, timesteps*features)\n","X_train_flat = X_train.reshape(X_train.shape[0], -1) # Corrected variable name\n","X_test_flat = X_test.reshape(X_test.shape[0], -1)   # Corrected variable name\n","\n","# Use a random subset of background data (e.g., 100 samples)\n","background = shap.sample(X_train_flat, 50)\n","\n","# ==========================================\n","# 3. RUN SHAP (KernelExplainer)\n","# ==========================================\n","print(\"Computing SHAP values... (This may take a few minutes)\")\n","\n","# The explainer expects the model_loss_function and the background data\n","explainer = shap.KernelExplainer(model_loss_function, background)\n","\n","# Calculate SHAP for a subset of Test data (e.g., the Anomalies)\n","# explaining first 50 test samples. This can be adjusted.\n","shap_values = explainer.shap_values(X_test_flat[0:50])\n","\n","# ==========================================\n","# 4. HANDLING FEATURE NAMES\n","# ==========================================\n","# The feature_cols list from earlier processing contains the names of your microbial genera.\n","feature_names = feature_cols # Use the feature_cols defined earlier\n","\n","# WE NEED TO AGGREGATE SHAP VALUES ACROSS TIMESTEPS\n","# Current shape of shap_values: (samples, timesteps * features)\n","# We want: (samples, features)\n","num_features = X_train.shape[2] # Use n_features from X_train shape\n","timesteps = SEQ_LEN # Use the globally defined sequence length\n","\n","shap_values_reshaped = shap_values.reshape(-1, timesteps, num_features)\n","# Sum or Mean across timesteps to get importance of the bacteria regardless of time\n","shap_values_aggr = np.mean(shap_values_reshaped, axis=1)\n","\n","# Same for the input data (for the color in the plot)\n","X_test_aggr = np.mean(X_test_flat[0:50].reshape(-1, timesteps, num_features), axis=1)\n","\n","# ==========================================\n","# 5. PLOTTING\n","# ==========================================\n","\n","# A. SUMMARY PLOT (Beeswarm) - The most important graph\n","plt.figure()\n","shap.summary_plot(\n","    shap_values_aggr,\n","    X_test_aggr,\n","    feature_names=feature_names, # List of your bacteria names\n","    max_display=20, # Only show top 20\n","    show=False\n",")\n","plt.title(\"Impact of Microbial Features on Anomaly Score\")\n","plt.savefig(\"Fig4_SHAP_Beeswarm.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# B. GLOBAL BAR PLOT\n","plt.figure()\n","shap.summary_plot(\n","    shap_values_aggr,\n","    X_test_aggr,\n","    feature_names=feature_names,\n","    plot_type=\"bar\",\n","    max_display=20,\n","    show=False\n",")\n","plt.title(\"Top 20 Drivers of Dysbiosis (Global Importance)\")\n","plt.savefig(\"Fig5_SHAP_Bar.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# C. LOCAL EXPLANATION (Waterfall) - For one specific patient\n","# Let's pick a high-anomaly patient (e.g., index 0)\n","plt.figure()\n","shap.plots.waterfall(\n","    shap.Explanation(\n","        values=shap_values_aggr[0],\n","        base_values=explainer.expected_value,\n","        data=X_test_aggr[0],\n","        feature_names=feature_names\n","    ),\n","    max_display=10,\n","    show=False\n",")\n","plt.title(\"Local Explanation: Why was Patient 0 flagged?\")\n","plt.savefig(\"Fig6_Local_Explanation.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"3IOkk3WZP11v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"545626c7"},"source":["# Task\n","Extract clinical markers (average `MaxTemperature`, average `NeutrophilCount`, and mode of `Consistency_liquid`) for each sequence in `df_test` (using a sequence length of `SEQ_LEN`), align these markers with the corresponding `test_mae` anomaly scores and `y_test` labels. Calculate and display the Pearson correlation coefficients between the `test_mae` and these clinical markers, and generate scatter plots to visualize the relationships between `test_mae` and `MaxTemperature` and `NeutrophilCount`."]},{"cell_type":"markdown","metadata":{"id":"61306d28"},"source":["## Extract clinical markers for test sequences\n","\n","### Subtask:\n","Create a new function to iterate through the `df_test` DataFrame, grouping by `PatientID`, and for each sequence (of length `SEQ_LEN`), extract the average `MaxTemperature`, `NeutrophilCount`, and the mode of `Consistency_liquid`.\n"]},{"cell_type":"markdown","metadata":{"id":"9c7ad5de"},"source":["**Reasoning**:\n","The subtask requires defining a new function `extract_clinical_markers` to process `df_test` and extract clinical features using a sliding window. This function will calculate the mean for 'MaxTemperature' and 'NeutrophilCount' and the mode for 'Consistency_liquid' for each sequence.\n","\n"]},{"cell_type":"code","metadata":{"id":"48267fb2"},"source":["def extract_clinical_markers(df, clinical_cols, seq_len):\n","    \"\"\"\n","    Generates clinical markers for sequences based on patient groups and sliding windows.\n","    \"\"\"\n","    extracted_features = []\n","    extracted_labels = []\n","\n","    # Group by patient to ensure window never crosses patient boundaries\n","    for pid, group in df.groupby('PatientID'):\n","        # Sort by time\n","        group = group.sort_values('DayRelativeToNearestHCT')\n","\n","        # Ensure all clinical columns are present in the group\n","        current_clinical_data = group[clinical_cols + ['DysbiosisLabel']].values\n","\n","        if len(current_clinical_data) >= seq_len:\n","            for i in range(len(current_clinical_data) - seq_len + 1):\n","                window_data = current_clinical_data[i : i + seq_len]\n","\n","                # Extract features for the current window\n","                temp_vals = window_data[:, clinical_cols.index('MaxTemperature')]\n","                neutro_vals = window_data[:, clinical_cols.index('NeutrophilCount')]\n","                consistency_vals = window_data[:, clinical_cols.index('Consistency_liquid')]\n","\n","                # Calculate mean for continuous variables\n","                mean_temp = np.mean(temp_vals)\n","                mean_neutro = np.mean(neutro_vals)\n","\n","                # Calculate mode for categorical/binary variable (Consistency_liquid)\n","                # Handle cases where there might be multiple modes, take the first one.\n","                # Ensure it's treated as integer for mode if it's 0/1\n","                mode_consistency = pd.Series(consistency_vals).mode()[0]\n","\n","                # The label for the sequence (using the last label in the window)\n","                # or the max label in the window, to align with build_sequences logic\n","                sequence_label = int(np.max(window_data[:, -1])) # -1 is DysbiosisLabel\n","\n","                extracted_features.append({\n","                    'PatientID': pid,\n","                    'SequenceIndex': i,\n","                    'AvgMaxTemperature': mean_temp,\n","                    'AvgNeutrophilCount': mean_neutro,\n","                    'ModeConsistencyLiquid': mode_consistency\n","                })\n","                extracted_labels.append(sequence_label)\n","\n","    # Create a DataFrame from the extracted features and add the labels\n","    df_clinical_features = pd.DataFrame(extracted_features)\n","    df_clinical_features['DysbiosisLabel'] = extracted_labels\n","    return df_clinical_features\n","\n","# Define clinical columns to extract\n","clinical_columns = ['MaxTemperature', 'NeutrophilCount', 'Consistency_liquid']\n","\n","# Extract clinical markers for the test set\n","df_test_clinical = extract_clinical_markers(df_test, clinical_columns, SEQ_LEN)\n","\n","print(f\"Extracted clinical features for {len(df_test_clinical)} test sequences.\")\n","print(df_test_clinical.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceff798f"},"source":["**Reasoning**:\n","The next step is to align the extracted clinical markers (`df_test_clinical`) with the anomaly scores (`test_mae`) and true labels (`y_test`). Then, I will combine them into a single DataFrame, calculate Pearson correlation coefficients, and generate scatter plots as requested by the subtask.\n","\n"]},{"cell_type":"code","metadata":{"id":"fec179ce"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Ensure lengths match\n","if len(df_test_clinical) == len(test_mae) == len(y_test):\n","    print(\"Lengths of df_test_clinical, test_mae, and y_test match.\")\n","else:\n","    print(\"Lengths do not match. Re-check sequence generation logic.\")\n","    # Handle error or adjust data if lengths don't match, e.g., truncate\n","    min_len = min(len(df_test_clinical), len(test_mae), len(y_test))\n","    df_test_clinical = df_test_clinical.head(min_len)\n","    test_mae = test_mae[:min_len]\n","    y_test = y_test[:min_len]\n","    print(f\"Adjusted all to minimum length: {min_len}\")\n","\n","# Add anomaly scores and true labels to the clinical features DataFrame\n","df_test_clinical['AnomalyScore_MAE'] = test_mae\n","df_test_clinical['TrueLabel_Dysbiosis'] = y_test\n","\n","# --- Calculate Pearson Correlation Coefficients ---\n","print(\"\\nPearson Correlation Coefficients with AnomalyScore_MAE:\")\n","correlation_temp = df_test_clinical['AnomalyScore_MAE'].corr(df_test_clinical['AvgMaxTemperature'])\n","correlation_neutro = df_test_clinical['AnomalyScore_MAE'].corr(df_test_clinical['AvgNeutrophilCount'])\n","\n","print(f\"  AnomalyScore_MAE vs AvgMaxTemperature: {correlation_temp:.4f}\")\n","print(f\"  AnomalyScore_MAE vs AvgNeutrophilCount: {correlation_neutro:.4f}\")\n","\n","# --- Generate Scatter Plots ---\n","plt.figure(figsize=(15, 6))\n","sns.set_style(\"whitegrid\")\n","\n","# Plot 1: Anomaly Score vs. Average Max Temperature\n","plt.subplot(1, 2, 1)\n","sns.scatterplot(x='AvgMaxTemperature', y='AnomalyScore_MAE', hue='TrueLabel_Dysbiosis', data=df_test_clinical, alpha=0.6, palette='viridis')\n","plt.title('Anomaly Score (MAE) vs. Average Max Temperature', fontsize=14)\n","plt.xlabel('Average Max Temperature', fontsize=12)\n","plt.ylabel('Anomaly Score (MAE)', fontsize=12)\n","plt.legend(title='True Dysbiosis', loc='upper left')\n","\n","# Plot 2: Anomaly Score vs. Average Neutrophil Count\n","plt.subplot(1, 2, 2)\n","sns.scatterplot(x='AvgNeutrophilCount', y='AnomalyScore_MAE', hue='TrueLabel_Dysbiosis', data=df_test_clinical, alpha=0.6, palette='viridis')\n","plt.title('Anomaly Score (MAE) vs. Average Neutrophil Count', fontsize=14)\n","plt.xlabel('Average Neutrophil Count', fontsize=12)\n","plt.ylabel('Anomaly Score (MAE)', fontsize=12)\n","plt.legend(title='True Dysbiosis', loc='upper right')\n","\n","plt.tight_layout()\n","plt.savefig('Pearson Correlation Coefficients with AnomalyScore_MAE.pdf', dpi=600)\n","plt.show()\n","\n","print(\"\\nCombined DataFrame head:\")\n","print(df_test_clinical.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"085a5b92"},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os\n","\n","# 1. Calculate Pearson correlation for AnomalyScore_MAE vs ModeConsistencyLiquid\n","correlation_consistency = df_test_clinical['AnomalyScore_MAE'].corr(\n","    df_test_clinical['ModeConsistencyLiquid']\n",")\n","\n","# 2. Print this correlation coefficient\n","print(f\"AnomalyScore_MAE vs ModeConsistencyLiquid: {correlation_consistency:.4f}\")\n","\n","# 3. Create a correlation matrix\n","correlation_cols = [\n","    'AnomalyScore_MAE',\n","    'AvgMaxTemperature',\n","    'AvgNeutrophilCount',\n","    'ModeConsistencyLiquid'\n","]\n","correlation_matrix = df_test_clinical[correlation_cols].corr()\n","\n","print(\"\\nCorrelation Matrix:\")\n","print(correlation_matrix)\n","\n","# 4. Generate a heatmap of this correlation matrix\n","sns.set_theme(style=\"white\")  # clean style\n","plt.figure(figsize=(9, 7))\n","\n","sns.heatmap(\n","    correlation_matrix,\n","    annot=True,\n","    cmap=\"coolwarm\",\n","    fmt=\".2f\",\n","    linewidths=0.7,\n","    linecolor=\"white\",\n","    center=0,\n","    square=True,\n","    annot_kws={\"size\":12, \"weight\":\"bold\"}\n",")\n","\n","# Make x-axis labels readable\n","plt.xticks(rotation=45, ha=\"right\", fontsize=12)\n","plt.yticks(rotation=0, fontsize=12)\n","\n","# Title\n","plt.title(\n","    'Pearson Correlation Matrix of Anomaly Score and Clinical Markers',\n","    fontsize=16, fontweight='bold', pad=20\n",")\n","\n","plt.tight_layout()\n","\n","# Save before showing\n","output_file = \"Pearson_Correlation_Heatmap.pdf\"\n","plt.savefig('Pearson_Correlation_Heatmap.pdf', dpi=600, bbox_inches='tight')\n","plt.close()\n","\n","print(f\"Figure saved successfully at: {os.path.abspath(output_file)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Set theme for aesthetics\n","sns.set_theme(style=\"white\")\n","\n","plt.figure(figsize=(9, 7))\n","sns.heatmap(\n","    correlation_matrix,\n","    annot=True,\n","    cmap=\"mako\",\n","    fmt=\".2f\",\n","    linewidths=0.7,\n","    linecolor=\"white\",\n","    center=0,\n","    square=True,\n","    annot_kws={\"size\":12, \"weight\":\"bold\"}\n",")\n","\n","# Flip axes: transpose the matrix\n","sns.heatmap(\n","    correlation_matrix.T,   # transpose flips x/y\n","    annot=True,\n","    cmap=\"mako\",\n","    fmt=\".2f\",\n","    linewidths=0.7,\n","    linecolor=\"white\",\n","    center=0,\n","    square=True,\n","    annot_kws={\"size\":12, \"weight\":\"bold\"}\n",")\n","\n","# Make x-axis labels readable\n","plt.xticks(rotation=45, ha=\"right\", fontsize=12)\n","plt.yticks(rotation=0, fontsize=12)\n","\n","# Title\n","plt.title('Pearson Correlation Matrix\\nAnomaly Score vs Clinical Markers',\n","          fontsize=16, fontweight='bold', pad=20)\n","\n","plt.tight_layout()\n","sns.despine()\n","plt.savefig('Pearson_Correlation_Heatmap_Flipped.pdf', dpi=600)\n","plt.show()"],"metadata":{"id":"iX0FTVyQwtkj"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b240d7e0"},"source":["df_test_clinical['AvgMaxTemperature_sq'] = df_test_clinical['AvgMaxTemperature']**2\n","df_test_clinical['AvgNeutrophilCount_sq'] = df_test_clinical['AvgNeutrophilCount']**2\n","df_test_clinical['Temp_x_Neutro'] = df_test_clinical['AvgMaxTemperature'] * df_test_clinical['AvgNeutrophilCount']\n","df_test_clinical['Temp_x_Consistency'] = df_test_clinical['AvgMaxTemperature'] * df_test_clinical['ModeConsistencyLiquid']\n","df_test_clinical['Neutro_x_Consistency'] = df_test_clinical['AvgNeutrophilCount'] * df_test_clinical['ModeConsistencyLiquid']\n","\n","print(\"Updated df_test_clinical with polynomial and interaction features:\")\n","print(df_test_clinical.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"126a9326"},"source":["new_features = [\n","    'AvgMaxTemperature_sq',\n","    'AvgNeutrophilCount_sq',\n","    'Temp_x_Neutro',\n","    'Temp_x_Consistency',\n","    'Neutro_x_Consistency'\n","]\n","\n","print(\"\\nPearson Correlation Coefficients with AnomalyScore_MAE (New Features):\")\n","for feature in new_features:\n","    correlation = df_test_clinical['AnomalyScore_MAE'].corr(df_test_clinical[feature])\n","    print(f\"  AnomalyScore_MAE vs {feature}: {correlation:.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"99080626"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Identify features with the highest absolute correlation from the previous step\n","# AnomalyScore_MAE vs Temp_x_Consistency: 0.2125\n","# AnomalyScore_MAE vs Temp_x_Neutro: -0.1961\n","\n","plt.figure(figsize=(15, 6))\n","sns.set_style(\"whitegrid\")\n","\n","# Plot 1: Anomaly Score vs. Temp_x_Consistency\n","plt.subplot(1, 2, 1)\n","sns.scatterplot(x='Temp_x_Consistency', y='AnomalyScore_MAE', hue='TrueLabel_Dysbiosis', data=df_test_clinical, alpha=0.6, palette='viridis')\n","plt.title('Anomaly Score (MAE) vs. Temp x Consistency (Interaction)', fontsize=14)\n","plt.xlabel('Average Max Temperature x Mode Consistency Liquid', fontsize=12)\n","plt.ylabel('Anomaly Score (MAE)', fontsize=12)\n","plt.legend(title='True Dysbiosis', loc='upper left')\n","\n","# Plot 2: Anomaly Score vs. Temp_x_Neutro\n","plt.subplot(1, 2, 2)\n","sns.scatterplot(x='Temp_x_Neutro', y='AnomalyScore_MAE', hue='TrueLabel_Dysbiosis', data=df_test_clinical, alpha=0.6, palette='viridis')\n","plt.title('Anomaly Score (MAE) vs. Temp x Neutrophil (Interaction)', fontsize=14)\n","plt.xlabel('Average Max Temperature x Average Neutrophil Count', fontsize=12)\n","plt.ylabel('Anomaly Score (MAE)', fontsize=12)\n","plt.legend(title='True Dysbiosis', loc='upper left')\n","\n","plt.tight_layout()\n","plt.savefig('AnomalyScore_ClinicalInteractions_Scatter.pdf', dpi=600)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 19. Generate Publication-Ready ROC & PR Curves (Enhanced)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n","\n","# --- PLOT SETUP ---\n","# Use a professional style and size\n","plt.rcParams.update({\n","    'font.family': 'serif',\n","    'font.size': 14,\n","    'axes.labelsize': 16,\n","    'axes.titlesize': 18,\n","    'xtick.labelsize': 14,\n","    'ytick.labelsize': 14,\n","    'legend.fontsize': 12,\n","    'lines.linewidth': 2\n","})\n","\n","# Create Figure\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), dpi=600)\n","\n","# --- STYLE CONFIGURATION ---\n","# Define precise styles to highlight your model vs. baselines\n","model_styles = {\n","    # PROPOSED MODEL (The Star) -> Bold, Solid, Red/Black\n","    'Stacked (LR)':        {'color': '#D62728', 'ls': '-',  'lw': 4.0, 'zorder': 100}, # Red\n","\n","    # SOTA COMPETITOR -> Bold, Solid, Blue\n","    'CNN-LSTM (SKD)':      {'color': '#1F77B4', 'ls': '-',  'lw': 3.0, 'zorder': 90},  # Blue\n","\n","    # STRONG ENSEMBLES -> Medium, Dashed\n","    'Averaged Ensemble':   {'color': 'black',   'ls': '--', 'lw': 2.5, 'zorder': 80},\n","    'Weighted Ensemble':   {'color': '#2CA02C', 'ls': '--', 'lw': 2.0, 'zorder': 70}, # Green\n","\n","    # BASELINES -> Thinner, Gray/Muted\n","    'LSTM-AE Baseline':    {'color': 'gray',    'ls': ':',  'lw': 2.5, 'zorder': 60},\n","    'One-Class SVM':       {'color': '#E377C2', 'ls': ':',  'lw': 2.0, 'zorder': 50}, # Pink\n","\n","    # INDIVIDUAL CLASSIFIERS -> Thin, Semi-transparent\n","    'Logistic Regression': {'color': '#7F7F7F', 'ls': '-',  'lw': 1.0, 'zorder': 40}, # Gray\n","    'Random Forest':       {'color': '#FF7F0E', 'ls': '-',  'lw': 1.5, 'zorder': 45}, # Orange\n","    'XGBoost':             {'color': '#8C564B', 'ls': '-',  'lw': 1.0, 'zorder': 40}, # Brown\n","    'MLP':                 {'color': '#9467BD', 'ls': '-',  'lw': 1.0, 'zorder': 40}, # Purple\n","    'KNN':                 {'color': '#BCBD22', 'ls': '-',  'lw': 1.0, 'zorder': 40}, # Olive\n","    'Stacked (XGB)':       {'color': '#17BECF', 'ls': ':',  'lw': 1.5, 'zorder': 45}, # Cyan\n","}\n","\n","# Ensure all models in your list have a style (default fallback)\n","default_style = {'color': 'gray', 'ls': '-', 'lw': 1.0, 'zorder': 10}\n","\n","# Define models to plot (Order matters for legend: Top performers first usually)\n","all_models_ordered = [\n","    'Stacked (LR)',          # Your Best\n","    'CNN-LSTM (SKD)',        # Benchmark\n","    'Averaged Ensemble',\n","    'Random Forest',         # Best Single\n","    'LSTM-AE Baseline',      # Unsupervised Baseline\n","    'One-Class SVM',\n","    'Logistic Regression',\n","    'XGBoost',\n","    'MLP',\n","    'KNN',\n","    'Weighted Ensemble',\n","    'Stacked (XGB)'\n","]\n","\n","# Check which models effectively exist in your dictionaries\n","valid_models = []\n","for m in all_models_ordered:\n","    if m in best_models or m in test_probs_dict or m == 'LSTM-AE Baseline':\n","        valid_models.append(m)\n","\n","\n","# --- PLOT LOOP ---\n","for name in valid_models:\n","    style = model_styles.get(name, default_style)\n","\n","    # GET SCORES\n","    if name == 'LSTM-AE Baseline':\n","        scores = test_mae\n","    elif name == 'One-Class SVM':\n","        scores = -best_models[name].decision_function(X_test_feat)\n","    elif name in test_probs_dict:\n","        scores = test_probs_dict[name]\n","    elif name in best_models:\n","        scores = best_models[name].predict_proba(X_test_feat)[:, 1]\n","    else:\n","        continue # Skip if data missing\n","\n","    # CALCULATE METRICS\n","    fpr, tpr, _ = roc_curve(y_test, scores)\n","    roc_val = roc_auc_score(y_test, scores)\n","\n","    precision, recall, _ = precision_recall_curve(y_test, scores)\n","    pr_val = average_precision_score(y_test, scores)\n","\n","    # PLOT ROC\n","    ax1.plot(fpr, tpr,\n","             label=f'{name} (AUC={roc_val:.3f})',\n","             color=style['color'],\n","             linestyle=style['ls'],\n","             linewidth=style['lw'],\n","             zorder=style['zorder'],\n","             alpha=0.85)\n","\n","    # PLOT PR\n","    ax2.plot(recall, precision,\n","             label=f'{name} (AP={pr_val:.3f})',\n","             color=style['color'],\n","             linestyle=style['ls'],\n","             linewidth=style['lw'],\n","             zorder=style['zorder'],\n","             alpha=0.85)\n","\n","# --- FORMATTING AXIS 1 (ROC) ---\n","ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1.5) # Diagonal\n","ax1.set_xlim([-0.01, 1.01])\n","ax1.set_ylim([-0.01, 1.02])\n","ax1.set_xlabel('False Positive Rate (1 - Specificity)', fontweight='bold')\n","ax1.set_ylabel('True Positive Rate (Sensitivity)', fontweight='bold')\n","ax1.set_title('A. Receiver Operating Characteristic (ROC)', fontweight='bold', pad=15)\n","ax1.grid(True, alpha=0.2, linestyle='--')\n","# Legend: placed inside, but you can move to 'lower right' or outside\n","ax1.legend(loc=\"lower right\", frameon=True, framealpha=0.9, edgecolor='gray', fontsize=11)\n","\n","# --- FORMATTING AXIS 2 (PR) ---\n","ax2.set_xlim([-0.01, 1.01])\n","ax2.set_ylim([-0.01, 1.02])\n","ax2.set_xlabel('Recall (Sensitivity)', fontweight='bold')\n","ax2.set_ylabel('Precision (PPV)', fontweight='bold')\n","ax2.set_title('B. Precision-Recall (PR) Curves', fontweight='bold', pad=15)\n","ax2.grid(True, alpha=0.2, linestyle='--')\n","# Legend: placed 'lower left' or 'upper right' for PR usually\n","ax2.legend(loc=\"lower left\", frameon=True, framealpha=0.9, edgecolor='gray', fontsize=11)\n","\n","# --- SAVE & SHOW ---\n","plt.tight_layout()\n","plt.savefig('Figure_Combined_ROCPR_Analysis_Enhanced.pdf', dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"-CUc01nGfqVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 20. Generate Targeted ROC & PR Comparisons (Supervised vs. Unsupervised)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n","\n","# --- SHARED PLOTTING FUNCTION ---\n","def plot_rocp_comparison(model_list, title_suffix, filename_suffix, ax_roc, ax_pr):\n","    \"\"\"\n","    Plots ROC and PR curves for a specific list of models on given axes.\n","    \"\"\"\n","    # 1. STYLE DEFINITIONS\n","    styles = {\n","        'Stacked (LR)':        {'color': '#D62728', 'ls': '-',  'lw': 3.5, 'zorder': 100}, # Red (Proposed)\n","\n","        # Supervised\n","        'CNN-LSTM (SKD)':      {'color': '#1F77B4', 'ls': '-',  'lw': 2.5, 'zorder': 90},  # Dark Blue\n","        'SelectKBest + LSTM':  {'color': '#6BAED6', 'ls': '--', 'lw': 2.0, 'zorder': 80},  # Med Blue\n","        'Sparse AE + LSTM':    {'color': '#9ECAE1', 'ls': ':',  'lw': 2.0, 'zorder': 70},  # Light Blue\n","\n","        # Unsupervised\n","        'Local Outlier Factor':{'color': '#7F7F7F', 'ls': '-',  'lw': 2.5, 'zorder': 90},  # Dark Gray\n","        'Isolation Forest':    {'color': '#C7C7C7', 'ls': '--', 'lw': 2.0, 'zorder': 80},  # Light Gray\n","    }\n","    default_style = {'color': 'gray', 'ls': '-', 'lw': 1.0, 'zorder': 10}\n","\n","    # 2. PLOT LOOP\n","    for name in model_list:\n","        style = styles.get(name, default_style)\n","        scores = None\n","\n","        # --- SCORE RETRIEVAL LOGIC ---\n","        # All scores are now expected to be in test_probs_dict\n","        if name in test_probs_dict:\n","            scores = test_probs_dict[name]\n","\n","        if scores is None:\n","            print(f\"Warning: Scores for {name} not found.\")\n","            continue\n","\n","        # 3. CALCULATE METRICS\n","        fpr, tpr, _ = roc_curve(y_test, scores)\n","        roc_val = roc_auc_score(y_test, scores)\n","        precision, recall, _ = precision_recall_curve(y_test, scores)\n","        pr_val = average_precision_score(y_test, scores)\n","\n","        # 4. PLOT ROC\n","        ax_roc.plot(fpr, tpr, label=f'{name} (AUC={roc_val:.3f})',\n","                    color=style['color'], ls=style['ls'], lw=style['lw'], zorder=style['zorder'], alpha=0.9)\n","\n","        # 5. PLOT PR\n","        ax_pr.plot(recall, precision, label=f'{name} (AP={pr_val:.3f})',\n","                   color=style['color'], ls=style['ls'], lw=style['lw'], zorder=style['zorder'], alpha=0.9)\n","\n","    # --- FORMATTING ---\n","    # ROC Settings\n","    ax_roc.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Chance')\n","    ax_roc.set_xlabel('False Positive Rate', fontweight='bold')\n","    ax_roc.set_ylabel('True Positive Rate', fontweight='bold')\n","    ax_roc.set_title(f'ROC: {title_suffix}', fontweight='bold', pad=10)\n","    ax_roc.legend(loc=\"lower right\", frameon=True, framealpha=0.9, edgecolor='white')\n","    ax_roc.spines['top'].set_visible(False)\n","    ax_roc.spines['right'].set_visible(False)\n","    ax_roc.grid(True, alpha=0.2, linestyle='--')\n","\n","    # PR Settings\n","    prevalence = np.sum(y_test) / len(y_test)\n","    ax_pr.plot([0, 1], [prevalence, prevalence], 'k--', alpha=0.3, label=f'Base ({prevalence:.2f})')\n","    ax_pr.set_xlabel('Recall', fontweight='bold')\n","    ax_pr.set_ylabel('Precision', fontweight='bold')\n","    ax_pr.set_title(f'PR: {title_suffix}', fontweight='bold', pad=10)\n","    ax_pr.legend(loc=\"lower left\", frameon=True, framealpha=0.9, edgecolor='white')\n","    ax_pr.spines['top'].set_visible(False)\n","    ax_pr.spines['right'].set_visible(False)\n","    ax_pr.grid(True, alpha=0.2, linestyle='--')\n","\n","# --- CONFIGURATION ---\n","plt.rcParams.update({'font.family': 'serif', 'font.size': 12})\n","\n","# ==========================================\n","# FIGURE 1: DYNABIOME vs. SUPERVISED BASELINES\n","# ==========================================\n","fig1, (ax1a, ax1b) = plt.subplots(1, 2, figsize=(16, 7), dpi=600)\n","supervised_list = [\n","    'Stacked (LR)',          # Proposed\n","    'CNN-LSTM (SKD)',        # SOTA (named consistently)\n","    'SelectKBest + LSTM',\n","    'Sparse AE + LSTM'\n","]\n","plot_rocp_comparison(supervised_list, \"Dynabiome (Stacked LR) vs. Supervised Models\", \"Supervised\", ax1a, ax1b)\n","plt.tight_layout()\n","plt.savefig('Figure_Comparison_Supervised.pdf', bbox_inches='tight')\n","plt.show()\n","\n","# ==========================================\n","# FIGURE 2: DYNABIOME vs. UNSUPERVISED BASELINES\n","# ==========================================\n","fig2, (ax2a, ax2b) = plt.subplots(1, 2, figsize=(16, 7), dpi=600)\n","unsupervised_list = [\n","    'Stacked (LR)',          # Proposed\n","    'Local Outlier Factor',  # Best Unsupervised\n","    'Isolation Forest'\n","]\n","plot_rocp_comparison(unsupervised_list, \"Dynabiome (Stacked LR) vs. Unsupervised Models\", \"Unsupervised\", ax2a, ax2b)\n","plt.tight_layout()\n","plt.savefig('Figure_Comparison_Unsupervised.pdf', bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"QQvhScMbfMaz"},"execution_count":null,"outputs":[]}]}