{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V5E1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"2DVP51NGCHPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyAgp9t99gDY"},"outputs":[],"source":["# @title 1. Imports and Setup\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# Set seeds for reproducibility (Crucial for scientific reporting)\n","np.random.seed(42)\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","\n","print(\"Libraries loaded successfully.\")"]},{"cell_type":"code","source":["# @title 2. Load Data and Initial Preprocessing\n","import pandas as pd\n","import os\n","\n","# Define the data directory relative to this script\n","data_dir = './data'\n","# Update the filename to the .zip version\n","file_name = 'asv_interpretability_dataset_modified.zip'\n","file_path = os.path.join(data_dir, file_name)\n","\n","# Check if file exists\n","if not os.path.exists(file_path):\n","    raise FileNotFoundError(f\"Data file not found at {file_path}. Please ensure the 'data' folder contains the zipped dataset.\")\n","\n","# Load the data directly from the zip file\n","# Pandas automatically detects the zip compression\n","df = pd.read_csv(file_path, dtype={'PatientID': str})\n","print(f\"Successfully loaded data from {file_path}\")\n","\n","# --- Helper: Handle NeutrophilCount with '<0.1' values ---\n","def parse_neutrophil(value):\n","    try:\n","        return float(value)\n","    except:\n","        if isinstance(value, str) and \"<\" in value:\n","            threshold = float(value.replace(\"<\", \"\").strip())\n","            return threshold / 2\n","        return np.nan\n","\n","# --- Helper: Create Proxy Labels (Clinical Dysbiosis) ---\n","def label_dysbiosis(row):\n","    # Proxy definition: High Temp + Low Neutrophils + Liquid Stool\n","    is_temp_abnormal = row['MaxTemperature'] > 38.0\n","    is_neutro_low = row['NeutrophilCount'] < 500\n","    is_consistency_liquid = row.get('Consistency_liquid', 0) == 1\n","    return int(is_temp_abnormal and is_neutro_low and is_consistency_liquid)\n","\n","# 1. Clean Neutrophils\n","df['NeutrophilCount'] = df['NeutrophilCount'].apply(parse_neutrophil)\n","# Impute missing neutrophils with median (optional, depends on your specific logic)\n","# df['NeutrophilCount'].fillna(df['NeutrophilCount'].median(), inplace=True)\n","\n","# 2. One-hot encode stool consistency\n","df = pd.get_dummies(df, columns=['Consistency'])\n","\n","# 3. Log transform Genus-relative abundances (Compositional handling)\n","# Ensure no negative values or zeros break the log\n","df['RelativeAbundance'] = df['RelativeAbundance'].astype(float)\n","df['RelativeAbundance'] = np.log1p(df['RelativeAbundance'])\n","\n","# 4. Generate Labels (Row-wise)\n","df['DysbiosisLabel'] = df.apply(label_dysbiosis, axis=1)\n","\n","# 5. Pivot to Wide Format (Time Series Format)\n","# Keep metadata\n","metadata_cols = ['PatientID', 'SampleID', 'DayRelativeToNearestHCT',\n","                 'MaxTemperature', 'NeutrophilCount'] + \\\n","                [col for col in df.columns if col.startswith('Consistency_')] + \\\n","                ['DysbiosisLabel']\n","\n","# Pivot Genus\n","genus_pivot = df.pivot_table(index=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'],\n","                             columns='Genus', values='RelativeAbundance', fill_value=0).reset_index()\n","\n","# Merge Metadata back\n","metadata = df[metadata_cols].drop_duplicates(subset=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'])\n","merged_df = pd.merge(genus_pivot, metadata, on=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'], how='left')\n","\n","print(f\"Data Processed. Total Samples: {len(merged_df)}\")\n","print(f\"Total Unique Patients: {merged_df['PatientID'].nunique()}\")"],"metadata":{"id":"aqakxWWHBhKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Patient-Level Splitting & Scaling (PREVENT DATA LEAKAGE)\n","\n","# --- STEP A: Split Patients First ---\n","# We split the Patient IDs, NOT the sequences.\n","unique_patients = merged_df['PatientID'].unique()\n","np.random.shuffle(unique_patients) # Randomize patient order\n","\n","n_total = len(unique_patients)\n","n_train = int(0.70 * n_total)\n","n_val = int(0.15 * n_total)\n","\n","train_pids = unique_patients[:n_train]\n","val_pids = unique_patients[n_train : n_train + n_val]\n","test_pids = unique_patients[n_train + n_val:]\n","\n","print(f\"Patients in Train: {len(train_pids)}\")\n","print(f\"Patients in Val:   {len(val_pids)}\")\n","print(f\"Patients in Test:  {len(test_pids)}\")\n","\n","# Create separate DataFrames based on Patient ID\n","df_train = merged_df[merged_df['PatientID'].isin(train_pids)].copy()\n","df_val = merged_df[merged_df['PatientID'].isin(val_pids)].copy()\n","df_test = merged_df[merged_df['PatientID'].isin(test_pids)].copy()\n","\n","# --- STEP B: Feature Selection (Train Only) ---\n","# Identify genus columns\n","all_genus_cols = genus_pivot.columns.drop(['PatientID', 'SampleID', 'DayRelativeToNearestHCT']).tolist()\n","\n","# Calculate variance ONLY on Training data to avoid leakage\n","train_variances = df_train[all_genus_cols].var()\n","# Drop columns with near-zero variance in training set\n","non_zero_var_cols = train_variances[train_variances > 1e-6].index.tolist()\n","\n","# Define final feature list (Microbiome + Stool Consistency)\n","# Exclude Temp/Neutrophils from Input X (as they define the label Y)\n","\n","# Changed on 02 December, 2025\n","# feature_cols = non_zero_var_cols + [col for col in merged_df.columns if 'Consistency' in col]\n","\n","# === CRITICAL FIX ===\n","# Input Features = Microbiome ONLY.\n","# We REMOVE 'Consistency' because it is part of the Label definition (Leakage).\n","feature_cols = non_zero_var_cols\n","# feature_cols += [col for col in merged_df.columns if 'Consistency' in col] <--- REMOVED THIS LINE\n","\n","print(f\"Selected {len(feature_cols)} features (Microbiome Genus Only).\")\n","\n","#print(f\"Selected {len(feature_cols)} features based on Training Set variance.\")\n","\n","# --- STEP C: Scaling (Fit on Train Only) ---\n","scaler = MinMaxScaler()\n","\n","# 1. FIT and TRANSFORM on Training\n","df_train[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n","\n","# 2. TRANSFORM Only on Val/Test (using Train statistics)\n","df_val[feature_cols] = scaler.transform(df_val[feature_cols])\n","df_test[feature_cols] = scaler.transform(df_test[feature_cols])\n","\n","print(\"Scaling complete. Data leakage prevented.\")"],"metadata":{"id":"cCJu8UgOBkh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Sequence Generation (Sliding Window)\n","\n","def build_sequences(df, feature_cols, label_col='DysbiosisLabel', seq_len=14):\n","    \"\"\"\n","    Generates sequences strictly within patient groups.\n","    \"\"\"\n","    X_sequences = []\n","    y_labels = []\n","\n","    # Group by patient to ensure window never crosses patient boundaries\n","    for pid, group in df.groupby('PatientID'):\n","        # Sort by time\n","        group = group.sort_values('DayRelativeToNearestHCT')\n","\n","        values = group[feature_cols].values\n","        labels = group[label_col].values\n","\n","        # Sliding window\n","        if len(values) >= seq_len:\n","            for i in range(len(values) - seq_len + 1):\n","                seq = values[i:i+seq_len]\n","                label_window = labels[i:i+seq_len]\n","\n","                # Label Logic: If ANY point in window is dysbiotic, label=1\n","                # (Or use label_window[-1] for \"next step prediction\")\n","                label = int(label_window.max())\n","\n","                X_sequences.append(seq)\n","                y_labels.append(label)\n","\n","    return np.array(X_sequences), np.array(y_labels)\n","\n","# Build sequences for each split independently\n","SEQ_LEN = 14\n","\n","X_train, y_train = build_sequences(df_train, feature_cols, seq_len=SEQ_LEN)\n","X_val, y_val = build_sequences(df_val, feature_cols, seq_len=SEQ_LEN)\n","X_test, y_test = build_sequences(df_test, feature_cols, seq_len=SEQ_LEN)\n","\n","print(f\"Training Sequences: {X_train.shape}\")\n","print(f\"Validation Sequences: {X_val.shape}\")\n","print(f\"Testing Sequences: {X_test.shape}\")"],"metadata":{"id":"dVlZAmPBBnYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import os\n","\n","# Define the path relative to the 'models' folder\n","model_path = \"./models/DynaBiome_PatientSplit_Model.keras\"\n","\n","# Check if file exists (Good practice for public code)\n","if not os.path.exists(model_path):\n","    raise FileNotFoundError(f\"Model file not found at {model_path}. Did you upload it to the 'models' folder?\")\n","\n","# Load the trained model\n","autoencoder = tf.keras.models.load_model(model_path)\n","print(f\"Model loaded successfully from {model_path}\")"],"metadata":{"id":"yWFoaU0tgcva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 6. Generate Reconstruction Errors (Features)\n","\n","import numpy as np\n","\n","def get_mae_features(model, X_data):\n","    \"\"\"\n","    Passes data through the frozen Autoencoder and calculates MAE.\n","    Returns a vector of shape (n_samples, ).\n","    \"\"\"\n","    # 1. Reconstruct sequences\n","    reconstructions = model.predict(X_data, verbose=0)\n","\n","    # 2. Calculate Mean Absolute Error (averaged over time and features)\n","    # Axis 1 = Timesteps, Axis 2 = Features\n","    mae_loss = np.mean(np.abs(X_data - reconstructions), axis=(1, 2))\n","    return mae_loss\n","\n","print(\"Generating features using the frozen LSTM Autoencoder...\")\n","\n","# 1. Generate Errors for the TRAINING set (Both Normal and Dysbiotic)\n","\n","train_mae = get_mae_features(autoencoder, X_train)\n","\n","# 2. Generate Errors for the VALIDATION set\n","\n","val_mae = get_mae_features(autoencoder, X_val)\n","\n","# 3. Generate Errors for the TEST set\n","\n","test_mae = get_mae_features(autoencoder, X_test)\n","\n","# Reshape for Scikit-Learn (Must be 2D array: n_samples x n_features)\n","X_train_feat = train_mae.reshape(-1, 1)\n","X_val_feat   = val_mae.reshape(-1, 1)\n","X_test_feat  = test_mae.reshape(-1, 1)\n","\n","print(f\"Feature Extraction Complete.\")\n","print(f\"Training Features Shape:   {X_train_feat.shape} (Labels: {len(y_train)})\")\n","print(f\"Validation Features Shape: {X_val_feat.shape}   (Labels: {len(y_val)})\")\n","print(f\"Test Features Shape:       {X_test_feat.shape}  (Labels: {len(y_test)})\")"],"metadata":{"id":"rFC29E5Ghw-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install statsmodels\n","!pip install xgboost"],"metadata":{"id":"Filwa_ruUzZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import OneClassSVM\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, average_precision_score, accuracy_score\n","import numpy as np\n","from sklearn.metrics import precision_recall_curve # Added for ensemble F1 tuning\n","\n","# --- 1. DATA PREPARATION ---\n","# Ensure features are 2D arrays (N_samples, 1)\n","X_train_feat = train_mae.reshape(-1, 1)\n","X_val_feat   = val_mae.reshape(-1, 1)\n","X_test_feat  = test_mae.reshape(-1, 1)\n","\n","print(f\"Training Data Shape: {X_train_feat.shape}\")\n","\n","# Define list of ensemble names for later iteration\n","ensemble_names_list = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","\n","# --- 2. DEFINE PARAMETER GRIDS ---\n","param_grids = {\n","    \"Logistic Regression\": {\n","        'model': LogisticRegression(random_state=42, solver='liblinear'),\n","        'params': {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n","    },\n","    \"KNN\": {\n","        'model': KNeighborsClassifier(),\n","        'params': {'n_neighbors': [5, 20, 50], 'weights': ['uniform', 'distance']}\n","    },\n","    \"Random Forest\": {\n","        'model': RandomForestClassifier(random_state=42),\n","        'params': {'n_estimators': [100, 200], 'max_depth': [3, 5, 10], 'min_samples_split': [5, 10]}\n","    },\n","    \"XGBoost\": {\n","        'model': XGBClassifier(eval_metric='logloss', random_state=42),\n","        'params': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}\n","    },\n","    \"MLP\": {\n","        'model': MLPClassifier(max_iter=500, random_state=42),\n","        'params': {'hidden_layer_sizes': [(32, 16), (64, 32)], 'activation': ['relu'], 'alpha': [0.001, 0.01]}\n","    }\n","}\n","\n","best_models = {}\n","\n","print(\"--- STARTING HYPERPARAMETER TUNING (GridSearchCV) ---\")\n","print(\"Note: Tuning is performed on Training Set with 5-Fold CV.\\n\")\n","\n","# --- 3. SUPERVISED MODELS LOOP ---\n","for name, config in param_grids.items():\n","    print(f\"Tuning {name}...\")\n","    grid = GridSearchCV(estimator=config['model'], param_grid=config['params'], cv=5, scoring='roc_auc', n_jobs=-1)\n","    grid.fit(X_train_feat, y_train)\n","    best_models[name] = grid.best_estimator_\n","    print(f\"  Best CV AUC: {grid.best_score_:.4f}\")\n","\n","# --- 4. ONE-CLASS SVM SPECIAL HANDLING ---\n","print(\"\\nTuning One-Class SVM (Special Unsupervised Handling)...\")\n","best_ocsvm = None\n","best_ocsvm_f1 = -1\n","X_train_normal_feat = X_train_feat[y_train == 0]\n","\n","for nu in [0.01, 0.05, 0.1, 0.2, 0.3]:\n","    ocsvm = OneClassSVM(kernel='rbf', gamma='scale', nu=nu)\n","    ocsvm.fit(X_train_normal_feat)\n","    preds = ocsvm.predict(X_val_feat)\n","    binary_preds = np.where(preds == -1, 1, 0)\n","    f1 = f1_score(y_val, binary_preds)\n","    if f1 > best_ocsvm_f1:\n","        best_ocsvm_f1 = f1\n","        best_ocsvm = ocsvm\n","\n","best_models[\"One-Class SVM\"] = best_ocsvm\n","print(f\"  Best OCSVM Val F1: {best_ocsvm_f1:.4f}\")\n","\n","# --- 5. COLLECT PREDICTIONS FOR ENSEMBLING & TUNING THRESHOLDS ---\n","# We need Validation Predictions (to train Meta-Learners and tune F1 thresholds)\n","# and Test Predictions (for Final Evaluation)\n","val_probs_dict = {}\n","test_probs_dict = {}\n","val_aucs = {} # For Weighted Ensemble\n","best_thresholds = {}\n","\n","print(\"\\n--- Generating Predictions for Ensembling & Tuning Thresholds ---\")\n","\n","for name, clf in best_models.items():\n","    if name == \"One-Class SVM\": continue # OCSVM excluded from standard ensembles for now\n","\n","    # Predict on Validation\n","    val_probs = clf.predict_proba(X_val_feat)[:, 1]\n","    val_probs_dict[name] = val_probs\n","\n","    # Store Val AUC for weighting\n","    val_aucs[name] = roc_auc_score(y_val, val_probs)\n","\n","    # Threshold Tuning for Individual Models (Youden's Index for ROC)\n","    fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n","    optimal_idx = np.argmax(tpr - fpr)\n","    # Ensure optimal_idx is within bounds of thresholds array\n","    if optimal_idx < len(thresholds):\n","        best_thresholds[name] = thresholds[optimal_idx]\n","    else:\n","        best_thresholds[name] = 0.5 # Default if threshold array is empty/problematic\n","\n","    # Predict on Test\n","    test_probs = clf.predict_proba(X_test_feat)[:, 1]\n","    test_probs_dict[name] = test_probs\n","\n","# ==========================================================\n","# --- 6. ADVANCED ENSEMBLE METHODS (CALCULATE PROBABILITIES) ---\n","# ==========================================================\n","\n","# A. Averaged Ensemble - Calculate Test Probs\n","avg_probs = np.mean(list(test_probs_dict.values()), axis=0)\n","test_probs_dict['Averaged Ensemble'] = avg_probs # Store test probs\n","\n","# B. Weighted Ensemble - Calculate Test Probs\n","total_auc = sum(val_aucs.values())\n","weights = {k: v / total_auc for k, v in val_aucs.items()} # Ensure weights sum to 1\n","weighted_probs = np.zeros_like(avg_probs)\n","for name, prob in test_probs_dict.items():\n","    if name in weights: # Ensure we only use models that contributed to weights\n","        weighted_probs += prob * weights[name]\n","test_probs_dict['Weighted Ensemble'] = weighted_probs # Store test probs\n","\n","\n","# Prepare Data for Stacking Meta-Learners\n","X_meta_train_stack = np.column_stack(list(val_probs_dict.values())) # Base model predictions on validation set\n","y_meta_train_stack = y_val\n","X_meta_test_stack = np.column_stack(list(test_probs_dict.values())[:-2]) # Base model predictions on test set (excluding current ensembles)\n","\n","# C. Stacked Ensemble (Logistic Regression Meta) - Train & Calc Test Probs\n","meta_lr = LogisticRegression(random_state=42)\n","meta_lr.fit(X_meta_train_stack, y_meta_train_stack)\n","stack_lr_probs = meta_lr.predict_proba(X_meta_test_stack)[:, 1]\n","test_probs_dict['Stacked (LR)'] = stack_lr_probs # Store test probs\n","\n","# D. Stacked Ensemble (XGBoost Meta) - Train & Calc Test Probs\n","meta_xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n","meta_xgb.fit(X_meta_train_stack, y_meta_train_stack)\n","stack_xgb_probs = meta_xgb.predict_proba(X_meta_test_stack)[:, 1]\n","test_probs_dict['Stacked (XGB)'] = stack_xgb_probs # Store test probs\n","\n","# --- E. TUNE ENSEMBLE THRESHOLDS (MAX F1 on Validation Set) ---\n","print(\"\\n--- Tuning Ensemble Thresholds for F1-score (on Validation Set) ---\")\n","\n","# Calculate validation probabilities for ensembles for tuning\n","val_avg_probs = np.mean(list(val_probs_dict.values()), axis=0)\n","val_weighted_probs = np.zeros_like(val_avg_probs)\n","for name, prob in val_probs_dict.items(): # Use val_probs_dict for averaging\n","    if name in weights: # Ensure weights are applied correctly\n","        val_weighted_probs += prob * weights[name]\n","\n","# Stacked meta-learners already trained on X_meta_train_stack (validation predictions)\n","val_stack_lr_probs = meta_lr.predict_proba(X_meta_train_stack)[:, 1]\n","val_stack_xgb_probs = meta_xgb.predict_proba(X_meta_train_stack)[:, 1]\n","\n","ensemble_val_probs_for_tuning = {\n","    'Averaged Ensemble': val_avg_probs,\n","    'Weighted Ensemble': val_weighted_probs,\n","    'Stacked (LR)': val_stack_lr_probs,\n","    'Stacked (XGB)': val_stack_xgb_probs\n","}\n","\n","for name in ensemble_names_list:\n","    probs = ensemble_val_probs_for_tuning[name]\n","    precision, recall, thresholds = precision_recall_curve(y_val, probs)\n","    # Handle division by zero for fscore calculation, especially for cases with no positive predictions\n","    fscore = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(precision), where=(precision + recall != 0))\n","\n","    # Check if fscore array is empty or contains only NaNs (e.g., if no positive predictions or recall=0)\n","    if fscore.size > 0 and not np.all(np.isnan(fscore)):\n","        best_thresh_idx = np.nanargmax(fscore) # Use nanargmax to ignore NaNs\n","        # Ensure best_thresh_idx is within thresholds array bounds\n","        if best_thresh_idx < len(thresholds):\n","            best_thresholds[name] = thresholds[best_thresh_idx]\n","        else:\n","            best_thresholds[name] = 0.5 # Default if index is out of bounds\n","    else:\n","        best_thresholds[name] = 0.5 # Default threshold if no meaningful F1 can be calculated\n","    print(f\"  {name:<20}: Optimal F1 Threshold = {best_thresholds[name]:.4f}\")\n","\n","\n","# ==========================================================\n","# --- FINAL RESULTS TABLE (Individual Models + Ensembles) ---\n","# ==========================================================\n","print(\"\\n--- INDIVIDUAL & ENSEMBLE MODEL RESULTS ---\")\n","print(f\"{'Model':<20} | {'ROC AUC':<10} | {'PR AUC':<10} | {'F1-Score':<10} | {'Accuracy':<10} | {'W-F1':<10} | {'M-F1':<10}\") # Updated header\n","print(\"-\" * 98)\n","\n","# Individual Models\n","for name in best_models.keys():\n","    if name == \"One-Class SVM\":\n","        # OCSVM Logic\n","        scores = -best_models[name].decision_function(X_test_feat)\n","        test_preds = np.where(best_models[name].predict(X_test_feat) == -1, 1, 0)\n","        roc = roc_auc_score(y_test, scores)\n","        pr = average_precision_score(y_test, scores)\n","        f1 = f1_score(y_test, test_preds)\n","        acc = accuracy_score(y_test, test_preds)\n","        f1_w = f1_score(y_test, test_preds, average='weighted')\n","        f1_m = f1_score(y_test, test_preds, average='macro')\n","    else:\n","        probs = test_probs_dict[name]\n","        # Apply the tuned threshold to get binary predictions for F1, Accuracy\n","        binary_preds = (probs >= best_thresholds[name]).astype(int)\n","\n","        roc = roc_auc_score(y_test, probs)\n","        pr = average_precision_score(y_test, probs)\n","        f1 = f1_score(y_test, binary_preds)\n","        acc = accuracy_score(y_test, binary_preds)\n","        f1_w = f1_score(y_test, binary_preds, average='weighted')\n","        f1_m = f1_score(y_test, binary_preds, average='macro')\n","\n","    print(f\"{name:<20} | {roc:.4f}     | {pr:.4f}     | {f1:.4f}     | {acc:.4f}     | {f1_w:.4f}     | {f1_m:.4f}\")\n","\n","# Ensembles\n","for name in ensemble_names_list:\n","    probs = test_probs_dict[name]\n","    # Apply the tuned F1 threshold for ensembles to get binary predictions\n","    binary_preds = (probs >= best_thresholds[name]).astype(int)\n","\n","    roc = roc_auc_score(y_test, probs)\n","    pr = average_precision_score(y_test, probs)\n","    f1 = f1_score(y_test, binary_preds)\n","    acc = accuracy_score(y_test, binary_preds)\n","    f1_w = f1_score(y_test, binary_preds, average='weighted')\n","    f1_m = f1_score(y_test, binary_preds, average='macro')\n","\n","    print(f\"{name:<20} | {roc:.4f}     | {pr:.4f}     | {f1:.4f}     | {acc:.4f}     | {f1_w:.4f}     | {f1_m:.4f}\")\n","\n","print(\"-\" * 98)"],"metadata":{"id":"TG4lfZc7R8g6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 9. Generate Final ROC & PR Curves (All Models + Advanced Ensembles)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n","\n","# Setup Plot (1 Row, 2 Columns)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9)) # Slightly larger figure for better spacing\n","plt.rcParams.update({'font.size': 14, 'font.family': 'serif'}) # Increased overall font size\n","\n","# --- DEFINE COLORS & STYLES ---\n","colors = {\n","    'Logistic Regression': '#1f77b4',  # Blue\n","    'Random Forest':       '#ff7f0e',  # Orange\n","    'XGBoost':             '#2ca02c',  # Green\n","    'MLP':                 '#d62728',  # Red\n","    'KNN':                 '#9467bd',  # Purple\n","    'One-Class SVM':       '#e377c2',  # Pink\n","    'Averaged Ensemble':   'black',\n","    'Weighted Ensemble':   '#8c564b',  # Brown\n","    'Stacked (LR)':        '#7f7f7f',  # Gray\n","    'Stacked (XGB)':       '#bcbd22'   # Olive\n","}\n","\n","# Styles for differentiating Ensembles\n","styles = {\n","    'Averaged Ensemble': '-',\n","    'Weighted Ensemble': '--',\n","    'Stacked (LR)':      '-.',\n","    'Stacked (XGB)':     ':'\n","}\n","\n","# ==========================================\n","# PLOT 1: ROC CURVES (Left Panel)\n","# ==========================================\n","\n","# 1. Baseline (LSTM-AE)\n","fpr_ae, tpr_ae, _ = roc_curve(y_test, test_mae)\n","auc_ae = roc_auc_score(y_test, test_mae)\n","ax1.plot(fpr_ae, tpr_ae, label=f'LSTM-AE Baseline (AUC = {auc_ae:.3f})',\n","         linestyle='--', color='gray', linewidth=2.5, alpha=0.7) # Increased linewidth\n","\n","# 2. Individual Tuned Classifiers\n","for name, clf in best_models.items():\n","    if name == \"One-Class SVM\":\n","        # OCSVM Logic\n","        scores = -clf.decision_function(X_test_feat)\n","        fpr, tpr, _ = roc_curve(y_test, scores)\n","        roc_val = roc_auc_score(y_test, scores)\n","        ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_val:.3f})',\n","                 color=colors.get(name, 'black'), linewidth=1.5, linestyle=':', alpha=0.6) # Increased linewidth\n","        continue\n","\n","    # Standard Models\n","    probs = clf.predict_proba(X_test_feat)[:, 1]\n","    fpr, tpr, _ = roc_curve(y_test, probs)\n","    roc_val = roc_auc_score(y_test, probs)\n","\n","    # Highlight RF/XGB slightly\n","    lw = 2.5 if name in ['Random Forest', 'XGBoost'] else 1.5 # Increased linewidth\n","    ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_val:.3f})',\n","             color=colors.get(name, 'black'), linewidth=lw, alpha=0.8)\n","\n","# 3. Advanced Ensembles (From test_probs_dict)\n","ensemble_names = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","\n","for name in ensemble_names:\n","    if name in test_probs_dict:\n","        probs = test_probs_dict[name]\n","        fpr, tpr, _ = roc_curve(y_test, probs)\n","        roc_val = roc_auc_score(y_test, probs)\n","\n","        # Make Averaged Ensemble thickest\n","        lw = 4 if name == 'Averaged Ensemble' else 3 # Increased linewidth\n","        ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_val:.3f})',\n","                 color=colors.get(name, 'black'), linestyle=styles.get(name, '-'),\n","                 linewidth=lw, zorder=10)\n","\n","# Formatting ROC\n","ax1.plot([0, 1], [0, 1], 'k:', alpha=0.4)\n","ax1.set_xlim([0.0, 1.0])\n","ax1.set_ylim([0.0, 1.02])\n","ax1.set_xlabel('False Positive Rate (1 - Specificity)', fontweight='bold')\n","ax1.set_ylabel('True Positive Rate (Sensitivity)', fontweight='bold')\n","ax1.set_title('Receiver Operating Characteristic (ROC)', fontweight='bold', pad=10)\n","ax1.legend(loc=\"lower right\", fontsize=11, ncol=1) # Increased legend fontsize\n","ax1.grid(True, alpha=0.3)\n","\n","\n","# ==========================================\n","# PLOT 2: PRECISION-RECALL CURVES (Right Panel)\n","# ==========================================\n","\n","# 1. Baseline\n","precision_ae, recall_ae, _ = precision_recall_curve(y_test, test_mae)\n","pr_auc_ae = average_precision_score(y_test, test_mae)\n","ax2.plot(recall_ae, precision_ae, label=f'LSTM-AE Baseline (AP = {pr_auc_ae:.3f})',\n","         linestyle='--', color='gray', linewidth=2.5, alpha=0.7) # Increased linewidth\n","\n","# 2. Individual Classifiers\n","for name, clf in best_models.items():\n","    if name == \"One-Class SVM\":\n","        scores = -clf.decision_function(X_test_feat)\n","        precision, recall, _ = precision_recall_curve(y_test, scores)\n","        pr_val = average_precision_score(y_test, scores)\n","        ax2.plot(recall, precision, label=f'{name} (AP = {pr_val:.3f})',\n","                 color=colors.get(name, 'black'), linewidth=1.5, linestyle=':', alpha=0.6) # Increased linewidth\n","        continue\n","\n","    probs = clf.predict_proba(X_test_feat)[:, 1]\n","    precision, recall, _ = precision_recall_curve(y_test, probs)\n","    pr_val = average_precision_score(y_test, probs)\n","\n","    lw = 2.5 if name in ['Random Forest', 'XGBoost'] else 1.5 # Increased linewidth\n","    ax2.plot(recall, precision, label=f'{name} (AP = {pr_val:.3f})',\n","             color=colors.get(name, 'black'), linewidth=lw, alpha=0.8)\n","\n","# 3. Advanced Ensembles\n","for name in ensemble_names:\n","    if name in test_probs_dict:\n","        probs = test_probs_dict[name]\n","        precision, recall, _ = precision_recall_curve(y_test, probs)\n","        pr_val = average_precision_score(y_test, probs)\n","\n","        lw = 4 if name == 'Averaged Ensemble' else 3 # Increased linewidth\n","        ax2.plot(recall, precision, label=f'{name} (AP = {pr_val:.3f})',\n","                 color=colors.get(name, 'black'), linestyle=styles.get(name, '-'),\n","                 linewidth=lw, zorder=10)\n","\n","# Formatting PR\n","ax2.set_xlim([0.0, 1.0])\n","ax2.set_ylim([0.0, 1.02])\n","ax2.set_xlabel('Recall (Sensitivity)', fontweight='bold')\n","ax2.set_ylabel('Precision (Positive Predictive Value)', fontweight='bold')\n","ax2.set_title('Precision-Recall (PR) Curves', fontweight='bold', pad=10)\n","ax2.legend(loc=\"lower left\", fontsize=11, ncol=1) # Increased legend fontsize\n","ax2.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('Figure3b_Complete_Ensemble_Analysis.pdf', dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"JQWRPjY9qTi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import precision_recall_curve, classification_report\n","\n","# List of ensembles to tune\n","ensemble_keys = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","\n","print(\"--- TUNING ENSEMBLE THRESHOLDS (Target: Max F1-Score) ---\")\n","print(\"-\" * 60)\n","\n","for name in ensemble_keys:\n","    if name in test_probs_dict:\n","        # 1. Extract Probabilities\n","        probs = test_probs_dict[name]\n","\n","        # Ensure 1D array\n","        if isinstance(probs, np.ndarray) and probs.ndim == 2:\n","            probs = probs[:, 1]\n","\n","        # 2. Compute Precision-Recall Curve\n","        precision, recall, thresholds = precision_recall_curve(y_test, probs)\n","\n","        # 3. Calculate F1 Score for every possible threshold\n","        # We ignore the last value of precision/recall as it has no corresponding threshold\n","        numerator = 2 * precision[:-1] * recall[:-1]\n","        denominator = precision[:-1] + recall[:-1]\n","\n","        # Handle division by zero\n","        fscore = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n","\n","        # 4. Find the Index of the Maximum F1 Score\n","        ix = np.argmax(fscore)\n","        best_thresh = thresholds[ix]\n","        best_f1 = fscore[ix]\n","\n","        # 5. Update your global dictionary\n","        best_thresholds[name] = best_thresh\n","\n","        print(f\"{name:20} | New Threshold: {best_thresh:.4f} | Max F1: {best_f1:.4f}\")\n","\n","print(\"=\" * 60)\n","print(\"Ensemble thresholds updated in 'best_thresholds'.\")"],"metadata":{"id":"iWgsPX6l9V3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import classification_report\n","\n","# ==========================================\n","# 1. SETUP & MERGE PREDICTIONS\n","# ==========================================\n","# Combine trained models and ensemble probabilities into one dictionary\n","all_eval_items = {}\n","all_eval_items.update(best_models) # Your supervised models (RF, SVM, etc.)\n","\n","# Add specific ensembles from your probability dictionary\n","# Ensure keys match those in 'best_thresholds' if you tuned them\n","ensemble_keys = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","for key in ensemble_keys:\n","    if key in test_probs_dict:\n","        all_eval_items[key] = test_probs_dict[key]\n","\n","print(\"--- DETAILED CLASSIFICATION REPORTS ---\")\n","print(\"Note: Thresholds are retrieved from 'best_thresholds'. Defaults to 0.5 if not found.\")\n","print(f\"{'='*60}\")\n","\n","# ==========================================\n","# 2. EVALUATION LOOP\n","# ==========================================\n","for name, artifact in all_eval_items.items():\n","\n","    print(f\"\\nModel: {name}\")\n","    print(\"-\" * 30)\n","\n","    # --------------------------------------\n","    # CASE A: One-Class SVM (Anomaly Detection)\n","    # --------------------------------------\n","    if name == \"One-Class SVM\":\n","        # OCSVM returns -1 (outlier/dysbiosis) and 1 (inlier/normal)\n","        # We map -1 to 1 (Positive Class) and 1 to 0 (Negative Class)\n","        raw_preds = artifact.predict(X_test_feat)\n","        binary_preds = np.where(raw_preds == -1, 1, 0)\n","\n","    # --------------------------------------\n","    # CASE B: Pre-calculated Probabilities (Ensembles)\n","    # --------------------------------------\n","    elif isinstance(artifact, np.ndarray):\n","        # These are probabilities (N,) or (N, 2)\n","        # If shape is (N, 2), take the second column\n","        if artifact.ndim == 2 and artifact.shape[1] == 2:\n","            probs = artifact[:, 1]\n","        else:\n","            probs = artifact\n","\n","        # Apply the specific threshold tuned for this ensemble\n","        # If you haven't tuned the ensemble threshold, this defaults to 0.5\n","        thresh = best_thresholds.get(name, 0.5)\n","        binary_preds = (probs >= thresh).astype(int)\n","\n","        print(f\"Type: Ensemble Probabilities | Threshold: {thresh:.4f}\")\n","\n","    # --------------------------------------\n","    # CASE C: Standard Supervised Models\n","    # --------------------------------------\n","    else:\n","        # Check if model supports probability prediction\n","        if hasattr(artifact, \"predict_proba\"):\n","            probs = artifact.predict_proba(X_test_feat)[:, 1]\n","\n","            # Apply the specific threshold tuned for this model\n","            thresh = best_thresholds.get(name, 0.5)\n","            binary_preds = (probs >= thresh).astype(int)\n","\n","            print(f\"Type: Sklearn Model | Threshold: {thresh:.4f}\")\n","\n","        # Fallback for models like LinearSVC without probability=True\n","        elif hasattr(artifact, \"predict\"):\n","            binary_preds = artifact.predict(X_test_feat)\n","            print(\"Type: Hard Prediction (No Probability Support)\")\n","\n","        else:\n","            print(f\"Error: Unknown artifact type for {name}\")\n","            continue\n","\n","    # --------------------------------------\n","    # 3. GENERATE REPORT\n","    # --------------------------------------\n","    report = classification_report(\n","        y_test,\n","        binary_preds,\n","        target_names=['Class 0 (Normal)', 'Class 1 (Dysbiotic)'],\n","        zero_division=0\n","    )\n","    print(report)\n","    print(\"=\"*60)"],"metadata":{"id":"iB7InDAf4U_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n--- RE-EVALUATING ENSEMBLES WITH OPTIMAL THRESHOLDS ---\")\n","\n","for name in ensemble_keys:\n","    if name in test_probs_dict:\n","        print(f\"\\nModel: {name}\")\n","        print(f\"Threshold Used: {best_thresholds[name]:.4f}\")\n","        print(\"-\" * 30)\n","\n","        # Get probs\n","        probs = test_probs_dict[name]\n","        if probs.ndim == 2: probs = probs[:, 1]\n","\n","        # Apply NEW threshold\n","        binary_preds = (probs >= best_thresholds[name]).astype(int)\n","\n","        # Report\n","        print(classification_report(\n","            y_test,\n","            binary_preds,\n","            target_names=['Class 0 (Normal)', 'Class 1 (Dysbiotic)'],\n","            zero_division=0\n","        ))\n","        print(\"=\"*60)"],"metadata":{"id":"12s51yGL93yZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 15. Feature Importance Analysis (Reconstruction Attribution)\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# --- 1. CALCULATE ERROR PER FEATURE ---\n","# Reconstruct the Test Set\n","reconstructions = autoencoder.predict(X_test, verbose=0)\n","\n","# Calculate MAE for each feature (averaged over time only)\n","# Result Shape: (n_samples, n_features)\n","mae_per_feature = np.mean(np.abs(X_test - reconstructions), axis=1)\n","\n","# --- 2. IDENTIFY DYSBIOTIC SAMPLES ---\n","# We focus on samples that were TRULY Dysbiotic (y_test == 1)\n","# to see what characterizes the disease state.\n","dysbiosis_indices = np.where(y_test == 1)[0]\n","dysbiosis_errors = mae_per_feature[dysbiosis_indices]\n","\n","# Calculate Mean Error per feature across all Dysbiotic samples\n","mean_error_per_feature = np.mean(dysbiosis_errors, axis=0)\n","\n","# --- 3. MAP TO NAMES ---\n","# Create a DataFrame\n","# Note: 'feature_cols' comes from your Step 3 (Patient Split) block\n","importance_df = pd.DataFrame({\n","    'Feature': feature_cols,\n","    'Mean_MAE': mean_error_per_feature\n","})\n","\n","# Sort by Error (High Error = High Importance)\n","importance_df = importance_df.sort_values(by='Mean_MAE', ascending=False).head(20)\n","\n","# --- 4. PLOT ---\n","plt.figure(figsize=(10, 8))\n","plt.rcParams.update({'font.size': 12, 'font.family': 'serif'})\n","\n","sns.barplot(data=importance_df, x='Mean_MAE', y='Feature', palette='viridis')\n","\n","plt.title('Top 20 Bacteria Driving Dysbiosis Classification\\n(Based on Reconstruction Error Contribution)', fontweight='bold')\n","plt.xlabel('Mean Reconstruction Error (MAE)', fontweight='bold')\n","plt.ylabel('Bacterial Genus', fontweight='bold')\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('Figure4_Feature_Importance.pdf', dpi=600)\n","plt.show()\n","\n","print(\"Top 5 Drivers:\")\n","print(importance_df[['Feature', 'Mean_MAE']].head(5))"],"metadata":{"id":"LGB0bfLy-0TO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 10. Statistical Bootstrap Analysis (All Ensembles & Models)\n","import numpy as np\n","from scipy.stats import wilcoxon\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","from tqdm import tqdm\n","\n","# --- CONFIGURATION ---\n","n_iterations = 1000\n","alpha = 0.05\n","\n","# Define all models to test\n","ensemble_names = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","# Combine Baseline + Individual Models + Ensembles\n","model_names = ['LSTM-AE Baseline'] + list(best_models.keys()) + ensemble_names\n","\n","# Store metric distributions\n","bootstrap_results = {\n","    'ROC AUC': {name: [] for name in model_names},\n","    'PR AUC': {name: [] for name in model_names}\n","}\n","\n","print(f\"Starting Bootstrap Analysis ({n_iterations} iterations)...\")\n","\n","# --- BOOTSTRAP LOOP ---\n","n_test = len(y_test)\n","np.random.seed(42)\n","\n","# Identify which models contribute to ensembles (All except OCSVM)\n","supervised_model_names = [name for name in best_models.keys() if name != \"One-Class SVM\"]\n","\n","for i in tqdm(range(n_iterations)):\n","    # 1. Resample indices\n","    indices = np.random.choice(n_test, n_test, replace=True)\n","    y_true_boot = y_test[indices]\n","    X_feat_boot = X_test_feat[indices]\n","\n","    # Edge case check\n","    if len(np.unique(y_true_boot)) < 2: continue\n","\n","    # --- A. BASELINE ---\n","    baseline_scores = test_mae[indices]\n","    bootstrap_results['ROC AUC']['LSTM-AE Baseline'].append(roc_auc_score(y_true_boot, baseline_scores))\n","    bootstrap_results['PR AUC']['LSTM-AE Baseline'].append(average_precision_score(y_true_boot, baseline_scores))\n","\n","    # --- B. INDIVIDUAL CLASSIFIERS ---\n","    boot_probs_list = [] # List to store probabilities for Ensembling\n","\n","    for name, clf in best_models.items():\n","        if name == \"One-Class SVM\":\n","            # OCSVM Logic (Not part of Ensembles)\n","            scores = -clf.decision_function(X_feat_boot)\n","            bootstrap_results['ROC AUC'][name].append(roc_auc_score(y_true_boot, scores))\n","            bootstrap_results['PR AUC'][name].append(average_precision_score(y_true_boot, scores))\n","        else:\n","            # Supervised Models\n","            scores = clf.predict_proba(X_feat_boot)[:, 1]\n","            boot_probs_list.append(scores)\n","\n","            # Store Individual Result\n","            bootstrap_results['ROC AUC'][name].append(roc_auc_score(y_true_boot, scores))\n","            bootstrap_results['PR AUC'][name].append(average_precision_score(y_true_boot, scores))\n","\n","    # --- C. ENSEMBLES ---\n","\n","    # 1. Averaged Ensemble\n","    avg_scores = np.mean(boot_probs_list, axis=0)\n","    bootstrap_results['ROC AUC']['Averaged Ensemble'].append(roc_auc_score(y_true_boot, avg_scores))\n","    bootstrap_results['PR AUC']['Averaged Ensemble'].append(average_precision_score(y_true_boot, avg_scores))\n","\n","    # 2. Weighted Ensemble\n","    # Uses 'weights' dictionary from Step 7\n","    w_scores = np.zeros_like(avg_scores)\n","    # Reconstruct weighted sum using the known order of supervised_model_names\n","    for idx, name in enumerate(supervised_model_names):\n","        w_scores += boot_probs_list[idx] * weights[name]\n","\n","    bootstrap_results['ROC AUC']['Weighted Ensemble'].append(roc_auc_score(y_true_boot, w_scores))\n","    bootstrap_results['PR AUC']['Weighted Ensemble'].append(average_precision_score(y_true_boot, w_scores))\n","\n","    # Prepare Input for Stacking Meta-Learners\n","    X_meta_boot = np.column_stack(boot_probs_list)\n","\n","    # 3. Stacked (LR)\n","    slr_scores = meta_lr.predict_proba(X_meta_boot)[:, 1]\n","    bootstrap_results['ROC AUC']['Stacked (LR)'].append(roc_auc_score(y_true_boot, slr_scores))\n","    bootstrap_results['PR AUC']['Stacked (LR)'].append(average_precision_score(y_true_boot, slr_scores))\n","\n","    # 4. Stacked (XGB)\n","    sxgb_scores = meta_xgb.predict_proba(X_meta_boot)[:, 1]\n","    bootstrap_results['ROC AUC']['Stacked (XGB)'].append(roc_auc_score(y_true_boot, sxgb_scores))\n","    bootstrap_results['PR AUC']['Stacked (XGB)'].append(average_precision_score(y_true_boot, sxgb_scores))\n","\n","\n","# --- STATISTICAL SIGNIFICANCE ---\n","# We compare everyone against the \"Averaged Ensemble\" (or whichever won in Step 7)\n","print(\"\\nCalculating P-Values (Comparison vs. Averaged Ensemble)...\")\n","\n","p_values = {}\n","ref_roc = bootstrap_results['ROC AUC']['Averaged Ensemble']\n","ref_pr = bootstrap_results['PR AUC']['Averaged Ensemble']\n","\n","for name in model_names:\n","    if name == 'Averaged Ensemble': continue\n","\n","    # Paired Wilcoxon Test\n","    _, p_roc = wilcoxon(bootstrap_results['ROC AUC'][name], ref_roc)\n","    _, p_pr = wilcoxon(bootstrap_results['PR AUC'][name], ref_pr)\n","\n","    p_values[name] = {'p_roc': p_roc, 'p_pr': p_pr}\n","\n","# --- FINAL REPORT GENERATION ---\n","print(\"\\n\" + \"=\"*95)\n","print(f\"{'Model':<25} | {'ROC AUC (95% CI)':<28} | {'PR AUC (95% CI)':<28}\")\n","print(\"-\" * 95)\n","\n","def get_ci(data):\n","    lower = np.percentile(data, 2.5)\n","    mean = np.mean(data)\n","    upper = np.percentile(data, 97.5)\n","    return f\"{mean:.3f} [{lower:.3f}, {upper:.3f}]\"\n","\n","# Print Table\n","for name in model_names:\n","    row_roc = get_ci(bootstrap_results['ROC AUC'][name])\n","    row_pr = get_ci(bootstrap_results['PR AUC'][name])\n","\n","    if name == 'Averaged Ensemble':\n","        print(f\"{name:<25} | {row_roc:<28} | {row_pr:<28}\")\n","    else:\n","        # Mark significance\n","        sig = \"*\" if p_values[name]['p_roc'] < 0.05 else \"\"\n","        print(f\"{name:<25} | {row_roc:<28}{sig} | {row_pr:<28}\")\n","\n","print(\"=\"*95)\n","print(\"* p < 0.05 compared to Averaged Ensemble (Wilcoxon Signed-Rank Test)\")"],"metadata":{"id":"6ae81rPlq7fl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import wilcoxon, ttest_rel\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve # Added f1_score and precision_recall_curve\n","from tqdm import tqdm\n","\n","# --- CONFIGURATION ---\n","n_iterations = 1000\n","alpha = 0.05\n","\n","# Define all models to test\n","# Base models + Ensembles\n","ensemble_names = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","model_names = ['LSTM-AE Baseline'] + list(best_models.keys()) + ensemble_names\n","\n","# Store metric distributions\n","bootstrap_results = {\n","    'ROC AUC': {name: [] for name in model_names},\n","    'PR AUC': {name: [] for name in model_names},\n","    'F1-Score': {name: [] for name in model_names} # Added F1-Score\n","}\n","\n","print(f\"Starting Bootstrap Analysis ({n_iterations} iterations)...\")\n","\n","# --- TUNE F1 THRESHOLDS ON VALIDATION SET FOR ALL MODELS (IF NOT ALREADY TUNED) ---\n","# This is crucial for consistent F1-score calculation during bootstrap\n","print(\"\\n--- Tuning F1 Thresholds on Validation Set for all models ---\")\n","\n","# Tune F1 Threshold for LSTM-AE Baseline\n","precision_ae_val, recall_ae_val, thresholds_ae_val = precision_recall_curve(y_val, val_mae)\n","fscore_ae_val = np.divide(2 * precision_ae_val * recall_ae_val, precision_ae_val + recall_ae_val, out=np.zeros_like(precision_ae_val), where=(precision_ae_val + recall_ae_val != 0))\n","if fscore_ae_val.size > 0 and not np.all(np.isnan(fscore_ae_val)):\n","    best_thresh_idx_ae_val = np.nanargmax(fscore_ae_val) # Use nanargmax to ignore NaNs\n","    if best_thresh_idx_ae_val < len(thresholds_ae_val):\n","        best_thresholds['LSTM-AE Baseline'] = thresholds_ae_val[best_thresh_idx_ae_val]\n","    else:\n","        best_thresholds['LSTM-AE Baseline'] = 0.5 # Default if index is out of bounds\n","else:\n","    best_thresholds['LSTM-AE Baseline'] = 0.5 # Default threshold if no meaningful F1 can be calculated\n","print(f\"  {'LSTM-AE Baseline':<20}: Optimal F1 Threshold = {best_thresholds['LSTM-AE Baseline']:.4f}\")\n","\n","# Tune F1 Thresholds for Individual Supervised Models (if not already set for F1)\n","supervised_models_for_f1_tuning = [name for name in best_models.keys() if name != \"One-Class SVM\"]\n","\n","for name in supervised_models_for_f1_tuning:\n","    # Assuming val_probs_dict contains probabilities for individual models from TG4lfZc7R8g6\n","    if name in val_probs_dict:\n","        probs = val_probs_dict[name]\n","        precision, recall, thresholds = precision_recall_curve(y_val, probs)\n","        fscore = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(precision), where=(precision + recall != 0))\n","\n","        if fscore.size > 0 and not np.all(np.isnan(fscore)):\n","            best_thresh_idx = np.nanargmax(fscore)\n","            if best_thresh_idx < len(thresholds):\n","                best_thresholds[name] = thresholds[best_thresh_idx]\n","            else:\n","                best_thresholds[name] = 0.5\n","        else:\n","            best_thresholds[name] = 0.5\n","        print(f\"  {name:<20}: Optimal F1 Threshold = {best_thresholds[name]:.4f}\")\n","\n","\n","# --- BOOTSTRAP LOOP ---\n","n_test = len(y_test)\n","np.random.seed(42)\n","\n","# Identify supervised models for stacking (must match order in Step 7)\n","supervised_models = [name for name in best_models.keys() if name != \"One-Class SVM\"]\n","\n","for i in tqdm(range(n_iterations)):\n","    # 1. Resample\n","    indices = np.random.choice(n_test, n_test, replace=True)\n","    y_true_boot = y_test[indices]\n","    X_feat_boot = X_test_feat[indices]\n","\n","    # Check bounds (edge case: only one class in sample)\n","    if len(np.unique(y_true_boot)) < 2: continue\n","\n","    # --- A. BASELINE (LSTM-AE) ---\n","    baseline_scores = test_mae[indices]\n","    bootstrap_results['ROC AUC']['LSTM-AE Baseline'].append(roc_auc_score(y_true_boot, baseline_scores))\n","    bootstrap_results['PR AUC']['LSTM-AE Baseline'].append(average_precision_score(y_true_boot, baseline_scores))\n","\n","    # F1-Score for baseline\n","    baseline_binary_preds = (baseline_scores >= best_thresholds['LSTM-AE Baseline']).astype(int)\n","    bootstrap_results['F1-Score']['LSTM-AE Baseline'].append(f1_score(y_true_boot, baseline_binary_preds))\n","\n","    # --- B. INDIVIDUAL CLASSIFIERS ---\n","    boot_probs_list = [] # Store probs for ensembles (order matters!)\n","\n","    for name, clf in best_models.items():\n","        if name == \"One-Class SVM\":\n","            # OCSVM Logic\n","            scores_ocsvm = -clf.decision_function(X_feat_boot)\n","            binary_preds_ocsvm = np.where(clf.predict(X_feat_boot) == -1, 1, 0)\n","            bootstrap_results['ROC AUC'][name].append(roc_auc_score(y_true_boot, scores_ocsvm))\n","            bootstrap_results['PR AUC'][name].append(average_precision_score(y_true_boot, scores_ocsvm))\n","            bootstrap_results['F1-Score'][name].append(f1_score(y_true_boot, binary_preds_ocsvm))\n","        else:\n","            # Standard Supervised Models\n","            scores_supervised = clf.predict_proba(X_feat_boot)[:, 1]\n","            boot_probs_list.append(scores_supervised) # Append for ensemble stacking\n","\n","            binary_preds_supervised = (scores_supervised >= best_thresholds[name]).astype(int)\n","\n","            bootstrap_results['ROC AUC'][name].append(roc_auc_score(y_true_boot, scores_supervised))\n","            bootstrap_results['PR AUC'][name].append(average_precision_score(y_true_boot, scores_supervised))\n","            bootstrap_results['F1-Score'][name].append(f1_score(y_true_boot, binary_preds_supervised))\n","\n","    # --- C. ENSEMBLES ---\n","\n","    # 1. Averaged Ensemble\n","    avg_scores = np.mean(boot_probs_list, axis=0)\n","    binary_preds_avg = (avg_scores >= best_thresholds['Averaged Ensemble']).astype(int)\n","    bootstrap_results['ROC AUC']['Averaged Ensemble'].append(roc_auc_score(y_true_boot, avg_scores))\n","    bootstrap_results['PR AUC']['Averaged Ensemble'].append(average_precision_score(y_true_boot, avg_scores))\n","    bootstrap_results['F1-Score']['Averaged Ensemble'].append(f1_score(y_true_boot, binary_preds_avg))\n","\n","    # 2. Weighted Ensemble\n","    # Uses 'weights' dictionary calculated in Step 7\n","    w_scores = np.zeros_like(avg_scores)\n","    for idx, name in enumerate(supervised_models):\n","        w_scores += boot_probs_list[idx] * weights[name]\n","    binary_preds_w = (w_scores >= best_thresholds['Weighted Ensemble']).astype(int)\n","    bootstrap_results['ROC AUC']['Weighted Ensemble'].append(roc_auc_score(y_true_boot, w_scores))\n","    bootstrap_results['PR AUC']['Weighted Ensemble'].append(average_precision_score(y_true_boot, w_scores))\n","    bootstrap_results['F1-Score']['Weighted Ensemble'].append(f1_score(y_true_boot, binary_preds_w))\n","\n","    # Prepare Input for Meta-Learners (Stacking)\n","    X_meta_boot = np.column_stack(boot_probs_list)\n","\n","    # 3. Stacked (LR)\n","    slr_scores = meta_lr.predict_proba(X_meta_boot)[:, 1]\n","    binary_preds_slr = (slr_scores >= best_thresholds['Stacked (LR)']).astype(int)\n","    bootstrap_results['ROC AUC']['Stacked (LR)'].append(roc_auc_score(y_true_boot, slr_scores))\n","    bootstrap_results['PR AUC']['Stacked (LR)'].append(average_precision_score(y_true_boot, slr_scores))\n","    bootstrap_results['F1-Score']['Stacked (LR)'].append(f1_score(y_true_boot, binary_preds_slr))\n","\n","    # 4. Stacked (XGB)\n","    sxgb_scores = meta_xgb.predict_proba(X_meta_boot)[:, 1]\n","    binary_preds_sxgb = (sxgb_scores >= best_thresholds['Stacked (XGB)']).astype(int)\n","    bootstrap_results['ROC AUC']['Stacked (XGB)'].append(roc_auc_score(y_true_boot, sxgb_scores))\n","    bootstrap_results['PR AUC']['Stacked (XGB)'].append(average_precision_score(y_true_boot, sxgb_scores))\n","    bootstrap_results['F1-Score']['Stacked (XGB)'].append(f1_score(y_true_boot, binary_preds_sxgb))\n","\n","\n","# --- STATISTICAL COMPARISON ---\n","print(\"\\n\" + \"=\"*115)\n","print(f\"{'Model':<25} | {'Metric':<10} | {'Value (95% CI)':<25} | {'p-value (Wilcoxon)':<20} | {'p-value (t-test)':<20}\")\n","print(\"-\" * 115)\n","\n","# Reference Model: Averaged Ensemble (You can change this to Weighted or Stacked if they perform better)\n","ref_roc = bootstrap_results['ROC AUC']['Averaged Ensemble']\n","ref_pr = bootstrap_results['PR AUC']['Averaged Ensemble']\n","ref_f1 = bootstrap_results['F1-Score']['Averaged Ensemble']\n","\n","def get_ci(data):\n","    lower = np.percentile(data, 2.5)\n","    mean = np.mean(data)\n","    upper = np.percentile(data, 97.5)\n","    return f\"{mean:.3f} [{lower:.3f}, {upper:.3f}]\"\n","\n","metrics_to_report = ['ROC AUC', 'PR AUC', 'F1-Score']\n","\n","# Print Table\n","for name in model_names:\n","    first_metric_printed = False\n","    for metric_name in metrics_to_report:\n","        metric_data = bootstrap_results[metric_name][name]\n","        ci_str = get_ci(metric_data)\n","\n","        p_wilcox = 1.0\n","        p_ttest = 1.0\n","        sig_mark = \"\"\n","\n","        if name == 'Averaged Ensemble':\n","            # Reference model - p-values are 1.0 by definition\n","            pass\n","        else:\n","            # Paired Tests against Reference for the specific metric\n","            ref_metric_data = None\n","            if metric_name == 'ROC AUC': ref_metric_data = ref_roc\n","            elif metric_name == 'PR AUC': ref_metric_data = ref_pr\n","            elif metric_name == 'F1-Score': ref_metric_data = ref_f1\n","\n","            _, p_wilcox = wilcoxon(metric_data, ref_metric_data)\n","            _, p_ttest = ttest_rel(metric_data, ref_metric_data)\n","            sig_mark = \"*\" if p_wilcox < 0.05 else \"\"\n","\n","        model_col = name if not first_metric_printed else \"\"\n","        first_metric_printed = True\n","\n","        print(f\"{model_col:<25} | {metric_name:<10} | {ci_str:<25} | {p_wilcox:.2e} {sig_mark:<10} | {p_ttest:.2e}\")\n","\n","print(\"=\"*115)\n","print(\"* p < 0.05 compared to Averaged Ensemble (Significant Difference)\")"],"metadata":{"id":"dhVAVlaA2aog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import wilcoxon\n","from statsmodels.stats.multitest import multipletests\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score # Added f1_score\n","from tqdm import tqdm\n","\n","# --- CONFIGURATION ---\n","n_iterations = 1000\n","alpha = 0.05 # Significance level\n","\n","# Define all models to test\n","ensemble_names = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)'] # Include all ensembles here\n","model_names = ['LSTM-AE Baseline'] + list(best_models.keys()) + ensemble_names # Ensure all models are included\n","\n","# Store metric distributions\n","bootstrap_results = {\n","    'ROC AUC': {name: [] for name in model_names},\n","    'PR AUC': {name: [] for name in model_names},\n","    'F1-Score': {name: [] for name in model_names} # Added F1-Score\n","}\n","\n","print(f\"Starting Bootstrap Analysis ({n_iterations} iterations)...\")\n","\n","# --- BOOTSTRAP LOOP ---\n","n_test = len(y_test)\n","np.random.seed(42)\n","\n","# Identify supervised models for stacking (must match order in Step 7)\n","supervised_models_for_ensembling = [name for name in best_models.keys() if name != \"One-Class SVM\"]\n","\n","for i in tqdm(range(n_iterations)):\n","    # 1. Resample\n","    indices = np.random.choice(n_test, n_test, replace=True)\n","    y_true_boot = y_test[indices]\n","    X_feat_boot = X_test_feat[indices]\n","\n","    # Check bounds (edge case: only one class in sample)\n","    if len(np.unique(y_true_boot)) < 2: continue\n","\n","    # --- A. BASELINE (LSTM-AE) ---\n","    baseline_scores = test_mae[indices]\n","    bootstrap_results['ROC AUC']['LSTM-AE Baseline'].append(roc_auc_score(y_true_boot, baseline_scores))\n","    bootstrap_results['PR AUC']['LSTM-AE Baseline'].append(average_precision_score(y_true_boot, baseline_scores))\n","    # F1-Score for baseline (using pre-tuned threshold)\n","    baseline_binary_preds = (baseline_scores >= best_thresholds['LSTM-AE Baseline']).astype(int)\n","    bootstrap_results['F1-Score']['LSTM-AE Baseline'].append(f1_score(y_true_boot, baseline_binary_preds))\n","\n","    # --- B. INDIVIDUAL CLASSIFIERS ---\n","    boot_probs_list = [] # Store probs for supervised models for ensembling\n","\n","    for name, clf in best_models.items():\n","        if name == \"One-Class SVM\":\n","            # OCSVM Logic\n","            scores_ocsvm = -clf.decision_function(X_feat_boot)\n","            binary_preds_ocsvm = np.where(clf.predict(X_feat_boot) == -1, 1, 0)\n","            bootstrap_results['ROC AUC'][name].append(roc_auc_score(y_true_boot, scores_ocsvm))\n","            bootstrap_results['PR AUC'][name].append(average_precision_score(y_true_boot, scores_ocsvm))\n","            bootstrap_results['F1-Score'][name].append(f1_score(y_true_boot, binary_preds_ocsvm))\n","        else:\n","            # Standard Supervised Models\n","            scores_supervised = clf.predict_proba(X_feat_boot)[:, 1]\n","            boot_probs_list.append(scores_supervised) # Append for ensemble stacking\n","\n","            # F1-Score for supervised models (using pre-tuned threshold)\n","            binary_preds_supervised = (scores_supervised >= best_thresholds[name]).astype(int)\n","\n","            bootstrap_results['ROC AUC'][name].append(roc_auc_score(y_true_boot, scores_supervised))\n","            bootstrap_results['PR AUC'][name].append(average_precision_score(y_true_boot, scores_supervised))\n","            bootstrap_results['F1-Score'][name].append(f1_score(y_true_boot, binary_preds_supervised))\n","\n","    # --- C. ENSEMBLES ---\n","\n","    # 1. Averaged Ensemble\n","    avg_scores = np.mean(boot_probs_list, axis=0)\n","    binary_preds_avg = (avg_scores >= best_thresholds['Averaged Ensemble']).astype(int)\n","    bootstrap_results['ROC AUC']['Averaged Ensemble'].append(roc_auc_score(y_true_boot, avg_scores))\n","    bootstrap_results['PR AUC']['Averaged Ensemble'].append(average_precision_score(y_true_boot, avg_scores))\n","    bootstrap_results['F1-Score']['Averaged Ensemble'].append(f1_score(y_true_boot, binary_preds_avg))\n","\n","    # 2. Weighted Ensemble\n","    # Uses 'weights' dictionary from Step 7 (assuming it's available and ordered correctly)\n","    w_scores = np.zeros_like(avg_scores)\n","    for idx, name in enumerate(supervised_models_for_ensembling):\n","        if name in weights: # Ensure model contributed to weights\n","             w_scores += boot_probs_list[idx] * weights[name]\n","    binary_preds_w = (w_scores >= best_thresholds['Weighted Ensemble']).astype(int)\n","    bootstrap_results['ROC AUC']['Weighted Ensemble'].append(roc_auc_score(y_true_boot, w_scores))\n","    bootstrap_results['PR AUC']['Weighted Ensemble'].append(average_precision_score(y_true_boot, w_scores))\n","    bootstrap_results['F1-Score']['Weighted Ensemble'].append(f1_score(y_true_boot, binary_preds_w))\n","\n","    # Prepare Input for Meta-Learners (Stacking)\n","    X_meta_boot = np.column_stack(boot_probs_list)\n","\n","    # 3. Stacked (LR)\n","    slr_scores = meta_lr.predict_proba(X_meta_boot)[:, 1]\n","    binary_preds_slr = (slr_scores >= best_thresholds['Stacked (LR)']).astype(int)\n","    bootstrap_results['ROC AUC']['Stacked (LR)'].append(roc_auc_score(y_true_boot, slr_scores))\n","    bootstrap_results['PR AUC']['Stacked (LR)'].append(average_precision_score(y_true_boot, slr_scores))\n","    bootstrap_results['F1-Score']['Stacked (LR)'].append(f1_score(y_true_boot, binary_preds_slr))\n","\n","    # 4. Stacked (XGB)\n","    sxgb_scores = meta_xgb.predict_proba(X_meta_boot)[:, 1]\n","    binary_preds_sxgb = (sxgb_scores >= best_thresholds['Stacked (XGB)']).astype(int)\n","    bootstrap_results['ROC AUC']['Stacked (XGB)'].append(roc_auc_score(y_true_boot, sxgb_scores))\n","    bootstrap_results['PR AUC']['Stacked (XGB)'].append(average_precision_score(y_true_boot, sxgb_scores))\n","    bootstrap_results['F1-Score']['Stacked (XGB)'].append(f1_score(y_true_boot, binary_preds_sxgb))\n","\n","\n","# --- HOLM-BONFERRONI CORRECTION ---\n","print(\"\\nCalculating P-Values & Applying Holm-Bonferroni Correction...\")\n","\n","p_vals_roc = []\n","p_vals_pr = []\n","p_vals_f1 = [] # Added for F1-Score\n","models_tested = []\n","\n","ref_model_name = 'Averaged Ensemble' # Use the best ensemble as reference\n","ref_roc = bootstrap_results['ROC AUC'][ref_model_name]\n","ref_pr = bootstrap_results['PR AUC'][ref_model_name]\n","ref_f1 = bootstrap_results['F1-Score'][ref_model_name] # Added reference F1\n","\n","# 1. Calculate Raw P-Values (Wilcoxon)\n","# Iterate through all models except the reference model itself\n","for name in model_names:\n","    if name == ref_model_name: continue\n","\n","    # Ensure lengths are matched if any bootstrap iteration was skipped\n","    min_len_ref = len(ref_roc)\n","    min_len_current = min([len(bootstrap_results['ROC AUC'][name]), len(bootstrap_results['PR AUC'][name]), len(bootstrap_results['F1-Score'][name])])\n","    current_len = min(min_len_ref, min_len_current)\n","\n","    # Skip if not enough samples for comparison\n","    if current_len < 2: continue # Wilcoxon needs at least 2 samples\n","\n","    # Get truncated data for Wilcoxon test\n","    current_roc_data = bootstrap_results['ROC AUC'][name][:current_len]\n","    current_pr_data = bootstrap_results['PR AUC'][name][:current_len]\n","    current_f1_data = bootstrap_results['F1-Score'][name][:current_len]\n","\n","    _, p_r = wilcoxon(current_roc_data, ref_roc[:current_len])\n","    _, p_p = wilcoxon(current_pr_data, ref_pr[:current_len])\n","    _, p_f = wilcoxon(current_f1_data, ref_f1[:current_len]) # Wilcoxon for F1\n","\n","    p_vals_roc.append(p_r)\n","    p_vals_pr.append(p_p)\n","    p_vals_f1.append(p_f) # Append F1 p-value\n","    models_tested.append(name)\n","\n","# 2. Apply Correction (separately for ROC, PR, and F1 families)\n","reject_roc, p_corrected_roc, _, _ = multipletests(p_vals_roc, alpha=alpha, method='holm')\n","reject_pr, p_corrected_pr, _, _ = multipletests(p_vals_pr, alpha=alpha, method='holm')\n","reject_f1, p_corrected_f1, _, _ = multipletests(p_vals_f1, alpha=alpha, method='holm') # Correct F1 p-values\n","\n","# Map back to models\n","significance_map = {}\n","for i, name in enumerate(models_tested):\n","    significance_map[name] = {\n","        'ROC': '*' if reject_roc[i] else 'ns',\n","        'PR': '*' if reject_pr[i] else 'ns',\n","        'F1': '*' if reject_f1[i] else 'ns', # Added F1 significance\n","        'p_adj_roc': p_corrected_roc[i],\n","        'p_adj_pr': p_corrected_pr[i],\n","        'p_adj_f1': p_corrected_f1[i] # Added adjusted F1 p-value\n","    }\n","\n","# --- FINAL TABLE ---\n","print(\"\\n\" + \"=\"*140)\n","print(f\"{'Model':<25} | {'ROC AUC (95% CI)':<28} | {'PR AUC (95% CI)':<28} | {'F1-Score (95% CI)':<28} | {'Sig (vs Ens)':<12}\") # Updated header\n","print(\"-\" * 140)\n","\n","def get_ci(data):\n","    mean = np.mean(data)\n","    lower = np.percentile(data, 2.5)\n","    upper = np.percentile(data, 97.5)\n","    return f\"{mean:.3f} [{lower:.3f}, {upper:.3f}]\"\n","\n","# Print Rows (ensure consistency in model_names order if changed)\n","# Re-sort model_names to match the order in models_tested and ref_model_name at the end\n","printable_model_names = sorted(model_names, key=lambda x: (x == ref_model_name, x))\n","\n","for name in printable_model_names:\n","    if name == ref_model_name:\n","        # Print reference model at the end\n","        continue\n","\n","    # Get current length to avoid errors if some lists are shorter\n","    current_len = len(bootstrap_results['ROC AUC'][name])\n","    if current_len == 0: # Skip if no data for this model\n","        continue\n","\n","    row_roc = get_ci(bootstrap_results['ROC AUC'][name][:current_len])\n","    row_pr = get_ci(bootstrap_results['PR AUC'][name][:current_len])\n","    row_f1 = get_ci(bootstrap_results['F1-Score'][name][:current_len]) # Added F1 CI\n","\n","    sig_roc = significance_map[name]['ROC']\n","    sig_pr = significance_map[name]['PR']\n","    sig_f1 = significance_map[name]['F1'] # Added F1 significance\n","\n","    # Mark significance if ANY metric is significant\n","    final_sig = \"*\" if ('*' in [sig_roc, sig_pr, sig_f1]) else \"ns\"\n","\n","    print(f\"{name:<25} | {row_roc:<28} | {row_pr:<28} | {row_f1:<28} | {final_sig:<12}\")\n","\n","print(\"-\" * 140)\n","\n","# Print Reference model row\n","ref_roc_ci = get_ci(bootstrap_results['ROC AUC'][ref_model_name])\n","ref_pr_ci = get_ci(bootstrap_results['PR AUC'][ref_model_name])\n","ref_f1_ci = get_ci(bootstrap_results['F1-Score'][ref_model_name])\n","print(f\"{ref_model_name:<25} | {ref_roc_ci:<28} | {ref_pr_ci:<28} | {ref_f1_ci:<28} | {'Reference':<12}\")\n","\n","print(\"=\"*140)\n","print(f\"* Statistically significant difference from {ref_model_name} (Holm-Bonferroni adj. p < {alpha}) \")"],"metadata":{"id":"MZ20FSP93McY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import wilcoxon\n","from statsmodels.stats.multitest import multipletests\n","\n","# --- 1. CONFIGURATION ---\n","alpha = 0.05\n","ref_model = 'Averaged Ensemble' # Reference for comparison\n","\n","# Ensure model_names matches the keys in bootstrap_results\n","if 'model_names' not in globals():\n","    keys = list(bootstrap_results['ROC AUC'].keys())\n","    model_names = sorted(keys, key=lambda x: (x == ref_model, 'Ensemble' in x, x))\n","\n","# --- 2. DIAGNOSE & FIX MISMATCH ---\n","print(\"Diagnosing Data Lengths...\")\n","min_len = 999999999\n","\n","# Find the shortest list length (in case of bootstrap interruptions)\n","for name in model_names:\n","    l = len(bootstrap_results['ROC AUC'][name]) # Use ROC AUC as a reference for length\n","    if l < min_len:\n","        min_len = l\n","\n","print(f\"Truncating all results to {min_len} samples to ensure alignment...\")\n","\n","# Truncate lists to match the minimum length for all metrics\n","for name in model_names:\n","    bootstrap_results['ROC AUC'][name] = bootstrap_results['ROC AUC'][name][:min_len]\n","    bootstrap_results['PR AUC'][name] = bootstrap_results['PR AUC'][name][:min_len]\n","    bootstrap_results['F1-Score'][name] = bootstrap_results['F1-Score'][name][:min_len]\n","\n","# --- 3. CALCULATE RAW WILCOXON P-VALUES ---\n","print(f\"Calculating raw p-values (Reference: {ref_model})...\")\n","ref_roc = bootstrap_results['ROC AUC'][ref_model]\n","ref_pr = bootstrap_results['PR AUC'][ref_model]\n","ref_f1 = bootstrap_results['F1-Score'][ref_model]\n","\n","p_vals_roc = []\n","p_vals_pr = []\n","p_vals_f1 = []\n","models_to_correct = []\n","\n","for name in model_names:\n","    if name == ref_model: continue\n","\n","    # Calculate Wilcoxon against Reference for each metric\n","    stat_roc, p_roc = wilcoxon(bootstrap_results['ROC AUC'][name], ref_roc)\n","    stat_pr, p_pr = wilcoxon(bootstrap_results['PR AUC'][name], ref_pr)\n","    stat_f1, p_f1 = wilcoxon(bootstrap_results['F1-Score'][name], ref_f1)\n","\n","    p_vals_roc.append(p_roc)\n","    p_vals_pr.append(p_pr)\n","    p_vals_f1.append(p_f1)\n","    models_to_correct.append(name)\n","\n","# --- 4. APPLY HOLM-BONFERRONI CORRECTION ---\n","reject_roc, p_corrected_roc, _, _ = multipletests(p_vals_roc, alpha=alpha, method='holm')\n","reject_pr, p_corrected_pr, _, _ = multipletests(p_vals_pr, alpha=alpha, method='holm')\n","reject_f1, p_corrected_f1, _, _ = multipletests(p_vals_f1, alpha=alpha, method='holm')\n","\n","# Map results back for printing\n","adj_results = {}\n","for i, name in enumerate(models_to_correct):\n","    adj_results[name] = {\n","        'p_adj_roc': p_corrected_roc[i],\n","        'sig_roc': reject_roc[i],\n","        'p_adj_pr': p_corrected_pr[i],\n","        'sig_pr': reject_pr[i],\n","        'p_adj_f1': p_corrected_f1[i],\n","        'sig_f1': reject_f1[i]\n","    }\n","\n","# --- 5. PRINT FINAL MANUSCRIPT TABLE ---\n","print(\"\\n\" + \"=\"*170)\n","print(f\"{'Model':<25} | {'ROC AUC (95% CI)':<28} | {'P-Value (Adj)':<15} | {'PR AUC (95% CI)':<28} | {'P-Value (Adj)':<15} | {'F1-Score (95% CI)':<28} | {'P-Value (Adj)':<15}\")\n","print(\"-\" * 170)\n","\n","def get_ci(data):\n","    mean = np.mean(data)\n","    lower = np.percentile(data, 2.5)\n","    upper = np.percentile(data, 97.5)\n","    return f\"{mean:.3f} [{lower:.3f}, {upper:.3f}]\"\n","\n","# Print Rows\n","for name in model_names:\n","    if name == ref_model: continue\n","\n","    ci_roc_str = get_ci(bootstrap_results['ROC AUC'][name])\n","    ci_pr_str = get_ci(bootstrap_results['PR AUC'][name])\n","    ci_f1_str = get_ci(bootstrap_results['F1-Score'][name])\n","\n","    # Retrieve stats\n","    stats = adj_results[name]\n","\n","    p_adj_roc = stats['p_adj_roc']\n","    is_sig_roc = stats['sig_roc']\n","\n","    p_adj_pr = stats['p_adj_pr']\n","    is_sig_pr = stats['sig_pr']\n","\n","    p_adj_f1 = stats['p_adj_f1']\n","    is_sig_f1 = stats['sig_f1']\n","\n","    p_roc_str = \"< 0.001*\" if p_adj_roc < 0.001 else f\"{p_adj_roc:.3f}\" + (\"*\" if is_sig_roc else \"\")\n","    p_pr_str = \"< 0.001*\" if p_adj_pr < 0.001 else f\"{p_adj_pr:.3f}\" + (\"*\" if is_sig_pr else \"\")\n","    p_f1_str = \"< 0.001*\" if p_adj_f1 < 0.001 else f\"{p_adj_f1:.3f}\" + (\"*\" if is_sig_f1 else \"\")\n","\n","    print(f\"{name:<25} | {ci_roc_str:<28} | {p_roc_str:<15} | {ci_pr_str:<28} | {p_pr_str:<15} | {ci_f1_str:<28} | {p_f1_str:<15}\")\n","\n","print(\"-\" * 170)\n","# Print Reference at the bottom\n","ens_roc_ci = get_ci(bootstrap_results['ROC AUC'][ref_model])\n","ens_pr_ci = get_ci(bootstrap_results['PR AUC'][ref_model])\n","ens_f1_ci = get_ci(bootstrap_results['F1-Score'][ref_model])\n","print(f\"{ref_model:<25} | {ens_roc_ci:<28} | {'Reference':<15} | {ens_pr_ci:<28} | {'Reference':<15} | {ens_f1_ci:<28} | {'Reference':<15}\")\n","print(\"=\"*170)\n","print(f\"* Statistically significant difference from {ref_model} (Holm-Bonferroni adjusted p < {alpha})\")"],"metadata":{"id":"LZ5-0SpndzG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 12. Generate Final \"Winning\" ROC Plot (Figure 3b)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import roc_curve, roc_auc_score\n","\n","# Setup\n","plt.figure(figsize=(10, 8))\n","plt.rcParams.update({'font.size': 12, 'font.family': 'serif'})\n","\n","# 1. Baseline\n","fpr_ae, tpr_ae, _ = roc_curve(y_test, test_mae)\n","auc_ae = roc_auc_score(y_test, test_mae)\n","plt.plot(fpr_ae, tpr_ae, label=f'LSTM-AE Baseline (AUC = {auc_ae:.3f})',\n","         linestyle='--', color='gray', linewidth=2, alpha=0.5)\n","\n","# 2. Key Individual Models (RF & XGB)\n","# We won't plot ALL of them to keep it clean, just the best ones + Winner\n","rf_probs = best_models['Random Forest'].predict_proba(X_test_feat)[:, 1]\n","fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)\n","auc_rf = roc_auc_score(y_test, rf_probs)\n","plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.3f})',\n","         color='#ff7f0e', linewidth=2, alpha=0.8)\n","\n","# 3. Averaged Ensemble (Reference)\n","fpr_avg, tpr_avg, _ = roc_curve(y_test, test_probs_dict['Averaged Ensemble'])\n","auc_avg = roc_auc_score(y_test, test_probs_dict['Averaged Ensemble'])\n","plt.plot(fpr_avg, tpr_avg, label=f'Averaged Ensemble (AUC = {auc_avg:.3f})',\n","         color='blue', linewidth=2, linestyle='-.', alpha=0.8)\n","\n","# 4. THE WINNER: Stacked (LR)\n","fpr_stack, tpr_stack, _ = roc_curve(y_test, test_probs_dict['Stacked (LR)'])\n","auc_stack = roc_auc_score(y_test, test_probs_dict['Stacked (LR)'])\n","plt.plot(fpr_stack, tpr_stack, label=f'Stacked Ensemble [LR] (AUC = {auc_stack:.3f})',\n","         color='black', linewidth=3.5, zorder=10)\n","\n","# Formatting\n","plt.plot([0, 1], [0, 1], 'k:', alpha=0.4)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.02])\n","plt.xlabel('False Positive Rate', fontweight='bold')\n","plt.ylabel('True Positive Rate', fontweight='bold')\n","plt.title('ROC Curves: Stacked Ensemble Optimization', fontweight='bold')\n","plt.legend(loc=\"lower right\", frameon=True, fontsize=11)\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('Figure3b_Final_Stacked_Winner.pdf', dpi=600)\n","plt.show()"],"metadata":{"id":"lhgXSgnXlw0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","import numpy as np\n","\n","print(\"--- DETAILED CLASSIFICATION REPORTS ---\")\n","print(\"Note: For supervised models, the F1-score is calculated using the optimal threshold tuned on the validation set.\\n      For ensembles, a threshold of 0.5 is used for simplicity unless a specific ensemble threshold was previously tuned for F1-score.\\n      For One-Class SVM, its inherent -1/1 prediction is mapped to 1/0.\\n\")\n","\n","# Combine all models and ensembles for reporting\n","all_models_for_report = {}\n","all_models_for_report.update(best_models)\n","\n","# Add ensemble predictions to the dictionary for consistent processing\n","# Ensure these are aligned with how they were generated and stored in test_probs_dict\n","if 'Averaged Ensemble' in test_probs_dict:\n","    all_models_for_report['Averaged Ensemble'] = test_probs_dict['Averaged Ensemble']\n","if 'Weighted Ensemble' in test_probs_dict:\n","    all_models_for_report['Weighted Ensemble'] = test_probs_dict['Weighted Ensemble']\n","if 'Stacked (LR)' in test_probs_dict:\n","    all_models_for_report['Stacked (LR)'] = test_probs_dict['Stacked (LR)']\n","if 'Stacked (XGB)' in test_probs_dict:\n","    all_models_for_report['Stacked (XGB)'] = test_probs_dict['Stacked (XGB)']\n","\n","for name, model_or_probs in all_models_for_report.items():\n","    print(f\"\\n{'='*50}\\nCLASSIFICATION REPORT FOR: {name}\\n{'='*50}\")\n","\n","    if name == \"One-Class SVM\":\n","        # OCSVM Logic: predict returns -1 for anomalies, 1 for normal\n","        # Map to 1 for positive class (dysbiosis), 0 for negative class (normal)\n","        preds = model_or_probs.predict(X_test_feat)\n","        binary_preds = np.where(preds == -1, 1, 0)\n","    elif isinstance(model_or_probs, np.ndarray): # This is for ensemble probabilities\n","        # For ensembles, we use 0.5 as the threshold for binary classification\n","        binary_preds = (model_or_probs >= 0.5).astype(int)\n","    else:\n","        # For other supervised models, use predict_proba and apply the tuned threshold\n","        probs = model_or_probs.predict_proba(X_test_feat)[:, 1]\n","        # Retrieve the best threshold for this specific model\n","        thresh = best_thresholds.get(name, 0.5) # Default to 0.5 if threshold not found (shouldn't happen)\n","        binary_preds = (probs >= thresh).astype(int)\n","\n","    print(classification_report(y_test, binary_preds, target_names=['Class 0 (Normal)', 'Class 1 (Dysbiotic)']))\n"],"metadata":{"id":"ZqabeOWyl5ql"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"45364b46"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, f1_score\n","import numpy as np\n","\n","print(\"--- GENERATING CONFUSION MATRICES ---\")\n","\n","# Combine trained models and ensemble probabilities into one dictionary\n","all_models_for_report = {}\n","all_models_for_report.update(best_models) # Your supervised models (RF, SVM, etc.)\n","\n","# Add specific ensembles from your probability dictionary\n","# Ensure keys match those in 'best_thresholds' if you tuned them\n","ensemble_keys_for_cm = ['Averaged Ensemble', 'Weighted Ensemble', 'Stacked (LR)', 'Stacked (XGB)']\n","for key in ensemble_keys_for_cm:\n","    if key in test_probs_dict:\n","        all_models_for_report[key] = test_probs_dict[key]\n","\n","for name, model_or_probs in all_models_for_report.items():\n","    print(f\"\\n{'='*50}\\nCONFUSION MATRIX FOR: {name}\\n{'='*50}\")\n","\n","    if name == \"One-Class SVM\":\n","        preds = model_or_probs.predict(X_test_feat)\n","        binary_preds = np.where(preds == -1, 1, 0) # Map OCSVM output to 0/1\n","    elif isinstance(model_or_probs, np.ndarray): # Ensembles are probabilities (numpy array)\n","        binary_preds = (model_or_probs >= 0.5).astype(int) # Default to 0.5 threshold for ensembles\n","    else:\n","        probs = model_or_probs.predict_proba(X_test_feat)[:, 1]\n","        thresh = best_thresholds.get(name, 0.5) # Use tuned threshold for supervised models\n","        binary_preds = (probs >= thresh).astype(int)\n","\n","    # Calculate Confusion Matrix\n","    cm = confusion_matrix(y_test, binary_preds)\n","\n","    # Calculate F1-score\n","    f1 = f1_score(y_test, binary_preds)\n","\n","    # Plot Confusion Matrix\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n","                xticklabels=['Predicted Normal', 'Predicted Dysbiotic'],\n","                yticklabels=['Actual Normal', 'Actual Dysbiotic'])\n","    plt.title(f'Confusion Matrix: {name}\\nF1-Score: {f1:.4f}', fontsize=14)\n","    plt.xlabel('Predicted Label', fontsize=12)\n","    plt.ylabel('True Label', fontsize=12)\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 13. Baseline Comparison: Alpha & Beta Diversity (Corrected)\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import entropy, mannwhitneyu\n","from scipy.spatial.distance import braycurtis\n","from sklearn.metrics import roc_auc_score\n","\n","# --- 1. PREPARE DATA ---\n","# FIX: Re-define X_train_normal just in case it is missing\n","X_train_normal = X_train[y_train == 0]\n","\n","print(f\"Normal Training Samples: {X_train_normal.shape[0]}\")\n","\n","# We use the mean abundance over the 14-day window as the representative profile\n","# Axis 1 is the time dimension (14 days)\n","X_train_mean = np.mean(X_train_normal, axis=1) # Shape: (N_train_normal, Features)\n","X_test_mean  = np.mean(X_test, axis=1)         # Shape: (N_test, Features)\n","\n","# --- 2. ALPHA DIVERSITY (Shannon Index) ---\n","# H = -sum(p * log(p))\n","def calc_shannon(data):\n","    # Add epsilon to avoid log(0)\n","    return entropy(data + 1e-10, axis=1)\n","\n","shannon_train = calc_shannon(X_train_mean)\n","shannon_test  = calc_shannon(X_test_mean)\n","\n","# --- 3. BETA DIVERSITY (Distance to Healthy Centroid) ---\n","# Calculate the \"Average Healthy Profile\" (Centroid) from Training Data\n","healthy_centroid = np.mean(X_train_mean, axis=0)\n","\n","# Calculate Bray-Curtis distance of every test sample to this centroid\n","beta_dist = []\n","for sample in X_test_mean:\n","    # Bray-Curtis: sum(|u-v|) / sum(|u+v|)\n","    d = braycurtis(sample, healthy_centroid)\n","    beta_dist.append(d)\n","beta_dist = np.array(beta_dist)\n","\n","# --- 4. EVALUATION (Can these metrics predict Dysbiosis?) ---\n","# Note: Lower Shannon = Dysbiosis (usually), so we flip sign for AUC calculation\n","# (We want \"Higher Score = Anomaly\")\n","auc_alpha = roc_auc_score(y_test, -shannon_test)\n","\n","# Higher Distance = Dysbiosis, so we use as is\n","auc_beta = roc_auc_score(y_test, beta_dist)\n","\n","print(f\"\\n--- DIVERSITY METRIC RESULTS ---\")\n","print(f\"Alpha Diversity (Shannon) AUC:        {auc_alpha:.4f}\")\n","print(f\"Beta Diversity (Dist to Healthy) AUC: {auc_beta:.4f}\")\n","print(f\"DynaBiome (Random Forest) AUC:        0.8903\") # Reference\n","\n","# --- 5. STATISTICAL TEST ---\n","# Compare Normal vs Dysbiotic in Test Set\n","norm_idx = (y_test == 0)\n","dys_idx  = (y_test == 1)\n","\n","stat_a, p_a = mannwhitneyu(shannon_test[norm_idx], shannon_test[dys_idx])\n","stat_b, p_b = mannwhitneyu(beta_dist[norm_idx], beta_dist[dys_idx])\n","\n","print(f\"\\nStats (Normal vs. Dysbiotic):\")\n","print(f\"Alpha P-value: {p_a:.2e}\")\n","print(f\"Beta P-value:  {p_b:.2e}\")\n","\n","# --- 6. PLOT ---\n","fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n","plt.rcParams.update({'font.size': 12, 'font.family': 'serif'})\n","\n","# Alpha Boxplot\n","sns.boxplot(x=y_test, y=shannon_test, ax=axes[0], palette=['#2ca02c', '#d62728'], hue=y_test, legend=False)\n","axes[0].set_xticks([0, 1]) # Explicitly set ticks before labels\n","axes[0].set_xticklabels(['Normal', 'Dysbiosis'])\n","axes[0].set_title(f'Alpha Diversity (Shannon)\\nAUC = {auc_alpha:.2f}')\n","axes[0].set_ylabel('Shannon Index (Higher is Healthier)')\n","axes[0].grid(True, alpha=0.3)\n","\n","# Beta Boxplot\n","sns.boxplot(x=y_test, y=beta_dist, ax=axes[1], palette=['#2ca02c', '#d62728'], hue=y_test, legend=False)\n","axes[1].set_xticks([0, 1]) # Explicitly set ticks before labels\n","axes[1].set_xticklabels(['Normal', 'Dysbiosis'])\n","axes[1].set_title(f'Beta Distance to Healthy Centroid\\nAUC = {auc_beta:.2f}')\n","axes[1].set_ylabel('Bray-Curtis Distance (Higher is Anomalous)')\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('Supplementary_Figure_S2_Diversity.pdf', dpi=600)\n","plt.show()"],"metadata":{"id":"urbb8i9UqcxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 14. Baseline Check: Median Predictor vs. LSTM\n","import numpy as np\n","\n","# 1. Calculate the Median of the Training Data (Per Feature)\n","# We flatten the time dimension to get the median abundance of each genus across all history\n","# X_train shape: (N_samples, 14, N_features)\n","X_train_flat = X_train.reshape(-1, X_train.shape[2])\n","medians = np.median(X_train_flat, axis=0) # Shape: (N_features,)\n","\n","# 2. Create \"Dummy Predictions\" for the Test Set\n","# We predict this static median for every timestep in the test set\n","X_test_flat = X_test.reshape(-1, X_test.shape[2])\n","# Tile the medians to match Test Set shape\n","dummy_preds = np.tile(medians, (X_test_flat.shape[0], 1))\n","\n","# 3. Calculate MAE of the Median Baseline\n","median_baseline_mae = np.mean(np.abs(X_test_flat - dummy_preds))\n","\n","# 4. Compare with LSTM Test MAE (calculated in Step 6)\n","lstm_mae_mean = np.mean(test_mae) # test_mae is vector of MAEs, take mean\n","\n","print(\"--- CONVERGENCE CHECK ---\")\n","print(f\"Median Baseline MAE (The 'Lazy' Guess): {median_baseline_mae:.6f}\")\n","print(f\"DynaBiome LSTM MAE (Your Model):        {lstm_mae_mean:.6f}\")\n","\n","# Calculate Improvement Factor\n","improvement = median_baseline_mae / lstm_mae_mean\n","print(f\"Factor of Improvement: {improvement:.1f}x lower error than baseline\")"],"metadata":{"id":"l7NujUdQsoEw"},"execution_count":null,"outputs":[]}]}