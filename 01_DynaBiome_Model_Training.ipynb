{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V5E1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"2DVP51NGCHPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyAgp9t99gDY"},"outputs":[],"source":["# @title 1. Imports and Setup\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# Set seeds for reproducibility (Crucial for scientific reporting)\n","np.random.seed(42)\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","\n","print(\"Libraries loaded successfully.\")"]},{"cell_type":"code","source":["# @title 2. Load Data and Initial Preprocessing\n","import pandas as pd\n","import os\n","\n","# Define the data directory relative to this script\n","data_dir = './data'\n","# Update the filename to the .zip version\n","file_name = 'asv_interpretability_dataset_modified.zip'\n","file_path = os.path.join(data_dir, file_name)\n","\n","# Check if file exists\n","if not os.path.exists(file_path):\n","    raise FileNotFoundError(f\"Data file not found at {file_path}. Please ensure the 'data' folder contains the zipped dataset.\")\n","\n","# Load the data directly from the zip file\n","df = pd.read_csv(file_path, dtype={'PatientID': str})\n","print(f\"Successfully loaded data from {file_path}\")\n","\n","# --- Helper: Handle NeutrophilCount with '<0.1' values ---\n","def parse_neutrophil(value):\n","    try:\n","        return float(value)\n","    except:\n","        if isinstance(value, str) and \"<\" in value:\n","            threshold = float(value.replace(\"<\", \"\").strip())\n","            return threshold / 2\n","        return np.nan\n","\n","# --- Helper: Create Proxy Labels (Clinical Dysbiosis) ---\n","def label_dysbiosis(row):\n","    # Proxy definition: High Temp + Low Neutrophils + Liquid Stool\n","    is_temp_abnormal = row['MaxTemperature'] > 38.0\n","    is_neutro_low = row['NeutrophilCount'] < 500\n","    is_consistency_liquid = row.get('Consistency_liquid', 0) == 1\n","    return int(is_temp_abnormal and is_neutro_low and is_consistency_liquid)\n","\n","# 1. Clean Neutrophils\n","df['NeutrophilCount'] = df['NeutrophilCount'].apply(parse_neutrophil)\n","# Impute missing neutrophils with median (optional, depends on your specific logic)\n","# df['NeutrophilCount'].fillna(df['NeutrophilCount'].median(), inplace=True)\n","\n","# 2. One-hot encode stool consistency\n","df = pd.get_dummies(df, columns=['Consistency'])\n","\n","# 3. Log transform Genus-relative abundances (Compositional handling)\n","# Ensure no negative values or zeros break the log\n","df['RelativeAbundance'] = df['RelativeAbundance'].astype(float)\n","df['RelativeAbundance'] = np.log1p(df['RelativeAbundance'])\n","\n","# 4. Generate Labels (Row-wise)\n","df['DysbiosisLabel'] = df.apply(label_dysbiosis, axis=1)\n","\n","# 5. Pivot to Wide Format (Time Series Format)\n","# Keep metadata\n","metadata_cols = ['PatientID', 'SampleID', 'DayRelativeToNearestHCT',\n","                 'MaxTemperature', 'NeutrophilCount'] + \\\n","                [col for col in df.columns if col.startswith('Consistency_')] + \\\n","                ['DysbiosisLabel']\n","\n","# Pivot Genus\n","genus_pivot = df.pivot_table(index=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'],\n","                             columns='Genus', values='RelativeAbundance', fill_value=0).reset_index()\n","\n","# Merge Metadata back\n","metadata = df[metadata_cols].drop_duplicates(subset=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'])\n","merged_df = pd.merge(genus_pivot, metadata, on=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'], how='left')\n","\n","print(f\"Data Processed. Total Samples: {len(merged_df)}\")\n","print(f\"Total Unique Patients: {merged_df['PatientID'].nunique()}\")"],"metadata":{"id":"aqakxWWHBhKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Patient-Level Splitting & Scaling (PREVENT DATA LEAKAGE)\n","\n","# --- STEP A: Split Patients First ---\n","# We split the Patient IDs, NOT the sequences.\n","unique_patients = merged_df['PatientID'].unique()\n","np.random.shuffle(unique_patients) # Randomize patient order\n","\n","n_total = len(unique_patients)\n","n_train = int(0.70 * n_total)\n","n_val = int(0.15 * n_total)\n","\n","train_pids = unique_patients[:n_train]\n","val_pids = unique_patients[n_train : n_train + n_val]\n","test_pids = unique_patients[n_train + n_val:]\n","\n","print(f\"Patients in Train: {len(train_pids)}\")\n","print(f\"Patients in Val:   {len(val_pids)}\")\n","print(f\"Patients in Test:  {len(test_pids)}\")\n","\n","# Create separate DataFrames based on Patient ID\n","df_train = merged_df[merged_df['PatientID'].isin(train_pids)].copy()\n","df_val = merged_df[merged_df['PatientID'].isin(val_pids)].copy()\n","df_test = merged_df[merged_df['PatientID'].isin(test_pids)].copy()\n","\n","# --- STEP B: Feature Selection (Train Only) ---\n","# Identify genus columns\n","all_genus_cols = genus_pivot.columns.drop(['PatientID', 'SampleID', 'DayRelativeToNearestHCT']).tolist()\n","\n","# Calculate variance ONLY on Training data to avoid leakage\n","train_variances = df_train[all_genus_cols].var()\n","# Drop columns with near-zero variance in training set\n","non_zero_var_cols = train_variances[train_variances > 1e-6].index.tolist()\n","\n","# Define final feature list (Microbiome + Stool Consistency)\n","# Exclude Temp/Neutrophils from Input X (as they define the label Y)\n","\n","#Changed on 02 December, 2025\n","# feature_cols = non_zero_var_cols + [col for col in merged_df.columns if 'Consistency' in col]\n","\n","# === CRITICAL FIX ===\n","# Input Features = Microbiome ONLY.\n","# We REMOVE 'Consistency' because it is part of the Label definition (Leakage).\n","feature_cols = non_zero_var_cols\n","# feature_cols += [col for col in merged_df.columns if 'Consistency' in col] <--- REMOVED THIS LINE\n","\n","print(f\"Selected {len(feature_cols)} features (Microbiome Genus Only).\")\n","\n","#print(f\"Selected {len(feature_cols)} features based on Training Set variance.\")\n","\n","# --- STEP C: Scaling (Fit on Train Only) ---\n","scaler = MinMaxScaler()\n","\n","# 1. FIT and TRANSFORM on Training\n","df_train[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n","\n","# 2. TRANSFORM Only on Val/Test (using Train statistics)\n","df_val[feature_cols] = scaler.transform(df_val[feature_cols])\n","df_test[feature_cols] = scaler.transform(df_test[feature_cols])\n","\n","print(\"Scaling complete. Data leakage prevented.\")"],"metadata":{"id":"cCJu8UgOBkh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Sequence Generation (Sliding Window)\n","\n","def build_sequences(df, feature_cols, label_col='DysbiosisLabel', seq_len=14):\n","    \"\"\"\n","    Generates sequences strictly within patient groups.\n","    \"\"\"\n","    X_sequences = []\n","    y_labels = []\n","\n","    # Group by patient to ensure window never crosses patient boundaries\n","    for pid, group in df.groupby('PatientID'):\n","        # Sort by time\n","        group = group.sort_values('DayRelativeToNearestHCT')\n","\n","        values = group[feature_cols].values\n","        labels = group[label_col].values\n","\n","        # Sliding window\n","        if len(values) >= seq_len:\n","            for i in range(len(values) - seq_len + 1):\n","                seq = values[i:i+seq_len]\n","                label_window = labels[i:i+seq_len]\n","\n","                # Label Logic: If ANY point in window is dysbiotic, label=1\n","                # (Or use label_window[-1] for \"next step prediction\")\n","                label = int(label_window.max())\n","\n","                X_sequences.append(seq)\n","                y_labels.append(label)\n","\n","    return np.array(X_sequences), np.array(y_labels)\n","\n","# Build sequences for each split independently\n","SEQ_LEN = 14\n","\n","X_train, y_train = build_sequences(df_train, feature_cols, seq_len=SEQ_LEN)\n","X_val, y_val = build_sequences(df_val, feature_cols, seq_len=SEQ_LEN)\n","X_test, y_test = build_sequences(df_test, feature_cols, seq_len=SEQ_LEN)\n","\n","print(f\"Training Sequences: {X_train.shape}\")\n","print(f\"Validation Sequences: {X_val.shape}\")\n","print(f\"Testing Sequences: {X_test.shape}\")"],"metadata":{"id":"dVlZAmPBBnYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 5. Train LSTM Autoencoder\n","# Filter Training Data: Autoencoder only sees \"Normal\" (Label=0) sequences\n","X_train_normal = X_train[y_train == 0]\n","\n","# For Validation, we can use X_val_normal to check reconstruction of normal unseen data\n","X_val_normal = X_val[y_val == 0]\n","\n","timesteps = X_train_normal.shape[1]\n","n_features = X_train_normal.shape[2]\n","\n","print(f\"Training on {len(X_train_normal)} normal sequences.\")\n","\n","# --- Architecture ---\n","inputs = Input(shape=(timesteps, n_features))\n","# Encoder\n","encoded = LSTM(128, return_sequences=True)(inputs)\n","encoded = LSTM(64, return_sequences=False)(encoded)\n","# Bottleneck\n","bottleneck = RepeatVector(timesteps)(encoded)\n","# Decoder\n","decoded = LSTM(64, return_sequences=True)(bottleneck)\n","decoded = LSTM(128, return_sequences=True)(decoded)\n","outputs = TimeDistributed(Dense(n_features))(decoded)\n","\n","# Compile\n","autoencoder = Model(inputs, outputs)\n","autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n","\n","# --- Callbacks ---\n","early_stop = EarlyStopping(\n","    monitor='val_loss',\n","    patience=10,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","# 1. Create the 'models' folder if it doesn't exist (Critical step!)\n","if not os.path.exists('./models'):\n","    os.makedirs('./models')\n","\n","# 2. Update the checkpoint to save INSIDE that folder\n","checkpoint_cb = ModelCheckpoint(\n","    filepath=\"./models/DynaBiome_PatientSplit_Model.keras\",  # <--- UPDATED PATH\n","    monitor=\"val_loss\",\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","# --- Fit ---\n","# Note: We validate on X_val_normal to see how well it reconstructs unseen HEALTHY patients.\n","history = autoencoder.fit(\n","    X_train_normal, X_train_normal,\n","    epochs=150, # 150-300 is usually sufficient\n","    batch_size=32,\n","    validation_data=(X_val_normal, X_val_normal),\n","    callbacks=[early_stop, checkpoint_cb],\n","    verbose=1\n",")\n","\n","# Plot Loss\n","plt.figure(figsize=(10,6))\n","plt.plot(history.history['loss'], label='Train Loss (Normal)')\n","plt.plot(history.history['val_loss'], label='Val Loss (Normal)')\n","plt.title('LSTM Autoencoder Reconstruction Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vWAmpCyeBqJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","import os\n","\n","# Update path to point to the 'models' folder\n","model_path = \"./models/DynaBiome_PatientSplit_Model.keras\"\n","\n","try:\n","    files.download(model_path)\n","    print(f\"Successfully downloaded {model_path}\")\n","except Exception as e:\n","    print(f\"Error: {e}\")\n","    print(f\"Could not find file at {model_path}. Did you save it to the './models' folder?\")"],"metadata":{"id":"3W-RhjI-IRrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming 'autoencoder_model' is your trained autoencoder\n","# Assuming 'X_val' is your validation data\n","\n","# Predict the reconstruction of X_val using the trained autoencoder\n","X_val_pred = autoencoder.predict(X_val)\n","\n","# Calculate the Mean Absolute Error (MAE) between the original X_val and its reconstruction\n","reconstruction_errors_val = np.mean(np.abs(X_val_pred - X_val), axis=(1, 2))\n","\n","# reconstruction_errors_val now contains a single reconstruction error value for each sequence in X_val\n","\n","threshold = 0.0003 # np.percentile(reconstruction_errors_val, 95)\n","# Newly Added on 11 May, 2025\n","# predicted_labels = (reconstruction_errors >= threshold).astype(int)\n","\n","print(f\"\\nAnomaly detection threshold (95th percentile): {threshold:.6f}\")"],"metadata":{"id":"QHIvdTnbXOd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test_pred = autoencoder.predict(X_test)\n","reconstruction_errors_test = np.mean(np.abs(X_test_pred - X_test), axis=(1, 2))\n","anomaly_predictions = reconstruction_errors_test > threshold  # threshold from training on validation set. Use the Threshold obtained via validation set\n","print(f\"Number of flagged sequences: {anomaly_predictions.sum()} out of {len(anomaly_predictions)}\")"],"metadata":{"id":"2iOv2LVEjbnb"},"execution_count":null,"outputs":[]},{"source":["from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, confusion_matrix\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","# --- 1. Find Optimal Threshold on the Validation Set ---\n","\n","# Compute ROC Curve on Validation Set to get thresholds\n","fpr_val, tpr_val, thresholds_val = roc_curve(y_val, reconstruction_errors_val)\n","\n","# Find the best threshold using Youden's index on Validation Set\n","optimal_idx_val = np.argmax(tpr_val - fpr_val)\n","optimal_threshold = thresholds_val[optimal_idx_val]\n","\n","print(f\"Optimal Threshold for anomaly detection (from Validation Set): {optimal_threshold:.4f}\")\n","\n","# --- 2. Evaluate Performance and Plot on the Test Set using the Optimal Threshold ---\n","\n","# Classify Test Instances using the optimal threshold found on the validation set\n","anomaly_predictions_test = (reconstruction_errors_test > optimal_threshold).astype(int)\n","\n","# Compute Evaluation Metrics on the Test Set\n","roc_auc_test = roc_auc_score(y_test, reconstruction_errors_test) # AUC is calculated from scores, not binary predictions\n","pr_auc_test = average_precision_score(y_test, reconstruction_errors_test) # PR AUC is calculated from scores\n","\n","# You can also calculate metrics based on the binary predictions\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","precision_test = precision_score(y_test, anomaly_predictions_test, zero_division=1)\n","recall_test = recall_score(y_test, anomaly_predictions_test, zero_division=1)\n","f1_test = f1_score(y_test, anomaly_predictions_test, zero_division=1)\n","\n","print(\"\\nEvaluation on Test Set using Optimal Threshold from Validation:\")\n","print(f\"Precision: {precision_test:.4f}\")\n","print(f\"Recall: {recall_test:.4f}\")\n","print(f\"F1-Score: {f1_test:.4f}\")\n","print(f\"ROC AUC: {roc_auc_test:.4f}\")\n","print(f\"PR AUC: {pr_auc_test:.4f}\")\n","print(\"\\nClassification Report on Test Set:\")\n","print(classification_report(y_test, anomaly_predictions_test, zero_division=1))\n","\n","\n","# --- 3. Plot ROC Curve on the Test Set ---\n","\n","# Compute ROC Curve on Test Set (for plotting the curve itself)\n","fpr_test, tpr_test, thresholds_test_plot = roc_curve(y_test, reconstruction_errors_test)\n","\n","plt.figure(figsize=(12, 5))\n","\n","# --- ROC ---\n","plt.subplot(1, 2, 1)\n","plt.plot(fpr_test, tpr_test, label=f'ROC AUC = {roc_auc_test:.3f}', color='blue')\n","\n","# Find the point on the Test ROC curve corresponding to the optimal threshold from Validation\n","# This requires finding the index in thresholds_test_plot closest to optimal_threshold\n","closest_threshold_idx_test = np.argmin(np.abs(thresholds_test_plot - optimal_threshold))\n","plt.scatter(fpr_test[closest_threshold_idx_test], tpr_test[closest_threshold_idx_test],\n","            color='red', label=f'Applied Threshold ({optimal_threshold:.4f})', zorder=5) # zorder to ensure point is visible\n","\n","plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve (Evaluated on Test Set)\")\n","plt.legend()\n","plt.grid(True)\n","\n","# Save the ROC plot\n","# Define a suitable output directory if not already defined\n","output_dir_evaluation_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Evaluation_Plots/\"\n","import os\n","os.makedirs(output_dir_evaluation_plots, exist_ok=True)\n","\n","plt.savefig(f\"{output_dir_evaluation_plots}/ROC_Curve_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n","\n","# --- 4. Plot Precision-Recall Curve on the Test Set ---\n","\n","# Compute PR Curve on Test Set (for plotting the curve itself)\n","precision_test_plot, recall_test_plot, thresholds_pr_test_plot = precision_recall_curve(y_test, reconstruction_errors_test)\n","\n","plt.subplot(1, 2, 2) # Plotting PR curve in the second subplot\n","plt.plot(recall_test_plot, precision_test_plot, label=f'PR AUC = {pr_auc_test:.3f}', color='red')\n","\n","# Find the point on the Test PR curve corresponding to the optimal threshold from Validation\n","# Similar to ROC, find the index in thresholds_pr_test_plot closest to optimal_threshold\n","# Note: precision_recall_curve thresholds are slightly different in number/values than roc_curve\n","closest_threshold_idx_pr_test = np.argmin(np.abs(thresholds_pr_test_plot - optimal_threshold))\n","# Need to handle the case where thresholds_pr_test_plot might be empty (e.g., all samples are the same)\n","if thresholds_pr_test_plot.size > 0:\n","     plt.scatter(recall_test_plot[closest_threshold_idx_pr_test], precision_test_plot[closest_threshold_idx_pr_test],\n","                 color='blue', label=f'Applied Threshold ({optimal_threshold:.4f})', zorder=5)\n","\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve (Evaluated on Test Set)')\n","plt.legend()\n","plt.grid(True)\n","\n","# Save the PR plot\n","plt.savefig(f\"{output_dir_evaluation_plots}/PR_Curve_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n","\n","\n","plt.tight_layout() # Adjust layout to prevent overlap\n","plt.show()\n","\n","# --- 5. Plot Confusion Matrix on the Test Set ---\n","cm_test = confusion_matrix(y_test, anomaly_predictions_test)\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n","            yticklabels=['Actual Normal', 'Actual Anomaly'])\n","plt.title('Confusion Matrix (Evaluated on Test Set)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.savefig(f\"{output_dir_evaluation_plots}/ConfusionMatrix_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"m1-yudzG8tNh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================== STEP 6: Visualization ====================\n","plt.figure(figsize=(10,4))\n","plt.hist(reconstruction_errors_test, bins=50, color='skyblue', edgecolor='black', density=True, alpha=0.7) # Use density=True for better area representation\n","\n","plt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Threshold ({optimal_threshold:.4f})')\n","\n","# Shade the areas\n","# Normal region (errors <= threshold)\n","plt.fill_betweenx([0, plt.gca().get_ylim()[1]], 0, optimal_threshold, color='green', alpha=0.2, label='Normal')\n","# Anomalous region (errors > threshold)\n","plt.fill_betweenx([0, plt.gca().get_ylim()[1]], optimal_threshold, plt.gca().get_xlim()[1], color='red', alpha=0.2, label='Anomalous')\n","\n","\n","plt.title(\"Reconstruction Error Distribution with Anomaly Threshold\")\n","plt.xlabel(\"Reconstruction Error (MAE)\")\n","plt.ylabel(\"Density\") # Changed to Density if density=True\n","plt.legend()\n","plt.tight_layout()\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/BenchMark_Figures/reconstruction_error_distribution.pdf\", dpi=600, bbox_inches='tight')\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/reconstruction_error_distribution_OriginalApproach_Thresholded.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"9jFGeLad9uat"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ajvOSZXVXqqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test_pred = autoencoder.predict(X_test)\n","reconstruction_errors_test = np.mean(np.abs(X_test_pred - X_test), axis=(1, 2))\n","anomaly_predictions = reconstruction_errors_test > threshold  # threshold from training on validation set. Use the Threshold obtained via validation set\n","print(f\"Number of flagged sequences: {anomaly_predictions.sum()} out of {len(anomaly_predictions)}\")"],"metadata":{"id":"e6OgcGkyXrUn"},"execution_count":null,"outputs":[]},{"source":["from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, confusion_matrix\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","# --- 1. Find Optimal Threshold on the Validation Set ---\n","\n","# Compute ROC Curve on Validation Set to get thresholds\n","fpr_val, tpr_val, thresholds_val = roc_curve(y_val, reconstruction_errors_val)\n","\n","# Find the best threshold using Youden's index on Validation Set\n","optimal_idx_val = np.argmax(tpr_val - fpr_val)\n","optimal_threshold = thresholds_val[optimal_idx_val]\n","\n","print(f\"Optimal Threshold for anomaly detection (from Validation Set): {optimal_threshold:.4f}\")\n","\n","# --- 2. Evaluate Performance and Plot on the Test Set using the Optimal Threshold ---\n","\n","# Classify Test Instances using the optimal threshold found on the validation set\n","anomaly_predictions_test = (reconstruction_errors_test > optimal_threshold).astype(int)\n","\n","# Compute Evaluation Metrics on the Test Set\n","roc_auc_test = roc_auc_score(y_test, reconstruction_errors_test) # AUC is calculated from scores, not binary predictions\n","pr_auc_test = average_precision_score(y_test, reconstruction_errors_test) # PR AUC is calculated from scores\n","\n","# You can also calculate metrics based on the binary predictions\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","precision_test = precision_score(y_test, anomaly_predictions_test, zero_division=1)\n","recall_test = recall_score(y_test, anomaly_predictions_test, zero_division=1)\n","f1_test = f1_score(y_test, anomaly_predictions_test, zero_division=1)\n","\n","print(\"\\nEvaluation on Test Set using Optimal Threshold from Validation:\")\n","print(f\"Precision: {precision_test:.4f}\")\n","print(f\"Recall: {recall_test:.4f}\")\n","print(f\"F1-Score: {f1_test:.4f}\")\n","print(f\"ROC AUC: {roc_auc_test:.4f}\")\n","print(f\"PR AUC: {pr_auc_test:.4f}\")\n","print(\"\\nClassification Report on Test Set:\")\n","print(classification_report(y_test, anomaly_predictions_test, zero_division=1))\n","\n","\n","# --- 3. Plot ROC Curve on the Test Set ---\n","\n","# Compute ROC Curve on Test Set (for plotting the curve itself)\n","fpr_test, tpr_test, thresholds_test_plot = roc_curve(y_test, reconstruction_errors_test)\n","\n","plt.figure(figsize=(12, 5))\n","\n","# --- ROC ---\n","plt.subplot(1, 2, 1)\n","plt.plot(fpr_test, tpr_test, label=f'ROC AUC = {roc_auc_test:.3f}', color='blue')\n","\n","# Find the point on the Test ROC curve corresponding to the optimal threshold from Validation\n","# This requires finding the index in thresholds_test_plot closest to optimal_threshold\n","closest_threshold_idx_test = np.argmin(np.abs(thresholds_test_plot - optimal_threshold))\n","plt.scatter(fpr_test[closest_threshold_idx_test], tpr_test[closest_threshold_idx_test],\n","            color='red', label=f'Applied Threshold ({optimal_threshold:.4f})', zorder=5) # zorder to ensure point is visible\n","\n","plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve (Evaluated on Test Set)\")\n","plt.legend()\n","plt.grid(True)\n","\n","# Save the ROC plot\n","# Define a suitable output directory if not already defined\n","output_dir_evaluation_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Evaluation_Plots/\"\n","import os\n","os.makedirs(output_dir_evaluation_plots, exist_ok=True)\n","\n","plt.savefig(f\"{output_dir_evaluation_plots}/ROC_Curve_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n","\n","# --- 4. Plot Precision-Recall Curve on the Test Set ---\n","\n","# Compute PR Curve on Test Set (for plotting the curve itself)\n","precision_test_plot, recall_test_plot, thresholds_pr_test_plot = precision_recall_curve(y_test, reconstruction_errors_test)\n","\n","plt.subplot(1, 2, 2) # Plotting PR curve in the second subplot\n","plt.plot(recall_test_plot, precision_test_plot, label=f'PR AUC = {pr_auc_test:.3f}', color='red')\n","\n","# Find the point on the Test PR curve corresponding to the optimal threshold from Validation\n","# Similar to ROC, find the index in thresholds_pr_test_plot closest to optimal_threshold\n","# Note: precision_recall_curve thresholds are slightly different in number/values than roc_curve\n","closest_threshold_idx_pr_test = np.argmin(np.abs(thresholds_pr_test_plot - optimal_threshold))\n","# Need to handle the case where thresholds_pr_test_plot might be empty (e.g., all samples are the same)\n","if thresholds_pr_test_plot.size > 0:\n","     plt.scatter(recall_test_plot[closest_threshold_idx_pr_test], precision_test_plot[closest_threshold_idx_pr_test],\n","                 color='blue', label=f'Applied Threshold ({optimal_threshold:.4f})', zorder=5)\n","\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve (Evaluated on Test Set)')\n","plt.legend()\n","plt.grid(True)\n","\n","# Save the PR plot\n","plt.savefig(f\"{output_dir_evaluation_plots}/PR_Curve_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n","\n","\n","plt.tight_layout() # Adjust layout to prevent overlap\n","plt.show()\n","\n","# --- 5. Plot Confusion Matrix on the Test Set ---\n","cm_test = confusion_matrix(y_test, anomaly_predictions_test)\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n","            yticklabels=['Actual Normal', 'Actual Anomaly'])\n","plt.title('Confusion Matrix (Evaluated on Test Set)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.savefig(f\"{output_dir_evaluation_plots}/ConfusionMatrix_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"wNiKpp9yXrUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================== STEP 6: Visualization ====================\n","plt.figure(figsize=(10,4))\n","plt.hist(reconstruction_errors_test, bins=50, color='skyblue', edgecolor='black', density=True, alpha=0.7) # Use density=True for better area representation\n","\n","plt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Threshold ({optimal_threshold:.4f})')\n","\n","# Shade the areas\n","# Normal region (errors <= threshold)\n","plt.fill_betweenx([0, plt.gca().get_ylim()[1]], 0, optimal_threshold, color='green', alpha=0.2, label='Normal')\n","# Anomalous region (errors > threshold)\n","plt.fill_betweenx([0, plt.gca().get_ylim()[1]], optimal_threshold, plt.gca().get_xlim()[1], color='red', alpha=0.2, label='Anomalous')\n","\n","\n","plt.title(\"Reconstruction Error Distribution with Anomaly Threshold\")\n","plt.xlabel(\"Reconstruction Error (MAE)\")\n","plt.ylabel(\"Density\") # Changed to Density if density=True\n","plt.legend()\n","plt.tight_layout()\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/BenchMark_Figures/reconstruction_error_distribution.pdf\", dpi=600, bbox_inches='tight')\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/reconstruction_error_distribution_OriginalApproach_Thresholded.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"-fqNT-WLXrUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set global plot style for publication\n","sns.set(style=\"whitegrid\")\n","plt.rcParams.update({\n","    'font.family': 'serif',\n","    'axes.titlesize': 18,\n","    'axes.labelsize': 16,\n","    'axes.labelweight': 'bold',  # Bold axis labels\n","    'axes.titleweight': 'bold',  # Bold title\n","    'xtick.labelsize': 13,\n","    'ytick.labelsize': 13,\n","    'figure.dpi': 600  # High resolution\n","})\n","\n","# Reconstruct predictions\n","X_pred_test = autoencoder.predict(X_test)\n","reconstruction_errors = np.mean(np.abs(X_pred_test - X_test), axis=(1, 2))\n","feature_errors = np.abs(X_pred_test - X_test)\n","feature_errors_avg = feature_errors.mean(axis=1)\n","\n","# Anomaly filtering\n","anomaly_flags = reconstruction_errors > optimal_threshold\n","anomalous_feature_errors = feature_errors_avg[anomaly_flags]\n","mean_error_per_feature = anomalous_feature_errors.mean(axis=0)\n","\n","# DataFrame preparation\n","error_df = pd.DataFrame({\n","    'Feature': feature_cols,\n","    'MeanAnomalyError': mean_error_per_feature\n","}).sort_values(by='MeanAnomalyError', ascending=True)\n","\n","# Plotting\n","plt.figure(figsize=(10, 8))\n","barplot = sns.barplot(\n","    data=error_df.tail(20),\n","    x='MeanAnomalyError',\n","    y='Feature',\n","    palette='coolwarm'\n",")\n","\n","# Annotate bars\n","for p in barplot.patches:\n","    width = p.get_width()\n","    barplot.text(\n","        width + 0.005,\n","        p.get_y() + p.get_height() / 2,\n","        f'{width:.3f}',\n","        ha='left',\n","        va='center',\n","        fontsize=10,\n","        weight='bold'\n","    )\n","\n","# Titles and labels\n","plt.title(\"Top Contributing Genera/Features to Anomalous Patterns\", fontsize=18, weight='bold')\n","plt.xlabel(\"Mean Absolute Error (Anomalous Sequences)\", fontsize=14, weight='bold')\n","plt.ylabel(\"Genus / Feature\", fontsize=14, weight='bold')\n","\n","plt.tight_layout()\n","\n","# Save as high-quality figure\n","plt.savefig(\"Top_Contributing_Genera.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"6d_8G5rNhY5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set global plot style for publication\n","sns.set(style=\"whitegrid\")\n","plt.rcParams.update({\n","    'font.family': 'serif',\n","    'axes.titlesize': 18,\n","    'axes.labelsize': 16,\n","    'axes.labelweight': 'bold',\n","    'axes.titleweight': 'bold',\n","    'xtick.labelsize': 13,\n","    'ytick.labelsize': 14,         # Increased for clarity\n","    'figure.dpi': 600              # High resolution\n","})\n","\n","# Reconstruct predictions\n","X_pred_test = autoencoder.predict(X_test)\n","reconstruction_errors = np.mean(np.abs(X_pred_test - X_test), axis=(1, 2))\n","feature_errors = np.abs(X_pred_test - X_test)\n","feature_errors_avg = feature_errors.mean(axis=1)\n","\n","# Anomaly filtering\n","anomaly_flags = reconstruction_errors > optimal_threshold\n","anomalous_feature_errors = feature_errors_avg[anomaly_flags]\n","mean_error_per_feature = anomalous_feature_errors.mean(axis=0)\n","\n","# DataFrame preparation\n","error_df = pd.DataFrame({\n","    'Feature': feature_cols,\n","    'MeanAnomalyError': mean_error_per_feature\n","}).sort_values(by='MeanAnomalyError', ascending=True)\n","\n","# Plotting\n","plt.figure(figsize=(10, 8))\n","barplot = sns.barplot(\n","    data=error_df.tail(20),\n","    x='MeanAnomalyError',\n","    y='Feature',\n","    palette='coolwarm'\n",")\n","\n","# Annotate bars\n","for p in barplot.patches:\n","    width = p.get_width()\n","    barplot.text(\n","        width + 0.005,\n","        p.get_y() + p.get_height() / 2,\n","        f'{width:.3f}',\n","        ha='left',\n","        va='center',\n","        fontsize=10,\n","        weight='bold'\n","    )\n","\n","# Improve y-axis label readability\n","barplot.set_yticklabels(\n","    barplot.get_yticklabels(),\n","    fontsize=14,\n","    weight='bold',\n","    color='black'\n",")\n","\n","# Titles and labels\n","plt.title(\"Top Contributing Genera/Features to Anomalous Patterns\", fontsize=18, weight='bold')\n","plt.xlabel(\"Mean Absolute Error (Anomalous Sequences)\", fontsize=14, weight='bold')\n","plt.ylabel(\"Genus / Feature\", fontsize=14, weight='bold')\n","\n","plt.tight_layout()\n","\n","# Save as high-quality figure\n","plt.savefig(\"Top_Contributing_Genera.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"dcN_pib5kFZP"},"execution_count":null,"outputs":[]},{"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import f1_score\n","\n","# Assuming you have reconstruction_errors_val and y_val\n","\n","# Define the parameter grid for Logistic Regression\n","# Ensure compatible combinations of penalty and solver\n","param_grid = [\n","    {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","     'penalty': ['l1', 'l2'],\n","     'solver': ['liblinear', 'saga']}, # liblinear and saga support l1/l2\n","    {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","     'penalty': ['elasticnet'],\n","     'solver': ['saga'], # Only saga supports elasticnet\n","     'l1_ratio': [0.1, 0.5, 0.9]}, # l1_ratio is needed for elasticnet\n","     {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","      'penalty': [None], # Change 'none' to None (capitalized)\n","      'solver': ['saga', 'lbfgs']} # saga and lbfgs support None penalty\n","]\n","\n","# Create a GridSearchCV object\n","grid_search = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=5, scoring='f1', n_jobs=-1, error_score='raise') # Using F1-score as the scoring metric\n","\n","# Fit the grid search to the validation data\n","grid_search.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n","\n","# Print the best hyperparameters and best score\n","print(\"Best hyperparameters:\", grid_search.best_params_)\n","print(\"Best F1-score:\", grid_search.best_score_)\n","\n","# You can then use the best model found by grid search to evaluate on the test set\n","best_clf = grid_search.best_estimator_\n","predictions = best_clf.predict(reconstruction_errors_test.reshape(-1, 1))\n","\n","# Evaluate on the test set as you did before\n","print(classification_report(y_test, predictions, zero_division=1))\n","print(roc_auc_score(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]))\n","print(average_precision_score(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]))"],"cell_type":"code","metadata":{"id":"3F04_jfhRWSp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict using trained classifier on test set\n","predictions = best_clf.predict(reconstruction_errors_test.reshape(-1, 1))\n","\n","# Individual Metrics\n","precision = precision_score(y_test, predictions,zero_division=1)\n","recall = recall_score(y_test, predictions)\n","f1 = f1_score(y_test, predictions)\n","roc_auc = roc_auc_score(y_test, reconstruction_errors_test)\n","pr_auc = average_precision_score(y_test, reconstruction_errors_test)\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\") # F1-score for the minority/positive class (Anomaly)\n","print(f\"ROC AUC: {roc_auc:.4f}\")\n","print(f\"PR AUC: {pr_auc:.4f}\")\n","\n","# Calculate weighted average F1-score\n","weighted_f1 = f1_score(y_test, predictions, average='weighted',zero_division=1) #Overall summary\n","print(f\"Weighted Average F1 Score: {weighted_f1:.4f}\")\n","\n","print(\"\\nClassification Report on Test Set (Unsupervised Fashion):\")\n","print(classification_report(y_test, predictions, zero_division=1))"],"metadata":{"id":"xbpFKZkSzawi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Draw PR Curve from pr_auc\n","import os # Ensure os is imported\n","from sklearn.metrics import precision_recall_curve\n","import matplotlib.pyplot as plt\n","\n","# Define the output directory for Logistic Regression plots\n","logistic_regression_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/BenchMark_Figures/LogisticRegression\"\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(logistic_regression_output_dir, exist_ok=True)\n","\n","\n","# Assuming 'reconstruction_errors' and 'true_labels' are defined as in your code\n","precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, marker='.', label=f'PR Curve (AUC = {pr_auc:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend()\n","plt.grid(True)\n","# Use the new directory variable\n","plt.savefig(f\"{logistic_regression_output_dir}/PR_Curve_LR.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"-eWIFQP9qmU_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, precision_recall_curve\n","import matplotlib.pyplot as plt\n","logistic_regression_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/LogisticRegression\"\n","os.makedirs(logistic_regression_output_dir, exist_ok=True)\n","\n","# Calculate fpr and tpr using roc_curve before plotting:\n","fpr, tpr, _ = roc_curve(y_test, predictions)  # Assuming 'y_test' and 'reconstruction_errors' are defined\n","\n","# Now you can proceed with plotting the ROC curve:\n","plt.figure(figsize=(12, 5))\n","\n","# --- ROC ---\n","plt.subplot(1, 2, 1)\n","plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.3f}')\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve\")\n","plt.legend()\n","plt.grid(True)\n","# Save as high-resolution PDF using the defined directory variable\n","plt.savefig(f\"{logistic_regression_output_dir}/ROC_Curve_LogisticRegression.pdf\", dpi=600, bbox_inches='tight')"],"metadata":{"id":"ACnn8cbgqy4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n","import seaborn as sns\n","# Calculate and assign the confusion matrix to 'cm'\n","cm = confusion_matrix(y_test, predictions) # Calculate confusion matrix\n","\n","# Draw Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n","            yticklabels=['Actual Normal', 'Actual Anomaly'])\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/LogisticRegression/PR_Curve_LogisticRegression.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"ukZyWEFh3oim"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Keras Tuner for MLP Hyperparameters"],"metadata":{"id":"9E3SN7ked0WJ"}},{"cell_type":"code","source":["# !pip install keras-tuner\n","# from kerastuner.tuners import RandomSearch\n","\n","# # Function to tune hyperparameters\n","# def tune_mlp_model(hp):\n","#     model = Sequential([\n","#         Dense(hp.Int('neurons1', 32, 128, step=32), activation=hp.Choice('activation', ['relu', 'tanh']), input_shape=(1,)),\n","#         Dense(hp.Int('neurons2', 16, 64, step=16), activation=hp.Choice('activation', ['relu', 'tanh'])),\n","#         Dense(1, activation='sigmoid')\n","#     ])\n","#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","#     return model\n","\n","# # Define tuner\n","# tuner = RandomSearch(\n","#     tune_mlp_model,\n","#     objective='val_accuracy',\n","#     max_trials=10,\n","#     executions_per_trial=1,\n","#     directory='mlp_tuner',\n","#     project_name='mlp_optimization'\n","# )\n","\n","# # Search for best hyperparameters\n","# tuner.search(reconstruction_errors_val.reshape(-1, 1), y_val, epochs=50, validation_split=0.2)\n","\n","# # Get the best hyperparameters\n","# best_hyperparameters = tuner.get_best_hyperparameters()[0]\n","# print(\"Best Hyperparameters:\", best_hyperparameters.values)\n"],"metadata":{"id":"joF47mBHbewR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Non-Linear Classifier (MLP on reconstruction errors)"],"metadata":{"id":"CCrPpdBWrlqW"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Input\n","\n","# Use best hyperparameters found via tuning\n","input_layer = Input(shape=(1,))\n","dense_1 = Dense(96, activation='relu')(input_layer)  # Update neurons1 to 96\n","dense_2 = Dense(64, activation='relu')(dense_1)  # Update neurons2 to 64\n","output_layer = Dense(1, activation='sigmoid')(dense_2)\n","\n","# Define the optimized model\n","mlp_model = Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compile the model\n","mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model with the same dataset but use best hyperparameters\n","mlp_model.fit(reconstruction_errors_val.reshape(-1, 1), y_val, epochs=100, batch_size=16)  # Keep epochs & batch_size unchanged"],"metadata":{"id":"xdAlxb8AdL9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from sklearn.metrics import classification_report, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, matthews_corrcoef # Import classification_report and other necessary functions\n","\n","# Define the directory path where the plots will be saved\n","mlp_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP\"\n","\n","# Create the directory if it doesn't exist. exist_ok=True prevents an error if the directory already exists.\n","os.makedirs(mlp_output_dir, exist_ok=True)\n","\n","# Get probabilities from the trained MLP model on the test set reconstruction errors\n","mlp_probs = mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten()\n","\n","# Convert probabilities to binary labels (you can adjust the threshold if needed)\n","threshold = 0.5\n","mlp_pred_classes = (mlp_probs > threshold).astype(int)\n","\n","# Classification report\n","print(\"MLP Classification Report (Trained on Validation, Tested on Test):\")\n","print(classification_report(y_test, mlp_pred_classes, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n","\n","# Compute ROC curve and AUC score\n","fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_probs)\n","roc_auc_mlp = roc_auc_score(y_test, mlp_probs)\n","print(f\"MLP ROC AUC Score: {roc_auc_mlp}\")\n","\n","# Compute Precision-Recall curve\n","precision_mlp, recall_mlp, _ = precision_recall_curve(y_test, mlp_probs)\n","average_precision_mlp = average_precision_score(y_test, mlp_probs)\n","print(f\"MLP Average Precision: {average_precision_mlp}\")\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_mlp, tpr_mlp, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_mlp:.2f})')\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve - MLP on Reconstruction Error (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower right\")\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP/ROC_Curve_MLP.pdf\", dpi=600, bbox_inches='tight')\n","\n","plt.show()\n","\n","# Plot Precision-Recall curve\n","plt.figure(figsize=(8, 5))\n","plt.plot(recall_mlp, precision_mlp, color='red', lw=2, label=f'PR curve (AP = {average_precision_mlp:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve - MLP on Reconstruction Error (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP/PR_Curve_MLP.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Compute Matthews Correlation Coefficient (MCC)\n","mcc_mlp = matthews_corrcoef(y_test, mlp_pred_classes)\n","print(f\"MLP Matthews Correlation Coefficient (MCC): {mcc_mlp}\")"],"metadata":{"id":"xC8FyH8AeZBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: PRINT PR AUC of MLP\n","\n","print(f\"PR AUC of MLP: {average_precision_mlp:.4f}\")\n"],"metadata":{"id":"Td-Z1c8QmbRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Compute Precision, recall and F1 Score on MLP\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Assuming y_test and mlp_pred_classes are already defined from the preceding code\n","\n","# Compute Precision\n","precision_mlp = precision_score(y_test, mlp_pred_classes, zero_division=1)\n","\n","# Compute Recall\n","recall_mlp = recall_score(y_test, mlp_pred_classes, zero_division=1)\n","\n","# Compute F1 Score\n","f1_mlp = f1_score(y_test, mlp_pred_classes, zero_division=1)\n","\n","print(f\"MLP Precision: {precision_mlp:.4f}\")\n","print(f\"MLP Recall: {recall_mlp:.4f}\")\n","print(f\"MLP F1 Score: {f1_mlp:.4f}\")"],"metadata":{"id":"aC0XFhqhl2_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Generate confusion matrix for mlp\n","\n","# Compute confusion matrix for MLP predictions\n","cm_mlp = confusion_matrix(y_test, mlp_pred_classes)\n","\n","# Display confusion matrix using seaborn heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_mlp, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n","plt.title('Confusion Matrix - MLP on Reconstruction Error (Trained Val, Tested Test)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP/ConfusionMatrix_MLP.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"swzEfntj36Y9"},"execution_count":null,"outputs":[]},{"source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.svm import OneClassSVM\n","from sklearn.metrics import make_scorer, f1_score\n","import numpy as np # Import numpy\n","\n","# Define custom scoring function\n","def custom_f1_scorer(estimator, X, y):\n","    anomaly_scores = estimator.decision_function(X)\n","    # In One-Class SVM, lower scores mean more anomalous.\n","    # We want to find a threshold such that a certain percentage of the *training* data\n","    # (which is assumed to be mostly normal) is considered anomalies.\n","    # The 5th percentile means the 5% lowest scores are considered anomalous.\n","    # Since decision_function gives higher values for inliers, we threshold on the low side.\n","    threshold = np.percentile(anomaly_scores, 5)\n","    anomaly_predictions = anomaly_scores < threshold # Anomalies are below the threshold\n","    # Only evaluate F1 on the actual anomalies in the test set.\n","    # For RandomizedSearchCV during validation, we score against y_val\n","    return f1_score(y, anomaly_predictions)\n","\n","\n","# Define the hyperparameter search space\n","param_grid = {\n","    'nu': np.linspace(0.01, 0.2, 10), # nu is an upper bound on the fraction of training errors\n","    'gamma': ['scale', 'auto', 0.01, 0.1, 1] # Kernel coefficient\n","}\n","\n","# Set up RandomizedSearchCV\n","ocsvm = OneClassSVM(kernel=\"rbf\")\n","# Use make_scorer with our custom F1 function, setting greater_is_better=True (default)\n","random_search = RandomizedSearchCV(ocsvm, param_grid, scoring=make_scorer(custom_f1_scorer), n_iter=10, cv=3, n_jobs=-1)\n","\n","# *** Calculate reconstruction errors for the training data ***\n","# Assuming 'autoencoder' and 'X_train' are available from previous steps\n","X_train_pred = autoencoder.predict(X_train)\n","reconstruction_errors_train = np.mean(np.abs(X_train_pred - X_train), axis=(1, 2))\n","\n","\n","# Fit search on training data's reconstruction errors\n","# The y_train is used by the custom_f1_scorer during cross-validation\n","# to calculate the F1 score on the validation folds within the training data.\n","random_search.fit(reconstruction_errors_train.reshape(-1, 1), y_train)\n","\n","# Show best parameters\n","print(\"Best Hyperparameters:\", random_search.best_params_)"],"cell_type":"code","metadata":{"id":"Z8lJ6O-9e2q4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Once Class SVM on reconstruction errors"],"metadata":{"id":"Jwb4R2WVwuHs"}},{"cell_type":"code","source":["from sklearn.svm import OneClassSVM\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, roc_curve, auc, precision_recall_curve\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler # Or StandardScaler\n","\n","# 1. Train OCSVM on Reconstruction Errors of Normal Data\n","# Assuming 'reconstruction_errors_train' contains errors for normal training data\n","\n","# 1. Calculate Reconstruction Errors for Training Data\n","X_train_pred = autoencoder.predict(X_train)  # Predict on training data\n","reconstruction_errors_train = np.mean(np.abs(X_train_pred - X_train), axis=(1, 2))\n","\n","# Use best hyperparameters\n","best_nu = 0.0944\n","best_gamma = 0.01\n","\n","# Train One-Class SVM on Reconstruction Errors of Normal Data\n","ocsvm = OneClassSVM(nu=best_nu, kernel=\"rbf\", gamma=best_gamma)\n","ocsvm.fit(reconstruction_errors_train.reshape(-1, 1))\n","\n","# 2. Predict Anomaly Scores on Test Data\n","anomaly_scores = ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1))\n","# Lower scores indicate higher anomaly probability\n","\n","# 3. Thresholding for Anomaly Detection\n","threshold = np.percentile(anomaly_scores, 5)  # Adjust percentile for desired sensitivity\n","anomaly_predictions = anomaly_scores < threshold # Points below threshold are anomalies\n","\n","# 4. Evaluate Performance and Print Classification Report\n","print(\"\\nOne-Class SVM Classification Report:\")\n","print(classification_report(y_test, anomaly_predictions))\n","\n","# Define the output directory for SVM plots\n","svm_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm\"\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(svm_output_dir, exist_ok=True)\n","\n","# 5. Plot ROC Curve\n","fpr, tpr, _ = roc_curve(y_test, -anomaly_scores) # Use negative scores for ROC (higher score = more anomalous)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic for One-Class SVM')\n","plt.legend(loc=\"lower right\")\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm/RoC_Curve_SVM.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# 6. Plot PR Curve\n","precision, recall, _ = precision_recall_curve(y_test, -anomaly_scores) # Use negative scores for PR (higher score = more anomalous)\n","pr_auc = auc(recall, precision)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve for One-Class SVM')\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm/PR_Curve_SVM.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"lUhxExVGgkPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Create confusion matrix for one class SVM Classification\n","from sklearn.metrics import confusion_matrix # Import confusion_matrix here\n","import seaborn as sns # Import seaborn for heatmap visualization\n","import matplotlib.pyplot as plt # Import matplotlib for plotting\n","\n","# Calculate and assign the confusion matrix for One-Class SVM to 'cm_ocsvm'\n","cm_ocsvm = confusion_matrix(y_test, anomaly_predictions)\n","\n","# Draw Confusion Matrix for One-Class SVM\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_ocsvm, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n","            yticklabels=['Actual Normal', 'Actual Anomaly'])\n","plt.title('Confusion Matrix - One-Class SVM (Scaled Errors)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"fB8KYh-f4oOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# Define KNN classifier\n","knn = KNeighborsClassifier()\n","\n","# Corrected hyperparameter search space\n","param_grid = {\n","    'n_neighbors': [3, 5, 7, 9, 11, 15],\n","    'weights': ['uniform', 'distance'],\n","    'metric': ['euclidean', 'manhattan', 'minkowski'],\n","    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n","}\n","\n","# Perform Randomized Search\n","random_search = RandomizedSearchCV(knn, param_grid, cv=3, scoring='accuracy', n_iter=10, n_jobs=-1)\n","random_search.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n","\n","# Print best parameters\n","print(\"Best Hyperparameters:\", random_search.best_params_)\n"],"metadata":{"id":"UL4kh0Hrg2x0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# K-Nearest Neighbors Classifier"],"metadata":{"id":"XEEetiLz4vtl"}},{"source":["# prompt: Add K-Nearest Neighbors Classifier\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, precision_recall_curve\n","from sklearn.metrics import confusion_matrix # Import confusion_matrix\n","import seaborn as sns # Import seaborn\n","\n","# Assuming you have already computed:\n","# reconstruction_errors_val, y_val\n","# reconstruction_errors_test, y_test\n","\n","# Train K-Nearest Neighbors Classifier\n","# You might want to tune the 'n_neighbors' parameter\n","#knn = KNeighborsClassifier(n_neighbors=5)\n","# Use the best hyperparameters found\n","knn = KNeighborsClassifier(weights='distance', n_neighbors=9, metric='minkowski', algorithm='brute')\n","\n","# **Corrected:** Train on validation set reconstruction errors and labels\n","knn.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n","\n","# Predict probabilities on Test Set Reconstruction Errors\n","# Use the predict_proba method on the trained KNN model\n","knn_probs = knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n","\n","# Get binary predictions using a threshold (default is 0.5 for predict_proba)\n","knn_preds = (knn_probs > 0.5).astype(int)\n","\n","# Evaluation on Test Set\n","print(\"\\nK-Nearest Neighbors Classification Report (Trained on Validation, Tested on Test):\\n\",\n","      classification_report(y_test, knn_preds, digits=4, zero_division=1))\n","\n","# Compute ROC AUC\n","# Use the probabilities for AUC calculation\n","roc_auc_knn = roc_auc_score(y_test, knn_probs)\n","print(\"K-Nearest Neighbors ROC AUC (Trained on Validation, Tested on Test):\", roc_auc_knn)\n","\n","# Compute Average Precision (PR AUC)\n","# Use the probabilities for PR AUC calculation\n","average_precision_knn = average_precision_score(y_test, knn_probs)\n","print(\"K-Nearest Neighbors Average Precision (PR AUC) (Trained on Validation, Tested on Test):\", average_precision_knn)\n","\n","# Calculate Confusion Matrix for KNN\n","cm_knn = confusion_matrix(y_test, knn_preds)\n","\n","# Draw Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_knn, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n","plt.title('Confusion Matrix - K-Nearest Neighbors on Reconstruction Error (Trained Val, Tested Test)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/K_Nearest_Neighbor/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","\n","# Plot ROC curve\n","fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_knn, tpr_knn, color='green', lw=2, label=f'KNN ROC curve (AUC = {roc_auc_knn:.2f})')\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve - K-Nearest Neighbors on Reconstruction Error (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/K_Nearest_Neighbor/KNN_ROC_curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot Precision-Recall curve\n","precision_knn, recall_knn, _ = precision_recall_curve(y_test, knn_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(recall_knn, precision_knn, color='purple', lw=2, label=f'KNN PR curve (AP = {average_precision_knn:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve - K-Nearest Neighbors on Reconstruction Error (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/K_Nearest_Neighbor/PR_curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"ArBpnkjs8IPl"},"execution_count":null,"outputs":[]},{"source":["!pip install xgboost"],"cell_type":"code","metadata":{"id":"X7DI0Rk_uLi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import cv, DMatrix\n","\n","# Prepare data in DMatrix format\n","dtrain = DMatrix(reconstruction_errors_val.reshape(-1, 1), label=y_val)\n","\n","# Set parameter space\n","xgb_params = {\n","    'max_depth': 5,\n","    'learning_rate': 0.1,\n","    'objective': 'binary:logistic',\n","    'eval_metric': 'logloss',\n","}\n","\n","# Perform Cross-Validation\n","cv_results = cv(xgb_params, dtrain, num_boost_round=200, nfold=3, metrics=\"logloss\", early_stopping_rounds=10)\n","\n","# Find best number of trees\n","best_n_estimators = cv_results.shape[0]\n","print(f\"Best number of estimators: {best_n_estimators}\")\n"],"metadata":{"id":"6tAwJldKiAPc"},"execution_count":null,"outputs":[]},{"source":["# %%\n","#!pip install xgboost\n","# %%\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns  # Import seaborn\n","\n","# Train XGBoost on Validation Set Reconstruction Errors\n","xgb = XGBClassifier(eval_metric='logloss', n_estimators=200)\n","\n","# **Corrected:** Train on validation set reconstruction errors\n","xgb.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n","\n","# Predict probabilities on Test Set Reconstruction Errors\n","xgb_probs = xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n","xgb_preds = (xgb_probs > 0.5).astype(int)\n","\n","# Evaluation on Test Set\n","print(\"XGBoost Classification Report (Trained on Validation, Tested on Test):\\n\",\n","      classification_report(y_test, xgb_preds, digits=4, zero_division=1))\n","print(\"XGBoost ROC AUC (Trained on Validation, Tested on Test):\",\n","      roc_auc_score(y_test, xgb_probs))\n","print(\"XGBoost Average Precision (PR AUC) (Trained on Validation, Tested on Test):\",\n","      average_precision_score(y_test, xgb_probs))\n","\n","# Calculate Confusion Matrix for XGBoost\n","cm_xgb = confusion_matrix(y_test, xgb_preds)\n","\n","# Draw Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_xgb, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n","plt.title('Confusion Matrix - XGBoost on Reconstruction Error (Trained Val, Tested Test)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/xgboost/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot ROC curve\n","fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probs)\n","roc_auc_xgb = roc_auc_score(y_test, xgb_probs) # Recalculate AUC using test probabilities\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_xgb, tpr_xgb, color='darkorange', lw=2, label=f'XGBoost ROC curve (AUC = {roc_auc_xgb:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic for XGBoost (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/xgboost/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot Precision-Recall curve\n","precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, xgb_probs)\n","pr_auc_xgb = average_precision_score(y_test, xgb_probs) # Recalculate PR AUC using test probabilities\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall_xgb, precision_xgb, color='darkorange', lw=2, label=f'XGBoost PR curve (AP = {pr_auc_xgb:.2f})')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve for XGBoost (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/xgboost/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"xd3_SOz49ZRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Define Random Forest classifier\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Corrected hyperparameter search space (removed 'weights')\n","param_grid = {\n","    'n_estimators': [50, 100, 200, 300],  # Number of trees\n","    'max_depth': [None, 10, 20, 30],  # Maximum depth\n","    'min_samples_split': [2, 5, 10],  # Minimum samples required to split\n","    'min_samples_leaf': [1, 2, 5],  # Minimum samples in a leaf node\n","    'max_features': ['sqrt', 'log2'],  # Number of features per tree\n","    'bootstrap': [True, False]  # Whether to bootstrap samples\n","}\n","\n","# Perform Randomized Search\n","random_search = RandomizedSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_iter=10, n_jobs=-1)\n","random_search.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n","\n","# Print best parameters\n","print(\"Best Hyperparameters:\", random_search.best_params_)\n"],"metadata":{"id":"1SNYWFNmnnWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forest Classifier"],"metadata":{"id":"E26cqMxxQz13"}},{"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, precision_recall_curve\n","from sklearn.metrics import confusion_matrix # Import confusion_matrix\n","import seaborn as sns # Import seaborn\n","\n","# Assuming you have already computed reconstruction_errors_val, y_val,\n","# reconstruction_errors_test, and y_test from previous steps.\n","\n","# Train Random Forest on Validation Set Reconstruction Errors\n","# You might want to tune the 'n_estimators' and other parameters\n","# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","# Use best hyperparameters found\n","rf = RandomForestClassifier(\n","    n_estimators=200,\n","    min_samples_split=5,\n","    min_samples_leaf=5,\n","    max_features='log2',\n","    max_depth=10,\n","    bootstrap=False,\n","    random_state=42\n",")\n","\n","rf.fit(reconstruction_errors_val.reshape(-1, 1), y_val) # Train on validation errors and labels\n","\n","# Predict probabilities on Test Set Reconstruction Errors\n","rf_probs = rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n","rf_preds = (rf_probs > 0.5).astype(int) # Use a threshold (e.g., 0.5) to get binary predictions\n","\n","# Evaluation on Test Set\n","print(\"Random Forest Classification Report (Trained on Validation, Tested on Test):\\n\",\n","      classification_report(y_test, rf_preds, digits=4, zero_division=1))\n","print(\"Random Forest ROC AUC (Trained on Validation, Tested on Test):\",\n","      roc_auc_score(y_test, rf_probs))\n","print(\"Random Forest Average Precision (PR AUC) (Trained on Validation, Tested on Test):\",\n","      average_precision_score(y_test, rf_probs))\n","\n","# Calculate Confusion Matrix for Random Forest\n","cm_rf = confusion_matrix(y_test, rf_preds)\n","\n","# Draw Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n","plt.title('Confusion Matrix - Random Forest on Reconstruction Error (Trained Val, Tested Test)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/RF/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","\n","# Plot ROC curve\n","fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)\n","roc_auc_rf = roc_auc_score(y_test, rf_probs) # Recalculate ROC AUC using test probabilities\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'Random Forest ROC curve (AUC = {roc_auc_rf:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic for Random Forest (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/RF/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot Precision-Recall curve\n","precision_rf, recall_rf, _ = precision_recall_curve(y_test, rf_probs)\n","average_precision_rf = average_precision_score(y_test, rf_probs) # Recalculate PR AUC using test probabilities\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall_rf, precision_rf, color='darkorange', lw=2, label=f'Random Forest PR curve (AP = {average_precision_rf:.2f})')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve for Random Forest (Trained Val, Tested Test)')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/RF/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"byFUIBiS62Jz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ensemble Learning on the Classifiers"],"metadata":{"id":"FbicO3dJ1oEo"}},{"cell_type":"markdown","source":["## Averaged Probabilities (Soft Voting)"],"metadata":{"id":"ELCSC1Q6zeKk"}},{"source":["# Assuming the following trained models are available from your notebook:\n","# best_clf (Logistic Regression - trained on reconstruction_errors_val)\n","# mlp_model (Keras MLP - trained on reconstruction_errors_val)\n","# ocsvm (One-Class SVM - trained on reconstruction_errors_train, decision_function on test)\n","# knn (K-Nearest Neighbors - trained on reconstruction_errors_val)\n","# xgb (XGBoost - trained on reconstruction_errors_val)\n","# rf (Random Forest - trained on reconstruction_errors_val)\n","\n","# Ensure reconstruction_errors_test is available and reshaped if needed for models expecting (n_samples, 1) input\n","reconstruction_errors_test_reshaped = reconstruction_errors_test.reshape(-1, 1)\n","\n","# 1. Logistic Regression Probabilities\n","lr_probs = best_clf.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n","\n","# 2. Keras MLP Probabilities\n","# Make sure the MLP model is loaded if necessary\n","# from tensorflow.keras.models import load_model\n","# mlp_model = load_model(\"your_mlp_model.h5\") # Load if not already in memory\n","mlp_probs = mlp_model.predict(reconstruction_errors_test_reshaped).flatten()\n","\n","# 3. KNN Probabilities\n","knn_probs = knn.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n","\n","# 4. XGBoost Probabilities\n","xgb_probs = xgb.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n","\n","# 5. Random Forest Probabilities\n","rf_probs = rf.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n","\n","# 6. One-Class SVM Scores\n","# OCSVM's decision_function gives a score. Lower scores are more anomalous.\n","# For soft voting, we need a probability-like score where higher indicates anomaly.\n","# We can use the negative of the decision function.\n","ocsvm_scores = -ocsvm.decision_function(reconstruction_errors_test_reshaped).flatten()\n","\n","# You might consider scaling the OCSVM scores to be on a similar scale as probabilities (0-1)\n","# For instance, using MinMaxScaler if needed, but it's not strictly necessary for simple averaging.\n","# from sklearn.preprocessing import MinMaxScaler\n","# scaler = MinMaxScaler()\n","# ocsvm_scaled_scores = scaler.fit_transform(ocsvm_scores.reshape(-1, 1)).flatten()\n","# We'll use the raw negative scores for now in the ensemble calculation."],"cell_type":"code","metadata":{"id":"L4hmrv0kuZIX"},"execution_count":null,"outputs":[]},{"source":["import numpy as np\n","\n","# Combine probabilities (excluding OCSVM for simple averaging with probability-based models)\n","# If you want to include OCSVM, you would need a way to convert its score to a probability or\n","# use a different ensembling approach that handles scores and probabilities.\n","# Let's average the probabilities from LR, MLP, KNN, XGBoost, and RF for a start.\n","averaged_probs = np.mean([lr_probs, mlp_probs, knn_probs, xgb_probs, rf_probs], axis=0)\n","\n","# You can set a threshold on the averaged probabilities to get binary predictions\n","ensemble_predictions = (averaged_probs > 0.5).astype(int)"],"cell_type":"code","metadata":{"id":"YjdjozwEub5v"},"execution_count":null,"outputs":[]},{"source":["from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, precision_recall_curve\n","import seaborn as sns\n","\n","# Classification Report\n","print(\"Ensemble (Averaged Probabilities) Classification Report on Test Set:\")\n","print(classification_report(y_test, ensemble_predictions, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n","\n","# Compute ROC AUC\n","roc_auc_ensemble = roc_auc_score(y_test, averaged_probs)\n","print(f\"Ensemble ROC AUC Score: {roc_auc_ensemble:.4f}\")\n","\n","# Compute Average Precision (PR AUC)\n","average_precision_ensemble = average_precision_score(y_test, averaged_probs)\n","print(f\"Ensemble Average Precision (PR AUC): {average_precision_ensemble:.4f}\")\n","\n","# Calculate Confusion Matrix\n","cm_ensemble = confusion_matrix(y_test, ensemble_predictions)\n","\n","# Draw Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_ensemble, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n","plt.title('Confusion Matrix - Ensemble (Averaged Probabilities)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Avg/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot ROC curve\n","fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, averaged_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_ensemble, tpr_ensemble, color='blue', lw=2, label=f'Ensemble ROC curve (AUC = {roc_auc_ensemble:.2f})')\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve - Ensemble (Averaged Probabilities)')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Avg/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot Precision-Recall curve\n","precision_ensemble, recall_ensemble, _ = precision_recall_curve(y_test, averaged_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(recall_ensemble, precision_ensemble, color='red', lw=2, label=f'Ensemble PR curve (AP = {average_precision_ensemble:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve - Ensemble (Averaged Probabilities)')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Avg/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"UQL043FSudl3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Weighted Mechanism (Weighted Averaging) (Soft Voting)\n","\n"],"metadata":{"id":"H_ptCBawz0rg"}},{"source":["from sklearn.ensemble import VotingClassifier\n","# Assuming you have the scikit-learn versions of your trained models:\n","# best_clf (Logistic Regression)\n","# sklearn_mlp (if you train a scikit-learn MLP)\n","# knn (K-Nearest Neighbors)\n","# xgb (XGBoost)\n","# rf (Random Forest)\n","\n","# For One-Class SVM, you would typically not include it in a standard soft voting classifier\n","# because its decision_function is not directly comparable to probability outputs of others.\n","# If you want to include its influence, you might use hard voting or a custom ensembling approach.\n","\n","estimators_sklearn = [\n","    ('lr', best_clf),\n","    ('knn', knn),\n","    ('xgb', xgb),\n","    ('rf', rf)\n","    # Add sklearn_mlp if available\n","    # ('mlp', sklearn_mlp)\n","]\n","\n","# Create the Voting Classifier with soft voting\n","voting_clf = VotingClassifier(estimators=estimators_sklearn, voting='soft', weights=[1, 1, 1, 1]) # Adjust weights as needed\n","\n","# Train the Voting Classifier (This trains the individual models if not already trained, but\n","# since yours are trained, it essentially just sets up the combination)\n","# However, typically, you train the individual models first and then use the VotingClassifier\n","# to combine their predictions without retraining.\n","# To use the already trained models' predictions directly in VotingClassifier for evaluation:\n","# You would fit the VotingClassifier on a small dummy dataset just to enable prediction,\n","# or more correctly, collect the predictions and combine them manually as shown in the previous steps.\n","\n","# Let's stick to the manual averaging of probabilities as it directly uses your existing trained models' outputs.\n","# If you were to use VotingClassifier's fit method, it would re-train the estimators,\n","# which is not what you want if you've already tuned and trained them on the validation data.\n","\n","# The manual averaging method shown above is the correct way to ensemble\n","# the predictions of your already trained models on the test set."],"cell_type":"code","metadata":{"id":"7r4JiM2sukd_"},"execution_count":null,"outputs":[]},{"source":["# Example of weighted averaging (adjust weights based on your analysis)\n","# You would need to determine these weights based on the performance of each model on the validation set (e.g., F1-score, ROC AUC).\n","# For example, if XGBoost had the highest F1 on validation, give it a higher weight.\n","weights = [1.2, 0.8, 1.5, 1.0, 1.1] # Example weights for LR, MLP, KNN, XGBoost, RF\n","weighted_averaged_probs = np.average([lr_probs, mlp_probs, knn_probs, xgb_probs, rf_probs], axis=0, weights=weights)\n","\n","weighted_ensemble_predictions = (weighted_averaged_probs > 0.5).astype(int)\n","\n","# Evaluate the weighted ensemble similarly to the unweighted one.\n","print(\"\\nWeighted Ensemble Classification Report on Test Set:\")\n","print(classification_report(y_test, weighted_ensemble_predictions, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))"],"cell_type":"code","metadata":{"id":"__-DfYUSuokG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: calculate and plot metrics for the weighted ensemble\n","\n","# Compute ROC AUC for weighted ensemble\n","roc_auc_weighted_ensemble = roc_auc_score(y_test, weighted_averaged_probs)\n","print(f\"Weighted Ensemble ROC AUC Score: {roc_auc_weighted_ensemble:.4f}\")\n","\n","# Compute Average Precision (PR AUC) for weighted ensemble\n","average_precision_weighted_ensemble = average_precision_score(y_test, weighted_averaged_probs)\n","print(f\"Weighted Ensemble Average Precision (PR AUC): {average_precision_weighted_ensemble:.4f}\")\n","\n","# Calculate Confusion Matrix for weighted ensemble\n","cm_weighted_ensemble = confusion_matrix(y_test, weighted_ensemble_predictions)\n","\n","# Define output directory for Weighted Ensemble plots\n","weighted_ensemble_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Weighted\"\n","os.makedirs(weighted_ensemble_output_dir, exist_ok=True)\n","\n","\n","# Draw Confusion Matrix for weighted ensemble\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_weighted_ensemble, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n","plt.title('Confusion Matrix - Weighted Ensemble')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.savefig(f\"{weighted_ensemble_output_dir}/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot ROC curve for weighted ensemble\n","fpr_weighted_ensemble, tpr_weighted_ensemble, _ = roc_curve(y_test, weighted_averaged_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_weighted_ensemble, tpr_weighted_ensemble, color='green', lw=2, label=f'Weighted Ensemble ROC curve (AUC = {roc_auc_weighted_ensemble:.2f})')\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve - Weighted Ensemble')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","plt.savefig(f\"{weighted_ensemble_output_dir}/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot Precision-Recall curve for weighted ensemble\n","precision_weighted_ensemble, recall_weighted_ensemble, _ = precision_recall_curve(y_test, weighted_averaged_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(recall_weighted_ensemble, precision_weighted_ensemble, color='purple', lw=2, label=f'Weighted Ensemble PR curve (AP = {average_precision_weighted_ensemble:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve - Weighted Ensemble')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","plt.savefig(f\"{weighted_ensemble_output_dir}/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"QGz8AAgeO0Sd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Print RoC (AUC) & PR (AUC) of weighted esemble\n","\n","print(f\"RoC (AUC) of Weighted Ensemble: {roc_auc_weighted_ensemble:.4f}\")\n","print(f\"PR (AUC) of Weighted Ensemble: {average_precision_weighted_ensemble:.4f}\")\n"],"metadata":{"id":"kuwzwPB3oaHN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Advanced Ensembling: Stacking"],"metadata":{"id":"4l8WY6sI1TyB"}},{"cell_type":"markdown","source":["## Stacking with LogisticRegression"],"metadata":{"id":"C7ycWbTw0KSy"}},{"source":["# Generate Out-of-Fold Predictions on the Validation Set\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","\n","# Assuming you have reconstruction_errors_val and y_val\n","\n","# Number of folds for cross-validation\n","n_splits = 5\n","skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","# Initialize arrays to store out-of-fold predictions\n","# Each column will be the predictions from one base model\n","oof_preds = np.zeros((reconstruction_errors_val.shape[0], 5)) # 5 columns for LR, MLP, KNN, XGB, RF\n","\n","# List of base models (excluding OCSVM for now as its output is a score, not probability)\n","base_models = [\n","    ('lr', best_clf),\n","    ('mlp', mlp_model), # We'll handle Keras prediction separately\n","    ('knn', knn),\n","    ('xgb', xgb),\n","    ('rf', rf)\n","]\n","\n","# Iterate through folds and generate out-of-fold predictions\n","for fold, (train_index, val_index) in enumerate(skf.split(reconstruction_errors_val.reshape(-1, 1), y_val)):\n","    print(f\"Processing Fold {fold + 1}/{n_splits}\")\n","\n","    X_train_fold, X_val_fold = reconstruction_errors_val[train_index].reshape(-1, 1), reconstruction_errors_val[val_index].reshape(-1, 1)\n","    y_train_fold, y_val_fold = y_val[train_index], y_val[val_index]\n","\n","    for i, (name, model) in enumerate(base_models):\n","        if name == 'mlp':\n","            # For Keras model, train on the fold data\n","            # Need to clone and retrain the model for each fold or save/load weights\n","            # A simpler approach for demonstration: assuming the trained mlp_model is sufficient\n","            # and we generate predictions on the val_index for this fold.\n","            # In a proper implementation, you would train a new Keras model on X_train_fold\n","            # and predict on X_val_fold.\n","            fold_mlp_preds = model.predict(X_val_fold).flatten()\n","            oof_preds[val_index, i] = fold_mlp_preds\n","        else:\n","            # Train a clone of the scikit-learn model on the fold data\n","            from sklearn.base import clone\n","            cloned_model = clone(model)\n","            cloned_model.fit(X_train_fold, y_train_fold)\n","            oof_preds[val_index, i] = cloned_model.predict_proba(X_val_fold)[:, 1]\n","\n","# oof_preds now contains the out-of-fold predictions for each base model on the validation set"],"cell_type":"code","metadata":{"id":"KJgZw3eIu1u1"},"execution_count":null,"outputs":[]},{"source":["# Combine test predictions from base models\n","test_preds = np.vstack([lr_probs, mlp_probs, knn_probs, xgb_probs, rf_probs]).T\n","\n","# test_preds now has shape (n_test_samples, n_base_models)"],"cell_type":"code","metadata":{"id":"PIiF7f3Zu6S-"},"execution_count":null,"outputs":[]},{"source":["from sklearn.linear_model import LogisticRegression\n","\n","# Define the meta-model\n","meta_model = LogisticRegression()\n","\n","# Train the meta-model on the out-of-fold predictions\n","meta_model.fit(oof_preds, y_val)"],"cell_type":"code","metadata":{"id":"LBSj-DPMu8Af"},"execution_count":null,"outputs":[]},{"source":["# Make final predictions on the test data using the meta-model\n","stacked_predictions_probs = meta_model.predict_proba(test_preds)[:, 1]\n","\n","# Convert probabilities to binary predictions\n","stacked_predictions = (stacked_predictions_probs > 0.5).astype(int)"],"cell_type":"code","metadata":{"id":"1im3Rc6Ou-Ru"},"execution_count":null,"outputs":[]},{"source":["from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, precision_recall_curve\n","import seaborn as sns\n","\n","# Classification Report\n","print(\"Stacked Ensemble Classification Report on Test Set:\")\n","print(classification_report(y_test, stacked_predictions, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n","\n","# Compute ROC AUC\n","roc_auc_stacked = roc_auc_score(y_test, stacked_predictions_probs)\n","print(f\"Stacked Ensemble ROC AUC Score: {roc_auc_stacked:.4f}\")\n","\n","# Compute Average Precision (PR AUC)\n","average_precision_stacked = average_precision_score(y_test, stacked_predictions_probs)\n","print(f\"Stacked Ensemble Average Precision (PR AUC): {average_precision_stacked:.4f}\")\n","\n","# Calculate Confusion Matrix\n","cm_stacked = confusion_matrix(y_test, stacked_predictions)\n","\n","# Draw Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_stacked, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Anomaly'])\n","plt.title('Confusion Matrix - Stacked Ensemble')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.show()\n","\n","# Plot ROC curve\n","fpr_stacked, tpr_stacked, _ = roc_curve(y_test, stacked_predictions_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_stacked, tpr_stacked, color='blue', lw=2, label=f'Stacked ROC curve (AUC = {roc_auc_stacked:.2f})')\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve - Stacked Ensemble')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot Precision-Recall curve\n","precision_stacked, recall_stacked, _ = precision_recall_curve(y_test, stacked_predictions_probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(recall_stacked, precision_stacked, color='red', lw=2, label=f'Stacked PR curve (AP = {average_precision_stacked:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve - Stacked Ensemble')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"rRjCF3hbu_1e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stacking with XGBoost Meta-Model"],"metadata":{"id":"lsF_TtYa0-Tj"}},{"source":["from xgboost import XGBClassifier\n","\n","# Define the XGBoost meta-model\n","# You might want to tune the hyperparameters of the meta-model as well\n","meta_model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n","\n","# Train the meta-model on the out-of-fold predictions\n","meta_model_xgb.fit(oof_preds, y_val)"],"cell_type":"code","metadata":{"id":"yxNFDHUm02ga"},"execution_count":null,"outputs":[]},{"source":["# Make final predictions on the test data using the XGBoost meta-model\n","stacked_predictions_probs_xgb = meta_model_xgb.predict_proba(test_preds)[:, 1]\n","\n","# Convert probabilities to binary predictions\n","stacked_predictions_xgb = (stacked_predictions_probs_xgb > 0.5).astype(int)"],"cell_type":"code","metadata":{"id":"WpMaEaQS04tR"},"execution_count":null,"outputs":[]},{"source":["from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, precision_recall_curve\n","import seaborn as sns\n","\n","# Classification Report\n","print(\"Stacked Ensemble (XGBoost Meta-Model) Classification Report on Test Set:\")\n","print(classification_report(y_test, stacked_predictions_xgb, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n","\n","# Compute ROC AUC\n","roc_auc_stacked_xgb = roc_auc_score(y_test, stacked_predictions_probs_xgb)\n","print(f\"Stacked Ensemble (XGBoost Meta-Model) ROC AUC Score: {roc_auc_stacked_xgb:.4f}\")\n","\n","# Compute Average Precision (PR AUC)\n","average_precision_stacked_xgb = average_precision_score(y_test, stacked_predictions_probs_xgb)\n","print(f\"Stacked Ensemble (XGBoost Meta-Model) Average Precision (PR AUC): {average_precision_stacked_xgb:.4f}\")\n","\n","# Calculate Confusion Matrix\n","cm_stacked_xgb = confusion_matrix(y_test, stacked_predictions_xgb)\n","\n","# Draw Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_stacked_xgb, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n","            yticklabels=['Actual Non-Dysbiotic', 'Actual Anomaly'])\n","plt.title('Confusion Matrix - Stacked Ensemble (XGBoost Meta-Model)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking_XGB/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot ROC curve\n","fpr_stacked_xgb, tpr_stacked_xgb, _ = roc_curve(y_test, stacked_predictions_probs_xgb)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr_stacked_xgb, tpr_stacked_xgb, color='blue', lw=2, label=f'Stacked (XGBoost) ROC curve (AUC = {roc_auc_stacked_xgb:.2f})')\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve - Stacked Ensemble (XGBoost Meta-Model)')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking_XGB/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Plot Precision-Recall curve\n","precision_stacked_xgb, recall_stacked_xgb, _ = precision_recall_curve(y_test, stacked_predictions_probs_xgb)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(recall_stacked_xgb, precision_stacked_xgb, color='red', lw=2, label=f'Stacked (XGBoost) PR curve (AP = {average_precision_stacked_xgb:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve - Stacked Ensemble (XGBoost Meta-Model)')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","# plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking_XGB/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()"],"cell_type":"code","metadata":{"id":"gBZZN9s705eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: prompt: There is too much overlap between the lines in the ROC (AUC) and PR(AUC). Fix it\n","\n","import matplotlib.pyplot as plt\n","# --- B. Combined ROC and PR Plotting ---\n","\n","# Store ROC and PR curve data for each model\n","roc_curves_data = {}\n","pr_curves_data = {}\n","\n","# Function to collect ROC and PR data\n","def collect_curve_data(model_name, y_true, y_probs_or_scores, color, linestyle='-'):\n","    \"\"\"Collects ROC and PR curve data for a given model's scores/probabilities.\"\"\"\n","    # For ROC curve, higher score/prob means higher true positive rate at lower false positive rate\n","    # For PR curve, higher score/prob means higher precision at higher recall\n","    # sklearn's roc_curve and precision_recall_curve work correctly with scores directly.\n","\n","    fpr, tpr, _ = roc_curve(y_true, y_probs_or_scores)\n","    roc_auc = roc_auc_score(y_true, y_probs_or_scores)\n","    roc_curves_data[model_name] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc, 'color': color, 'linestyle': linestyle}\n","\n","    precision, recall, _ = precision_recall_curve(y_true, y_probs_or_scores)\n","    pr_auc = average_precision_score(y_true, y_probs_or_scores)\n","    pr_curves_data[model_name] = {'precision': precision, 'recall': recall, 'auc': pr_auc, 'color': color, 'linestyle': linestyle}\n","\n","# Collect data for Original Model (LSTM-AE with optimal threshold)\n","# Use reconstruction errors as the score directly. Higher error -> more likely anomaly.\n","collect_curve_data(\n","    'LSTM-AE (Reconstruction Error)',\n","    y_test,\n","    reconstruction_errors_test,\n","    'darkorange',\n","    '-'\n",")\n","\n","# Collect data for Individual Classifiers (assuming probabilities are available)\n","# Use predict_proba[:, 1] for standard classifiers\n","# For OCSVM, use negative scores as the \"probability\" or score for AUC\n","collect_curve_data('Logistic Regression', y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'blue', '--')\n","collect_curve_data('MLP', y_test, mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'green', '--')\n","# For OCSVM, use negative decision_function scores. Lower scores are anomalous, so negative makes higher values anomalous.\n","collect_curve_data('One-Class SVM', y_test, -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'red', '--')\n","collect_curve_data('K-Nearest Neighbors', y_test, knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'purple', '--')\n","collect_curve_data('XGBoost', y_test, xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'brown', '--')\n","collect_curve_data('Random Forest', y_test, rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'pink', '--')\n","\n","\n","# Collect data for Ensemble Methods (using the averaged/stacked probabilities)\n","collect_curve_data('Averaged Probabilities Ensemble', y_test, averaged_probs, 'cyan', '-')\n","collect_curve_data('Weighted Ensemble', y_test, weighted_averaged_probs, 'magenta', '-')\n","# Use the stacked ensemble probabilities as scores\n","collect_curve_data('Stacked Ensemble (LR Meta)', y_test, meta_model.predict_proba(test_preds)[:, 1], 'gray', '-')\n","collect_curve_data('Stacked Ensemble (XGB Meta)', y_test, meta_model_xgb.predict_proba(test_preds)[:, 1], 'olive', '-')\n","\n","\n","# --- Plotting ---\n","\n","output_dir_evaluation_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Combined_Evaluation_Plots/\"\n","os.makedirs(output_dir_evaluation_plots, exist_ok=True)\n","\n","# Figure 1: ROC Curves\n","plt.figure(figsize=(10, 8))\n","for model_name, data in roc_curves_data.items():\n","    plt.plot(data['fpr'], data['tpr'], color=data['color'], linestyle=data['linestyle'],\n","             label=f'{model_name} (AUC = {data[\"auc\"]:.3f})')\n","\n","plt.plot([0, 1], [0, 1], 'k--', lw=1)  # Diagonal line\n","plt.xlabel(\"False Positive Rate\", fontsize=12)\n","plt.ylabel(\"True Positive Rate\", fontsize=12)\n","plt.title(\"Receiver Operating Characteristic (ROC) Curves\", fontsize=14)\n","plt.legend(loc=\"lower right\", fontsize=10)\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.tight_layout()\n","plt.savefig(f\"{output_dir_evaluation_plots}/Combined_ROC_Curves.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n","\n","# Figure 2: PR Curves\n","plt.figure(figsize=(10, 8))\n","for model_name, data in pr_curves_data.items():\n","    plt.plot(data['recall'], data['precision'], color=data['color'], linestyle=data['linestyle'],\n","             label=f'{model_name} (AP = {data[\"auc\"]:.3f})')\n","\n","plt.xlabel(\"Recall\", fontsize=12)\n","plt.ylabel(\"Precision\", fontsize=12)\n","plt.title(\"Precision-Recall (PR) Curves\", fontsize=14)\n","plt.legend(loc=\"lower left\", fontsize=10)\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.ylim([0.0, 1.05]) # Standard practice for PR curves\n","plt.xlim([0.0, 1.0])\n","plt.tight_layout()\n","plt.savefig(f\"{output_dir_evaluation_plots}/Combined_PR_Curves.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"VNOy1nJcqbY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Generate RoC (AUC) as original vs All Classifiers except ensemble learning\n","\n","import matplotlib.pyplot as plt\n","# --- Plotting ROC Curve for Original Model vs. Individual Classifiers (Excluding Ensemble) ---\n","\n","# Define the output directory\n","output_dir_comparison_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Comparison_Plots/\"\n","os.makedirs(output_dir_comparison_plots, exist_ok=True)\n","\n","\n","plt.figure(figsize=(10, 8))\n","\n","# Plot ROC Curve for the Original Model (LSTM-AE Reconstruction Error)\n","# Use reconstruction errors as the score directly. Higher error -> more likely anomaly.\n","fpr_original, tpr_original, _ = roc_curve(y_test, reconstruction_errors_test)\n","roc_auc_original = roc_auc_score(y_test, reconstruction_errors_test)\n","plt.plot(fpr_original, tpr_original, color='darkorange', lw=2, label=f'LSTM-AE (Reconstruction Error) (AUC = {roc_auc_original:.3f})')\n","\n","\n","# Plot ROC Curves for Individual Classifiers (trained on reconstruction errors)\n","# Use predict_proba[:, 1] for standard classifiers. For OCSVM, use negative scores.\n","\n","# Logistic Regression\n","fpr_lr, tpr_lr, _ = roc_curve(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1])\n","roc_auc_lr = roc_auc_score(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1])\n","plt.plot(fpr_lr, tpr_lr, color='blue', linestyle='--', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.3f})')\n","\n","# MLP\n","mlp_probs_test = mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten()\n","fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_probs_test)\n","roc_auc_mlp = roc_auc_score(y_test, mlp_probs_test)\n","plt.plot(fpr_mlp, tpr_mlp, color='green', linestyle='--', lw=2, label=f'MLP (AUC = {roc_auc_mlp:.3f})')\n","\n","# One-Class SVM\n","ocsvm_scores_test = -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten()\n","fpr_ocsvm, tpr_ocsvm, _ = roc_curve(y_test, ocsvm_scores_test)\n","roc_auc_ocsvm = roc_auc_score(y_test, ocsvm_scores_test)\n","plt.plot(fpr_ocsvm, tpr_ocsvm, color='red', linestyle='--', lw=2, label=f'One-Class SVM (AUC = {roc_auc_ocsvm:.3f})')\n","\n","# K-Nearest Neighbors\n","knn_probs_test = knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n","fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_probs_test)\n","roc_auc_knn = roc_auc_score(y_test, knn_probs_test)\n","plt.plot(fpr_knn, tpr_knn, color='purple', linestyle='--', lw=2, label=f'K-Nearest Neighbors (AUC = {roc_auc_knn:.3f})')\n","\n","# XGBoost\n","xgb_probs_test = xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n","fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probs_test)\n","roc_auc_xgb = roc_auc_score(y_test, xgb_probs_test)\n","plt.plot(fpr_xgb, tpr_xgb, color='brown', linestyle='--', lw=2, label=f'XGBoost (AUC = {roc_auc_xgb:.3f})')\n","\n","# Random Forest\n","rf_probs_test = rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n","fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs_test)\n","roc_auc_rf = roc_auc_score(y_test, rf_probs_test)\n","plt.plot(fpr_rf, tpr_rf, color='pink', linestyle='--', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.3f})')\n","\n","\n","# Plot the diagonal reference line\n","plt.plot([0, 1], [0, 1], 'k--', lw=1)\n","\n","plt.xlabel(\"False Positive Rate\", fontsize=12)\n","plt.ylabel(\"True Positive Rate\", fontsize=12)\n","plt.title(\"ROC Curves: LSTM-AE vs. Individual Classifiers\", fontsize=14)\n","plt.legend(loc=\"lower right\", fontsize=10)\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.tight_layout()\n","plt.savefig(f\"{output_dir_comparison_plots}/ROC_Curve_Original_vs_Individual_Classifiers.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"LXsO9A_jvF_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Generate RoC (AUC) as original vs all ensemble learning only\n","\n","import matplotlib.pyplot as plt\n","# --- Plotting ROC Curve for Original Model vs. Ensemble Learning ---\n","\n","# Define the output directory for comparison plots if not already defined\n","output_dir_comparison_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Comparison_Plots/\"\n","os.makedirs(output_dir_comparison_plots, exist_ok=True)\n","\n","\n","plt.figure(figsize=(10, 8))\n","\n","# Plot ROC Curve for the Original Model (LSTM-AE Reconstruction Error)\n","# Use reconstruction errors as the score directly. Higher error -> more likely anomaly.\n","fpr_original, tpr_original, _ = roc_curve(y_test, reconstruction_errors_test)\n","roc_auc_original = roc_auc_score(y_test, reconstruction_errors_test)\n","plt.plot(fpr_original, tpr_original, color='darkorange', lw=2, label=f'LSTM-AE (Reconstruction Error) (AUC = {roc_auc_original:.3f})')\n","\n","\n","# Plot ROC Curves for Ensemble Models\n","\n","# Averaged Probabilities Ensemble\n","fpr_avg_ensemble, tpr_avg_ensemble, _ = roc_curve(y_test, averaged_probs)\n","roc_auc_avg_ensemble = roc_auc_score(y_test, averaged_probs)\n","plt.plot(fpr_avg_ensemble, tpr_avg_ensemble, color='cyan', linestyle='-', lw=2, label=f'Averaged Probabilities Ensemble (AUC = {roc_auc_avg_ensemble:.3f})')\n","\n","# Weighted Ensemble\n","fpr_weighted_ensemble, tpr_weighted_ensemble, _ = roc_curve(y_test, weighted_averaged_probs)\n","roc_auc_weighted_ensemble = roc_auc_score(y_test, weighted_averaged_probs)\n","plt.plot(fpr_weighted_ensemble, tpr_weighted_ensemble, color='magenta', linestyle='-', lw=2, label=f'Weighted Ensemble (AUC = {roc_auc_weighted_ensemble:.3f})')\n","\n","# Stacked Ensemble (Logistic Regression Meta-Model)\n","# Assuming test_preds contains the stacked inputs for test data\n","stacked_predictions_probs_lr_meta = meta_model.predict_proba(test_preds)[:, 1]\n","fpr_stacked_lr, tpr_stacked_lr, _ = roc_curve(y_test, stacked_predictions_probs_lr_meta)\n","roc_auc_stacked_lr = roc_auc_score(y_test, stacked_predictions_probs_lr_meta)\n","plt.plot(fpr_stacked_lr, tpr_stacked_lr, color='gray', linestyle='-', lw=2, label=f'Stacked Ensemble (LR Meta) (AUC = {roc_auc_stacked_lr:.3f})')\n","\n","# Stacked Ensemble (XGBoost Meta-Model)\n","# Assuming test_preds contains the stacked inputs for test data\n","stacked_predictions_probs_xgb_meta = meta_model_xgb.predict_proba(test_preds)[:, 1]\n","fpr_stacked_xgb, tpr_stacked_xgb, _ = roc_curve(y_test, stacked_predictions_probs_xgb_meta)\n","roc_auc_stacked_xgb = roc_auc_score(y_test, stacked_predictions_probs_xgb_meta)\n","plt.plot(fpr_stacked_xgb, tpr_stacked_xgb, color='olive', linestyle='-', lw=2, label=f'Stacked Ensemble (XGB Meta) (AUC = {roc_auc_stacked_xgb:.3f})')\n","\n","\n","# Plot the diagonal reference line\n","plt.plot([0, 1], [0, 1], 'k--', lw=1)\n","\n","plt.xlabel(\"False Positive Rate\", fontsize=12)\n","plt.ylabel(\"True Positive Rate\", fontsize=12)\n","plt.title(\"ROC Curves: LSTM-AE vs. Ensemble Classifiers\", fontsize=14)\n","plt.legend(loc=\"lower right\", fontsize=10)\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.tight_layout()\n","plt.savefig(f\"{output_dir_comparison_plots}/ROC_Curve_Original_vs_Ensemble_Classifiers.pdf\", dpi=600, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"3L3SX45XvhZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical Tests"],"metadata":{"id":"qbrPd2Z5YVQU"}},{"cell_type":"code","source":["!pip install statsmodels"],"metadata":{"id":"pZYjvmiqYcis"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical Evaluation and Bootstrap Analysis\n"],"metadata":{"id":"tKlBVkg9ke1d"}},{"source":["# prompt: I have time series data processed into overlapping windows. I used an LSTM Autoencoder (LSTM-AE) to compute reconstruction errors, then trained these classifiers on the errors: Logistic Regression, MLP, One-Class SVM, KNN, XGBoost, and Random Forest.\n","# I already have prediction probabilities, binary predictions (0/1), and true labels for each model.\n","# Please do the following:\n","# Bootstrap Testing (1000 iterations):\n","# For each model (including LSTM-AE), sample the test set with replacement.\n","# For each sample, compute ROC AUC, PR AUC, and F1-score.\n","# Return the metric distributions, mean, and 95% CI.\n","# Statistical Testing:\n","# For each model vs LSTM-AE:\n","# Paired t-test and Wilcoxon test on ROC AUC, PR AUC, and F1-score.\n","# McNemars test on binary predictions.\n","# Return all p-values.\n","# Visualizations:\n","# Box/violin plots of metrics.\n","# ROC and PR curves with 95% bootstrap confidence bands.\n","# Summary Table:\n","# For each model: mean  95% CI per metric, p-values vs LSTM-AE, and a significant? flag.\n","# Python Code:\n","# Use sklearn, numpy, scipy, seaborn, matplotlib.\n","# Implement all above steps assuming variables like:\n","# y_true, y_pred_probs_<model>, y_preds_<model>\n","# Return the complete code and results suitable for a research paper\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.utils import resample\n","from scipy.stats import ttest_rel, wilcoxon\n","# from statsmodels.sandbox.stats.runs import mcnemar # Keeping this import, although statsmodels.stats.contingency_tables.mcnemar is used\n","from statsmodels.stats.contingency_tables import mcnemar\n","from typing import Dict, Any, List\n","import seaborn as sns # Ensure seaborn is imported for plotting\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, precision_recall_curve # Ensure all metrics are imported\n","import os\n","\n","# Assuming y_test, reconstruction_errors_test, anomaly_predictions, best_clf, mlp_model, ocsvm, knn, xgb, rf are available from previous cells\n","\n","# Store results in dictionaries\n","metric_distributions: Dict[str, Dict[str, List[float]]] = {}\n","metric_summary: Dict[str, Dict[str, Dict[str, Any]]] = {}\n","statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n","\n","# Define the models and their corresponding probabilities and binary predictions\n","models = {\n","    'LSTM-AE': {'probs': reconstruction_errors_test, 'preds': anomaly_predictions}, # Assuming reconstruction_errors_test is the \"score\" for LSTM-AE\n","    'Logistic Regression': {'probs': best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': best_clf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'MLP': {'probs': mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten() > 0.5).astype(int)},\n","    'One-Class SVM': {'probs': -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten() < 0).astype(int)}, # Using negative scores for AUC/PR AUC, and thresholding at 0 for binary predictions\n","    'KNN': {'probs': knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': knn.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'XGBoost': {'probs': xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': xgb.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'Random Forest': {'probs': rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': rf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    # Add ensemble models if desired\n","    'Averaged Ensemble': {'probs': averaged_probs, 'preds': ensemble_predictions},\n","    'Weighted Ensemble': {'probs': weighted_averaged_probs, 'preds': weighted_ensemble_predictions},\n","    'Stacked (LR Meta)': {'probs': stacked_predictions_probs, 'preds': stacked_predictions},\n","    'Stacked (XGB Meta)': {'probs': stacked_predictions_probs_xgb, 'preds': stacked_predictions_xgb},\n","}\n","\n","n_iterations = 1000\n","alpha = 0.05 # For confidence intervals and statistical significance\n","\n","print(\"Performing Bootstrap Testing...\")\n","for model_name, data in models.items():\n","    model_probs = data['probs']\n","    model_preds = data['preds']\n","\n","    roc_aucs = []\n","    pr_aucs = []\n","    f1_scores = []\n","\n","    # Handle potential issues with OCSVM or other models having constant predictions/scores\n","    # that would prevent AUC/F1 calculation on some bootstrapped samples.\n","    # For simplicity, we'll skip iterations where metrics fail.\n","    successful_iterations = 0\n","    # Use a fixed random state for reproducibility of bootstrap samples themselves\n","    bootstrap_rng = np.random.default_rng(seed=42)\n","\n","    # Store (y_true_sample, probs_sample) for plotting later\n","    bootstrap_samples_for_plotting: List[tuple[np.ndarray, np.ndarray]] = []\n","\n","\n","    while successful_iterations < n_iterations:\n","        # Resample with replacement\n","        indices = bootstrap_rng.choice(np.arange(len(y_test)), size=len(y_test), replace=True)\n","\n","        y_true_sample = y_test[indices]\n","        probs_sample = model_probs[indices]\n","        preds_sample = model_preds[indices]\n","\n","        try:\n","            # Compute metrics for the sample\n","            # Check for multiple classes in true labels and variation in scores/predictions\n","            if len(np.unique(y_true_sample)) > 1:\n","                 if len(np.unique(probs_sample)) > 1:\n","                    roc_auc_sample = roc_auc_score(y_true_sample, probs_sample)\n","                    pr_auc_sample = average_precision_score(y_true_sample, probs_sample)\n","                    roc_aucs.append(roc_auc_sample)\n","                    pr_aucs.append(pr_auc_sample)\n","                 # We still try to compute F1 even if probs are constant, as preds might vary\n","                 if len(np.unique(preds_sample)) > 1:\n","                      f1_score_sample = f1_score(y_true_sample, preds_sample, zero_division=0)\n","                      f1_scores.append(f1_score_sample)\n","            # If only one class in y_true_sample, metrics like AUC, PR AUC, and standard F1 are not well-defined.\n","            # We append np.nan or skip, skipping is better for distribution calculations later.\n","\n","            # Store sample for plotting if metrics were attempted (even if they failed)\n","            # This ensures the lists for plotting match the iterations where we *tried* to compute metrics\n","            bootstrap_samples_for_plotting.append((y_true_sample, probs_sample))\n","\n","            successful_iterations += 1 # Increment only if sample was generated and attempted\n","\n","        except Exception as e:\n","            # print(f\"Skipping iteration {successful_iterations} for {model_name} due to error: {e}\")\n","            # Still store the sample even if metric calculation failed, for consistent plotting sample count\n","            bootstrap_samples_for_plotting.append((y_true_sample, probs_sample))\n","            successful_iterations += 1 # Increment even on error to match total iterations\n","\n","\n","    # Store distributions\n","    metric_distributions[model_name] = {\n","        'ROC AUC': roc_aucs,\n","        'PR AUC': pr_aucs,\n","        'F1 Score': f1_scores,\n","        'bootstrap_samples_for_plotting': bootstrap_samples_for_plotting # Store samples for plotting\n","    }\n","\n","    # Compute mean and 95% CI\n","    metric_summary[model_name] = {}\n","    for metric_name, values in {\n","        'ROC AUC': roc_aucs,\n","        'PR AUC': pr_aucs,\n","        'F1 Score': f1_scores\n","        }.items():\n","        if values: # Ensure list is not empty\n","            mean_val = np.mean(values)\n","            # Compute confidence interval using percentile method\n","            lower_bound = np.percentile(values, (alpha / 2) * 100)\n","            upper_bound = np.percentile(values, 100 - (alpha / 2) * 100)\n","            metric_summary[model_name][metric_name] = {\n","                'mean': mean_val,\n","                'ci_lower': lower_bound,\n","                'ci_upper': upper_bound,\n","                'distribution': values # Keep distribution for later plotting\n","            }\n","        else:\n","             metric_summary[model_name][metric_name] = {\n","                'mean': np.nan,\n","                'ci_lower': np.nan,\n","                'ci_upper': np.nan,\n","                'distribution': []\n","            }\n","\n","\n","print(\"\\nPerforming Statistical Testing...\")\n","# Collect bootstrap distributions for LSTM-AE explicitly after the loop finishes\n","lstm_ae_roc_auc_dist = metric_distributions['LSTM-AE']['ROC AUC']\n","lstm_ae_pr_auc_dist = metric_distributions['LSTM-AE']['PR AUC']\n","lstm_ae_f1_dist = metric_distributions['LSTM-AE']['F1 Score']\n","lstm_ae_preds = models['LSTM-AE']['preds'] # Binary predictions from LSTM-AE thresholding\n","\n","statistical_test_results['vs LSTM-AE'] = {}"],"cell_type":"code","metadata":{"id":"qfLCnopsVdDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical Testing Against LSTM-AE"],"metadata":{"id":"3HeRZe-3kRi0"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy.stats import ttest_rel, wilcoxon\n","from statsmodels.stats.contingency_tables import mcnemar\n","\n","for model_name, data in models.items():\n","    if model_name == 'LSTM-AE':\n","        continue  # Skip comparison with itself\n","\n","    model_roc_auc_dist = metric_distributions[model_name]['ROC AUC']\n","    model_pr_auc_dist = metric_distributions[model_name]['PR AUC']\n","    model_f1_dist = metric_distributions[model_name]['F1 Score']\n","    model_preds = data['preds']\n","\n","    statistical_test_results['vs LSTM-AE'][model_name] = {}\n","\n","    # Ensure lengths match for paired tests\n","    min_len_roc = min(len(lstm_ae_roc_auc_dist), len(model_roc_auc_dist))\n","    if min_len_roc >= 2:\n","        t_stat_roc, p_value_t_roc = ttest_rel(lstm_ae_roc_auc_dist[:min_len_roc], model_roc_auc_dist[:min_len_roc])\n","        statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (T-test p)'] = p_value_t_roc\n","\n","        # Wilcoxon test fix: Ensure at least 2 distinct values\n","        unique_values_lstm = len(set(lstm_ae_roc_auc_dist[:min_len_roc]))\n","        unique_values_model = len(set(model_roc_auc_dist[:min_len_roc]))\n","        if min_len_roc >= 2 and unique_values_lstm > 1 and unique_values_model > 1:\n","            try:\n","                w_stat_roc, p_value_w_roc = wilcoxon(lstm_ae_roc_auc_dist[:min_len_roc], model_roc_auc_dist[:min_len_roc])\n","                statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = p_value_w_roc\n","            except ValueError:\n","                statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = np.nan\n","        else:\n","            statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = np.nan\n","    else:\n","        statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (T-test p)'] = np.nan\n","        statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = np.nan\n","\n","    # Repeat fixes for PR AUC and F1 Score\n","    for metric_name, lstm_dist, model_dist in zip(\n","        ['PR AUC', 'F1 Score'],\n","        [lstm_ae_pr_auc_dist, lstm_ae_f1_dist],\n","        [model_pr_auc_dist, model_f1_dist]\n","    ):\n","        min_len = min(len(lstm_dist), len(model_dist))\n","        if min_len >= 2:\n","            t_stat, p_value_t = ttest_rel(lstm_dist[:min_len], model_dist[:min_len])\n","            statistical_test_results['vs LSTM-AE'][model_name][f'{metric_name} (T-test p)'] = p_value_t\n","\n","            # Wilcoxon test fix\n","            unique_values_lstm = len(set(lstm_dist[:min_len]))\n","            unique_values_model = len(set(model_dist[:min_len]))\n","            if min_len >= 2 and unique_values_lstm > 1 and unique_values_model > 1:\n","                try:\n","                    w_stat, p_value_w = wilcoxon(lstm_dist[:min_len], model_dist[:min_len])\n","                    statistical_test_results['vs LSTM-AE'][model_name][f'{metric_name} (Wilcoxon p)'] = p_value_w\n","                except ValueError:\n","                    statistical_test_results['vs LSTM-AE']"],"metadata":{"id":"FWqKcJslbSTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical Comparison of Models"],"metadata":{"id":"M-s8wmFBkNFc"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n","from statsmodels.stats.contingency_tables import mcnemar\n","\n","# Define the ensemble models\n","ensemble_models = {\n","    'Averaged Ensemble': {'probs': averaged_probs, 'preds': ensemble_predictions},\n","    'Weighted Ensemble': {'probs': weighted_averaged_probs, 'preds': weighted_ensemble_predictions},\n","    'Stacked (LR Meta)': {'probs': stacked_predictions_probs, 'preds': stacked_predictions},\n","    'Stacked (XGB Meta)': {'probs': stacked_predictions_probs_xgb, 'preds': stacked_predictions_xgb},\n","}\n","\n","# Store statistical test results for ensemble comparisons\n","ensemble_statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n","\n","print(\"\\nPerforming Statistical Testing Among Ensemble Learners...\")\n","\n","ensemble_model_names = list(ensemble_models.keys())\n","\n","# Iterate through all unique pairs of ensemble models\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        model1_name = ensemble_model_names[i]\n","        model2_name = ensemble_model_names[j]\n","\n","        model1_data = ensemble_models[model1_name]\n","        model2_data = ensemble_models[model2_name]\n","\n","        model1_roc_auc_dist = np.array(metric_distributions[model1_name]['ROC AUC'], dtype=np.float64)\n","        model1_pr_auc_dist = np.array(metric_distributions[model1_name]['PR AUC'], dtype=np.float64)\n","        model1_f1_dist = np.array(metric_distributions[model1_name]['F1 Score'], dtype=np.float64)\n","        model1_preds = model1_data['preds']\n","\n","        model2_roc_auc_dist = np.array(metric_distributions[model2_name]['ROC AUC'], dtype=np.float64)\n","        model2_pr_auc_dist = np.array(metric_distributions[model2_name]['PR AUC'], dtype=np.float64)\n","        model2_f1_dist = np.array(metric_distributions[model2_name]['F1 Score'], dtype=np.float64)\n","        model2_preds = model2_data['preds']\n","\n","        comparison_key = f'{model1_name} vs {model2_name}'\n","        ensemble_statistical_test_results[comparison_key] = {}\n","\n","        # Perform paired tests on metric distributions (ROC AUC, PR AUC, F1 Score)\n","        for metric_name, dist1, dist2 in zip(\n","            ['ROC AUC', 'PR AUC', 'F1 Score'],\n","            [model1_roc_auc_dist, model1_pr_auc_dist, model1_f1_dist],\n","            [model2_roc_auc_dist, model2_pr_auc_dist, model2_f1_dist]\n","        ):\n","            min_len = min(len(dist1), len(dist2))\n","            if min_len >= 2:\n","                # Paired t-test\n","                t_stat, p_value_t = ttest_rel(dist1[:min_len], dist2[:min_len])\n","                ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = p_value_t\n","\n","                # Wilcoxon test\n","                if len(set(dist1[:min_len])) > 1 and len(set(dist2[:min_len])) > 1:\n","                    try:\n","                        w_stat, p_value_w = wilcoxon(dist1[:min_len], dist2[:min_len])\n","                        ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = p_value_w\n","                    except ValueError:\n","                        ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                else:\n","                    ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","\n","                 # Mann-Whitney U test (alternative)\n","                u_stat, p_value_u = mannwhitneyu(dist1[:min_len], dist2[:min_len])\n","                ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = p_value_u\n","\n","\n","            else:\n","                ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = np.nan\n","                ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n","\n","\n","        # McNemar test on binary predictions\n","        # Create the contingency table with reindex to ensure 2x2 structure\n","        crosstab_result = pd.crosstab(model1_preds, model2_preds)\n","        contingency_table_mcnemar = crosstab_result.reindex(index=[0, 1], columns=[0, 1], fill_value=0)\n","\n","        # Extract the counts for the McNemar table\n","        n00_val = contingency_table_mcnemar.loc[0, 0]\n","        n01_val = contingency_table_mcnemar.loc[0, 1]\n","        n10_val = contingency_table_mcnemar.loc[1, 0]\n","        n11_val = contingency_table_mcnemar.loc[1, 1]\n","\n","        table_array_mcnemar = np.array([[n00_val, n01_val],\n","                                        [n10_val, n11_val]])\n","\n","        if (n01_val + n10_val) > 0: # Ensure there are discordant pairs for the test to be meaningful\n","            mcnemar_result = mcnemar(table_array_mcnemar, exact=True)\n","            ensemble_statistical_test_results[comparison_key]['McNemar p'] = mcnemar_result.pvalue\n","        else:\n","            ensemble_statistical_test_results[comparison_key]['McNemar p'] = 1.0 # Perfect agreement on discordant pairs\n","\n","\n","# Now, format the results into a readable table\n","print(\"\\nStatistical Test Results Among Ensemble Learners:\")\n","\n","ensemble_comparison_data = []\n","for comparison, tests in ensemble_statistical_test_results.items():\n","    row = {'Comparison': comparison}\n","    for test_name, p_value in tests.items():\n","         if not np.isnan(p_value):\n","            row[test_name] = f\"{p_value:.4f}\"\n","            row[f'Significant? ({test_name})'] = '*' if p_value < alpha else ''\n","         else:\n","            row[test_name] = \"N/A\"\n","            row[f'Significant? ({test_name})'] = ''\n","\n","    ensemble_comparison_data.append(row)\n","\n","ensemble_comparison_df = pd.DataFrame(ensemble_comparison_data)\n","\n","# Define a desired column order for clarity\n","ensemble_ordered_columns = ['Comparison']\n","for metric_name in ['ROC AUC', 'PR AUC', 'F1 Score']:\n","    ensemble_ordered_columns.append(f'{metric_name} (T-test p)')\n","    ensemble_ordered_columns.append(f'Significant? ({metric_name} (T-test p))')\n","    ensemble_ordered_columns.append(f'{metric_name} (Wilcoxon p)')\n","    ensemble_ordered_columns.append(f'Significant? ({metric_name} (Wilcoxon p))')\n","    ensemble_ordered_columns.append(f'{metric_name} (Mann-Whitney p)')\n","    ensemble_ordered_columns.append(f'Significant? ({metric_name} (Mann-Whitney p))')\n","\n","ensemble_ordered_columns.append('McNemar p')\n","ensemble_ordered_columns.append('Significant? (McNemar p)')\n","\n","# Filter columns to only include those present in the DataFrame\n","final_ensemble_columns = [col for col in ensemble_ordered_columns if col in ensemble_comparison_df.columns]\n","ensemble_comparison_df = ensemble_comparison_df[final_ensemble_columns]\n","\n","\n","print(ensemble_comparison_df.to_string())\n","\n","# Optionally save this table\n","output_dir_tables = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n","os.makedirs(output_dir_tables, exist_ok=True)\n","ensemble_comparison_df.to_csv(f\"{output_dir_tables}/ensemble_statistical_comparison.csv\", index=False)\n","\n","# Prepare for LaTeX\n","latex_ensemble_df = ensemble_comparison_df.copy()\n","latex_ensemble_df.columns = latex_ensemble_df.columns.str.replace('_', '\\\\_')\n","latex_ensemble_df.columns = latex_ensemble_df.columns.str.replace('%', '\\\\%')\n","latex_ensemble_df.columns = latex_ensemble_df.columns.str.replace('(', '{(', regex=False).str.replace(')', ')}')\n","\n","latex_ensemble_df.to_latex(f\"{output_dir_tables}/ensemble_statistical_comparison.tex\", index=False, float_format=\"%.4f\", escape=False)\n","\n","print(f\"\\nEnsemble statistical comparison table saved to {output_dir_tables}/ensemble_statistical_comparison.csv and .tex\")"],"metadata":{"id":"37KIAfLMeKRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical Tests Plots and Summary Table"],"metadata":{"id":"-MzPgwQzkyId"}},{"cell_type":"code","source":["# Box/Violin plots of metrics\n","metric_names = ['ROC AUC', 'PR AUC', 'F1 Score']\n","for metric_name in metric_names:\n","    plt.figure(figsize=(12, 6))\n","    plot_data = []\n","    model_order = list(models.keys()) # Maintain a consistent order\n","\n","    for model_name in model_order:\n","        if metric_name in metric_distributions[model_name] and metric_distributions[model_name][metric_name]:\n","             plot_data.append(pd.DataFrame({\n","                 'Model': model_name,\n","                 metric_name: metric_distributions[model_name][metric_name]\n","             }))\n","\n","\n","    if plot_data:\n","        combined_df = pd.concat(plot_data)\n","        # Ensure the order is maintained in the plot\n","        sns.boxplot(x='Model', y=metric_name, data=combined_df, palette='viridis', order=model_order)\n","        # sns.violinplot(x='Model', y=metric_name, data=combined_df, palette='viridis', inner=\"quartile\", order=model_order) # Alternative violin plot\n","        plt.title(f'{metric_name} Distribution from Bootstrap ({n_iterations} Iterations)')\n","        plt.xticks(rotation=45, ha='right')\n","        plt.tight_layout()\n","        #plt.savefig(f\"{output_dir_bootstrap_plots}/{metric_name.replace(' ', '_')}_Bootstrap_BoxPlot.pdf\", dpi=600, bbox_inches='tight')\n","        plt.show()\n","    else:\n","        print(f\"No data available to plot {metric_name}.\")\n","\n","\n","# Function to plot ROC/PR curve with bootstrap confidence bands\n","def plot_curve_with_bands(model_name, bootstrap_samples_for_plotting, title_prefix, filename_prefix, curve_type='roc'):\n","    plt.figure(figsize=(10, 8))\n","\n","    all_curves_interp = []\n","    n_bootstrap_success = 0 # Count how many bootstrap samples were successfully used for curves\n","\n","    if curve_type == 'roc':\n","        base_fpr = np.linspace(0, 1, 101) # Standard x-axis for interpolation\n","        for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n","\n","            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","                 fpr, tpr, _ = roc_curve(y_true_sample, probs_sample)\n","                 interp_tpr = np.interp(base_fpr, fpr, tpr)\n","                 interp_tpr[0] = 0.0 # Ensure start at (0,0)\n","                 all_curves_interp.append(interp_tpr)\n","                 n_bootstrap_success += 1\n","\n","        if n_bootstrap_success > 1: # Need at least 2 curves to compute band\n","            mean_tpr = np.mean(all_curves_interp, axis=0)\n","            # Using t-distribution for smaller sample sizes, but for n=1000, Z is fine\n","            # std_tpr = np.std(all_curves_interp, axis=0)\n","            # ci_width = 1.96 * std_tpr / np.sqrt(n_bootstrap_success)\n","            # tprs_lower = mean_tpr - ci_width\n","            # tprs_upper = mean_tpr + ci_width\n","\n","            # Percentile method for CI band - more robust to non-normal distributions\n","            tprs_lower = np.percentile(all_curves_interp, (alpha / 2) * 100, axis=0)\n","            tprs_upper = np.percentile(all_curves_interp, 100 - (alpha / 2) * 100, axis=0)\n","\n","\n","            tprs_lower = np.maximum(tprs_lower, 0)\n","            tprs_upper = np.minimum(tprs_upper, 1)\n","\n","            # Mean AUC from the bootstrap distribution (calculated earlier)\n","            mean_auc_from_dist = np.mean(metric_distributions[model_name]['ROC AUC']) if metric_distributions[model_name]['ROC AUC'] else np.nan\n","\n","\n","            plt.plot(base_fpr, mean_tpr, label=f'Mean ROC (AUC = {mean_auc_from_dist:.3f})')\n","            plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='95% CI')\n","            plt.plot([0, 1], [0, 1], 'k--', lw=1)\n","            plt.xlabel(\"False Positive Rate\", fontsize=12)\n","            plt.ylabel(\"True Positive Rate\", fontsize=12)\n","            plt.title(f'{title_prefix} ROC Curve with Bootstrap CI', fontsize=14)\n","            plt.legend(loc=\"lower right\", fontsize=10)\n","        else:\n","             print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot ROC CI band for {title_prefix}.\")\n","             # Plot the curve on the original test set if band cannot be plotted\n","             # Check y_test directly for original plot\n","             if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n","                 fpr, tpr, _ = roc_curve(y_test, models[model_name]['probs'])\n","                 auc_orig = roc_auc_score(y_test, models[model_name]['probs'])\n","                 plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_orig:.3f})')\n","                 plt.plot([0, 1], [0, 1], 'k--', lw=1)\n","                 plt.xlabel(\"False Positive Rate\", fontsize=12)\n","                 plt.ylabel(\"True Positive Rate\", fontsize=12)\n","                 plt.title(f'{title_prefix} ROC Curve (No CI band)', fontsize=14)\n","                 plt.legend(loc=\"lower right\", fontsize=10)\n","             else:\n","                  print(f\"Skipping ROC plot for {title_prefix} - not enough unique values in original data.\")\n","\n","\n","    elif curve_type == 'pr':\n","         base_recall = np.linspace(0, 1, 101)\n","         all_precisions_interp = []\n","         n_bootstrap_success = 0\n","\n","         for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n","\n","            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","                precision, recall, _ = precision_recall_curve(y_true_sample, probs_sample)\n","                # Sort by recall for interpolation\n","                sort_indices = np.argsort(recall)\n","                recall = recall[sort_indices]\n","                precision = precision[sort_indices]\n","\n","                # Handle interpolation carefully, especially at low recall\n","                # Append (0, 1) point if not present\n","                if recall[0] > 0:\n","                    recall = np.insert(recall, 0, 0)\n","                    precision = np.insert(precision, 0, 1)\n","\n","                # Append (proportion of positives, proportion of positives) at recall 1 if not present\n","                # or ensure the last point is handled\n","                if recall[-1] < 1.0:\n","                     prop_pos = np.sum(y_true_sample) / len(y_true_sample) if len(y_true_sample) > 0 else 0\n","                     recall = np.append(recall, 1.0)\n","                     precision = np.append(precision, prop_pos) # Precision at recall 1 is proportion of positives\n","\n","                interp_precision = np.interp(base_recall, recall, precision)\n","\n","                all_precisions_interp.append(interp_precision)\n","                n_bootstrap_success += 1\n","\n","         if n_bootstrap_success > 1: # Need at least 2 curves to compute band\n","            mean_precision = np.mean(all_precisions_interp, axis=0)\n","            # std_precision = np.std(all_precisions_interp, axis=0)\n","            # ci_width_pr = 1.96 * std_precision / np.sqrt(n_bootstrap_success)\n","            # precisions_lower = mean_precision - ci_width_pr\n","            # precisions_upper = mean_precision + ci_width_pr\n","\n","            # Percentile method for CI band\n","            precisions_lower = np.percentile(all_precisions_interp, (alpha / 2) * 100, axis=0)\n","            precisions_upper = np.percentile(all_precisions_interp, 100 - (alpha / 2) * 100, axis=0)\n","\n","            precisions_lower = np.maximum(precisions_lower, 0)\n","            precisions_upper = np.minimum(precisions_upper, 1)\n","\n","            # Mean AP from the bootstrap distribution (calculated earlier)\n","            mean_ap_from_dist = np.mean(metric_distributions[model_name]['PR AUC']) if metric_distributions[model_name]['PR AUC'] else np.nan\n","\n","\n","            plt.plot(base_recall, mean_precision, label=f'Mean PR (AP = {mean_ap_from_dist:.3f})')\n","            plt.fill_between(base_recall, precisions_lower, precisions_upper, color='grey', alpha=.2, label='95% CI')\n","            plt.xlabel(\"Recall\", fontsize=12)\n","            plt.ylabel(\"Precision\", fontsize=12)\n","            plt.title(f'{title_prefix} PR Curve with Bootstrap CI', fontsize=14)\n","            plt.legend(loc=\"lower left\", fontsize=10)\n","            plt.ylim([0.0, 1.05])\n","            plt.xlim([0.0, 1.0])\n","         else:\n","              print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot PR CI band for {title_prefix}.\")\n","              # Plot the curve on the original test set if band cannot be plotted\n","              if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n","                precision, recall, _ = precision_recall_curve(y_test, models[model_name]['probs'])\n","                ap_orig = average_precision_score(y_test, models[model_name]['probs'])\n","                plt.plot(recall, precision, label=f'PR (AP = {ap_orig:.3f})')\n","                plt.xlabel(\"Recall\", fontsize=12)\n","                plt.ylabel(\"Precision\", fontsize=12)\n","                plt.title(f'{title_prefix} PR Curve (No CI band)', fontsize=14)\n","                plt.legend(loc=\"lower left\", fontsize=10)\n","                plt.ylim([0.0, 1.05])\n","                plt.xlim([0.0, 1.0])\n","              else:\n","                   print(f\"Skipping PR plot for {title_prefix} - not enough unique values in original data.\")\n","\n","\n","    plt.grid(True, linestyle='--', alpha=0.6)\n","    plt.tight_layout()\n","    #plt.savefig(f\"{output_dir_bootstrap_plots}/{filename_prefix}_{curve_type.upper()}_Curve_Bootstrap_CI.pdf\", dpi=600, bbox_inches='tight')\n","    plt.show()\n","\n","\n","# Plot curves with bands for each model\n","for model_name in models.keys():\n","     plot_curve_with_bands(\n","         model_name,\n","         metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n","         model_name,\n","         model_name.replace(' ', '_').replace('-', '_'),\n","         curve_type='roc'\n","         )\n","     plot_curve_with_bands(\n","         model_name,\n","         metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n","         model_name,\n","         model_name.replace(' ', '_').replace('-', '_'),\n","         curve_type='pr'\n","         )\n","\n","\n","print(\"\\nGenerating Summary Table...\")\n","\n","summary_data = []\n","for model_name in models.keys():\n","    row: Dict[str, Any] = {'Model': model_name}\n","\n","    for metric_name in metric_names:\n","        summary_info = metric_summary[model_name][metric_name]\n","        mean = summary_info['mean']\n","        ci_lower = summary_info['ci_lower']\n","        ci_upper = summary_info['ci_upper']\n","\n","        if not np.isnan(mean):\n","             row[f'{metric_name} Mean  95% CI'] = f\"{mean:.3f}  ({mean - ci_lower:.3f}, {ci_upper - mean:.3f})\"\n","        else:\n","             row[f'{metric_name} Mean  95% CI'] = \"N/A\"\n","\n","\n","    if model_name != 'LSTM-AE':\n","        vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n","        for test_name, p_value in vs_lstm_ae_tests.items():\n","            if not np.isnan(p_value):\n","                row[f'{test_name} (vs LSTM-AE)'] = f\"{p_value:.4f}\"\n","                # Add significance flag (p < alpha)\n","                row[f'Significant? ({test_name})'] = '*' if p_value < alpha else ''\n","            else:\n","                 row[f'{test_name} (vs LSTM-AE)'] = \"N/A\"\n","                 row[f'Significant? ({test_name})'] = ''\n","\n","    summary_data.append(row)\n","\n","summary_df = pd.DataFrame(summary_data)\n","\n","# Reorder columns for better readability\n","ordered_columns = ['Model']\n","for metric_name in metric_names:\n","    ordered_columns.append(f'{metric_name} Mean  95% CI')\n","    if metric_name != 'F1 Score': # Only ROC and PR have T-test and Wilcoxon on distribution\n","        ordered_columns.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n","        ordered_columns.append(f'Significant? ({metric_name} (T-test p))')\n","        ordered_columns.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n","        ordered_columns.append(f'Significant? ({metric_name} (Wilcoxon p))')\n","    else: # F1 score has T-test and Wilcoxon\n","        ordered_columns.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n","        ordered_columns.append(f'Significant? ({metric_name} (T-test p))')\n","        ordered_columns.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n","        ordered_columns.append(f'Significant? ({metric_name} (Wilcoxon p))')\n","\n","\n","ordered_columns.append('McNemar p (vs LSTM-AE)')\n","ordered_columns.append('Significant? (McNemar p)')\n","\n","# Ensure only existing columns are included (handles LSTM-AE which has no vs tests)\n","final_columns = [col for col in ordered_columns if col in summary_df.columns]\n","summary_df = summary_df[final_columns]\n","\n","\n","print(\"\\nSummary Table:\")\n","print(summary_df.to_string()) # Use to_string to display the full table without truncation\n","\n","# Save the summary table to a file\n","output_dir_tables = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n","os.makedirs(output_dir_tables, exist_ok=True)\n","summary_df.to_csv(f\"{output_dir_tables}/model_performance_summary.csv\", index=False)\n","summary_df.to_latex(f\"{output_dir_tables}/model_performance_summary.tex\", index=False, float_format=\"%.3f\")\n","\n","print(\"\\nAnalysis Complete.\")"],"metadata":{"id":"7a9Ded6EWkgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Suggestions for Improvement or Extension\n","# 1. Enhance Boxplot Aesthetics:\n","# Consider adding swarm/strip plots for better visualization of distribution density:\n","# sns.boxplot(...)  # Existing line\n","# sns.stripplot(x='Model', y=metric_name, data=combined_df, color='black', alpha=0.3, jitter=True, size=3)\n","# 2. Add Effect Sizes to Summary Table:\n","# P-values tell you if a difference exists, not how large it is.\n","# \tInclude Cohens d or Cliffs Delta for effect size on continuous metrics.\n","# \tFor paired comparisons (e.g., ROC AUCs), Cohens d can be added easily:\n","# def cohen_d(x, y):\n","#     return (np.mean(x) - np.mean(y)) / np.sqrt((np.std(x, ddof=1) ** 2 + np.std(y, ddof=1) ** 2) / 2)\n","# 3. Show Sample Size in Plots:\n","# Add bootstrap sample count directly in the titles:\n","# plt.title(f'{title_prefix} ROC Curve with Bootstrap CI\\n(N = {n_bootstrap_success} bootstrap samples)', fontsize=14)\n","# 4. Make Alpha Adjustable:\n","# Instead of a hardcoded alpha, you could parameterize it:\n","# def plot_curve_with_bands(..., alpha=0.05):\n","# 5. Save Plots in Multiple Formats (PDF, PNG):\n","# To ensure compatibility across journals and presentations:\n","# plt.savefig(f\"{output_dir}/{filename}.pdf\", dpi=600)\n","# plt.savefig(f\"{output_dir}/{filename}.png\", dpi=300)\n","# Optional: Parallelize Bootstrap Computation\n","# If bootstrap iteration is time-consuming, you could parallelize curve computation using joblib:\n","# from joblib import Parallel, delayed\n","# def compute_bootstrap_curve(...):  # Wrapper function\n","#     ...\n","# results = Parallel(n_jobs=-1)(delayed(compute_bootstrap_curve)(...) for _ in range(n_iterations))\n","# Optional: Include Metrics Distribution in Summary Table\n","# Add SD or full distribution stats:\n","# row[f'{metric_name} Std'] = f\"{np.std(metric_distributions[model_name][metric_name]):.3f}\"\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Enhance Boxplot Aesthetics: Add swarm/strip plots\n","metric_names = ['ROC AUC', 'PR AUC', 'F1 Score']\n","output_dir_bootstrap_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Bootstrap_Plots/\"\n","os.makedirs(output_dir_bootstrap_plots, exist_ok=True)\n","\n","for metric_name in metric_names:\n","    plt.figure(figsize=(12, 6))\n","    plot_data = []\n","    model_order = list(models.keys())  # Maintain a consistent order\n","\n","    for model_name in model_order:\n","        if metric_name in metric_distributions[model_name] and metric_distributions[model_name][metric_name]:\n","            plot_data.append(pd.DataFrame({\n","                'Model': model_name,\n","                metric_name: metric_distributions[model_name][metric_name]\n","            }))\n","\n","    if plot_data:\n","        combined_df = pd.concat(plot_data)\n","        # Ensure the order is maintained in the plot\n","        sns.boxplot(x='Model', y=metric_name, data=combined_df, palette='viridis', order=model_order)\n","        # Add stripplot for individual data points\n","        sns.stripplot(x='Model', y=metric_name, data=combined_df, color='black', alpha=0.3, jitter=True, size=3, order=model_order)\n","\n","        plt.title(f'{metric_name} Distribution from Bootstrap ({n_iterations} Iterations)')\n","        plt.xticks(rotation=45, ha='right')\n","        plt.tight_layout()\n","        plt.savefig(f\"{output_dir_bootstrap_plots}/{metric_name.replace(' ', '_')}_Bootstrap_BoxPlot_Stripplot.pdf\", dpi=600, bbox_inches='tight')\n","        plt.show()\n","    else:\n","        print(f\"No data available to plot {metric_name}.\")\n","\n","\n","# Modify plot_curve_with_bands to show sample size in title\n","def plot_curve_with_bands(model_name, bootstrap_samples_for_plotting, title_prefix, filename_prefix, curve_type='roc', n_bootstrap_total=None):\n","    plt.figure(figsize=(10, 8))\n","\n","    all_curves_interp = []\n","    n_bootstrap_success = 0  # Count how many bootstrap samples were successfully used for curves\n","\n","    if curve_type == 'roc':\n","        base_fpr = np.linspace(0, 1, 101)  # Standard x-axis for interpolation\n","        for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n","            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","                fpr, tpr, _ = roc_curve(y_true_sample, probs_sample)\n","                interp_tpr = np.interp(base_fpr, fpr, tpr)\n","                interp_tpr[0] = 0.0  # Ensure start at (0,0)\n","                all_curves_interp.append(interp_tpr)\n","                n_bootstrap_success += 1\n","\n","        if n_bootstrap_success > 1:  # Need at least 2 curves to compute band\n","            mean_tpr = np.mean(all_curves_interp, axis=0)\n","            tprs_lower = np.percentile(all_curves_interp, (alpha / 2) * 100, axis=0)\n","            tprs_upper = np.percentile(all_curves_interp, 100 - (alpha / 2) * 100, axis=0)\n","\n","            tprs_lower = np.maximum(tprs_lower, 0)\n","            tprs_upper = np.minimum(tprs_upper, 1)\n","\n","            mean_auc_from_dist = np.mean(metric_distributions[model_name]['ROC AUC']) if metric_distributions[model_name]['ROC AUC'] else np.nan\n","\n","            plt.plot(base_fpr, mean_tpr, label=f'Mean ROC (AUC = {mean_auc_from_dist:.3f})')\n","            plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='95% CI')\n","            plt.plot([0, 1], [0, 1], 'k--', lw=1)\n","            plt.xlabel(\"False Positive Rate\", fontsize=12)\n","            plt.ylabel(\"True Positive Rate\", fontsize=12)\n","            # Include sample size in title\n","            plt.title(f'{title_prefix} ROC Curve with Bootstrap CI\\n(N = {n_bootstrap_total} bootstrap samples, {n_bootstrap_success} successful)', fontsize=14)\n","            plt.legend(loc=\"lower right\", fontsize=10)\n","        else:\n","            print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot ROC CI band for {title_prefix}.\")\n","            # Plot the curve on the original test set if band cannot be plotted\n","            if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n","                fpr, tpr, _ = roc_curve(y_test, models[model_name]['probs'])\n","                auc_orig = roc_auc_score(y_test, models[model_name]['probs'])\n","                plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_orig:.3f})')\n","                plt.plot([0, 1], [0, 1], 'k--', lw=1)\n","                plt.xlabel(\"False Positive Rate\", fontsize=12)\n","                plt.ylabel(\"True Positive Rate\", fontsize=12)\n","                plt.title(f'{title_prefix} ROC Curve (No CI band)\\n(N = {len(y_test)} test samples)', fontsize=14) # Use test set size for this case\n","                plt.legend(loc=\"lower right\", fontsize=10)\n","            else:\n","                print(f\"Skipping ROC plot for {title_prefix} - not enough unique values in original data.\")\n","\n","\n","    elif curve_type == 'pr':\n","        base_recall = np.linspace(0, 1, 101)\n","        all_precisions_interp = []\n","        n_bootstrap_success = 0\n","\n","        for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n","            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","                precision, recall, _ = precision_recall_curve(y_true_sample, probs_sample)\n","                sort_indices = np.argsort(recall)\n","                recall = recall[sort_indices]\n","                precision = precision[sort_indices]\n","\n","                if recall[0] > 0:\n","                    recall = np.insert(recall, 0, 0)\n","                    precision = np.insert(precision, 0, 1)\n","\n","                if recall[-1] < 1.0:\n","                    prop_pos = np.sum(y_true_sample) / len(y_true_sample) if len(y_true_sample) > 0 else 0\n","                    recall = np.append(recall, 1.0)\n","                    precision = np.append(precision, prop_pos)\n","\n","                interp_precision = np.interp(base_recall, recall, precision)\n","\n","                all_precisions_interp.append(interp_precision)\n","                n_bootstrap_success += 1\n","\n","        if n_bootstrap_success > 1:  # Need at least 2 curves to compute band\n","            mean_precision = np.mean(all_precisions_interp, axis=0)\n","            precisions_lower = np.percentile(all_precisions_interp, (alpha / 2) * 100, axis=0)\n","            precisions_upper = np.percentile(all_precisions_interp, 100 - (alpha / 2) * 100, axis=0)\n","\n","            precisions_lower = np.maximum(precisions_lower, 0)\n","            precisions_upper = np.minimum(precisions_upper, 1)\n","\n","            mean_ap_from_dist = np.mean(metric_distributions[model_name]['PR AUC']) if metric_distributions[model_name]['PR AUC'] else np.nan\n","\n","            plt.plot(base_recall, mean_precision, label=f'Mean PR (AP = {mean_ap_from_dist:.3f})')\n","            plt.fill_between(base_recall, precisions_lower, precisions_upper, color='grey', alpha=.2, label='95% CI')\n","            plt.xlabel(\"Recall\", fontsize=12)\n","            plt.ylabel(\"Precision\", fontsize=12)\n","            # Include sample size in title\n","            plt.title(f'{title_prefix} PR Curve with Bootstrap CI\\n(N = {n_bootstrap_total} bootstrap samples, {n_bootstrap_success} successful)', fontsize=14)\n","            plt.legend(loc=\"lower left\", fontsize=10)\n","            plt.ylim([0.0, 1.05])\n","            plt.xlim([0.0, 1.0])\n","        else:\n","            print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot PR CI band for {title_prefix}.\")\n","            # Plot the curve on the original test set if band cannot be plotted\n","            if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n","                precision, recall, _ = precision_recall_curve(y_test, models[model_name]['probs'])\n","                ap_orig = average_precision_score(y_test, models[model_name]['probs'])\n","                plt.plot(recall, precision, label=f'PR (AP = {ap_orig:.3f})')\n","                plt.xlabel(\"Recall\", fontsize=12)\n","                plt.ylabel(\"Precision\", fontsize=12)\n","                plt.title(f'{title_prefix} PR Curve (No CI band)\\n(N = {len(y_test)} test samples)', fontsize=14) # Use test set size for this case\n","                plt.legend(loc=\"lower left\", fontsize=10)\n","                plt.ylim([0.0, 1.05])\n","                plt.xlim([0.0, 1.0])\n","            else:\n","                print(f\"Skipping PR plot for {title_prefix} - not enough unique values in original data.\")\n","\n","    plt.grid(True, linestyle='--', alpha=0.6)\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_dir_bootstrap_plots}/{filename_prefix}_{curve_type.upper()}_Curve_Bootstrap_CI.pdf\", dpi=600, bbox_inches='tight')\n","    plt.show()\n","\n","# Plot curves with bands using the modified function\n","for model_name in models.keys():\n","    plot_curve_with_bands(\n","        model_name,\n","        metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n","        model_name,\n","        model_name.replace(' ', '_').replace('-', '_'),\n","        curve_type='roc',\n","        n_bootstrap_total=n_iterations # Pass the total number of iterations\n","    )\n","    plot_curve_with_bands(\n","        model_name,\n","        metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n","        model_name,\n","        model_name.replace(' ', '_').replace('-', '_'),\n","        curve_type='pr',\n","        n_bootstrap_total=n_iterations # Pass the total number of iterations\n","    )\n","\n","# Add Effect Sizes to Summary Table: Include Cohens d\n","# Cohen's d for paired samples (appropriate for paired bootstrap samples)\n","# We'll compare each model's bootstrap distribution to the LSTM-AE's distribution\n","\n","def cohen_d_paired(x, y):\n","    \"\"\"Calculates Cohen's d for paired samples.\"\"\"\n","    diff = np.array(x) - np.array(y)\n","    mean_diff = np.mean(diff)\n","    std_diff = np.std(diff, ddof=1) # Use ddof=1 for sample standard deviation\n","    # Handle case where std_diff is zero\n","    if std_diff == 0:\n","        return np.inf if mean_diff != 0 else 0.0\n","    return mean_diff / std_diff\n","\n","print(\"\\nCalculating Effect Sizes (Cohen's d vs LSTM-AE) and updating Summary Table...\")\n","\n","# Update summary_data list with effect sizes\n","summary_data = []\n","for model_name in models.keys():\n","    row: Dict[str, Any] = {'Model': model_name}\n","\n","    for metric_name in metric_names:\n","        summary_info = metric_summary[model_name][metric_name]\n","        mean = summary_info['mean']\n","        ci_lower = summary_info['ci_lower']\n","        ci_upper = summary_info['ci_upper']\n","        distribution = summary_info['distribution'] # Get the distribution\n","\n","        if not np.isnan(mean):\n","            row[f'{metric_name} Mean (95% CI)'] = f\"{mean:.3f} ({ci_lower:.3f} - {ci_upper:.3f})\"\n","        else:\n","            row[f'{metric_name} Mean (95% CI)'] = \"N/A\"\n","\n","        # Calculate Cohen's d vs LSTM-AE if not the LSTM-AE model and distributions exist\n","        if model_name != 'LSTM-AE' and distribution and metric_distributions['LSTM-AE'][metric_name]:\n","             lstm_ae_dist = metric_distributions['LSTM-AE'][metric_name]\n","             min_len = min(len(distribution), len(lstm_ae_dist))\n","\n","             if min_len >= 2: # Need at least 2 samples for Cohen's d\n","                # Use the truncated distributions for Cohen's d to match the paired test length\n","                d = cohen_d_paired(distribution[:min_len], lstm_ae_dist[:min_len])\n","                row[f'{metric_name} Cohen\\'s d (vs LSTM-AE)'] = f\"{d:.3f}\"\n","             else:\n","                 row[f'{metric_name} Cohen\\'s d (vs LSTM-AE)'] = \"N/A\"\n","        else:\n","            row[f'{metric_name} Cohen\\'s d (vs LSTM-AE)'] = \"N/A\"\n","\n","\n","    if model_name != 'LSTM-AE':\n","        vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n","        for test_name, p_value in vs_lstm_ae_tests.items():\n","            if not np.isnan(p_value):\n","                row[f'{test_name} (vs LSTM-AE)'] = f\"{p_value:.4f}\"\n","                row[f'Sig ({test_name})'] = '*' if p_value < alpha else ''\n","            else:\n","                row[f'{test_name} (vs LSTM-AE)'] = \"N/A\"\n","                row[f'Sig ({test_name})'] = ''\n","\n","    summary_data.append(row)\n","\n","summary_df_updated = pd.DataFrame(summary_data)\n","\n","# Reorder columns for better readability with effect sizes\n","ordered_columns_updated = ['Model']\n","for metric_name in metric_names:\n","    ordered_columns_updated.append(f'{metric_name} Mean (95% CI)')\n","    ordered_columns_updated.append(f'{metric_name} Cohen\\'s d (vs LSTM-AE)')\n","    # Add p-values and significance flags for statistical tests related to this metric\n","    if metric_name != 'F1 Score':\n","         ordered_columns_updated.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n","         ordered_columns_updated.append(f'Sig ({metric_name} (T-test p))')\n","         ordered_columns_updated.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n","         ordered_columns_updated.append(f'Sig ({metric_name} (Wilcoxon p))')\n","         ordered_columns_updated.append(f'{metric_name} (Mann-Whitney p) (vs LSTM-AE)')\n","         ordered_columns_updated.append(f'Sig ({metric_name} (Mann-Whitney p))')\n","\n","    else:\n","        ordered_columns_updated.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n","        ordered_columns_updated.append(f'Sig ({metric_name} (T-test p))')\n","        ordered_columns_updated.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n","        ordered_columns_updated.append(f'Sig ({metric_name} (Wilcoxon p))')\n","        ordered_columns_updated.append(f'{metric_name} (Mann-Whitney p) (vs LSTM-AE)')\n","        ordered_columns_updated.append(f'Sig ({metric_name} (Mann-Whitney p))')\n","\n","\n","# Add McNemar test results at the end\n","ordered_columns_updated.append('McNemar p (vs LSTM-AE)')\n","ordered_columns_updated.append('Sig (McNemar p)')\n","\n","\n","# Ensure only existing columns are included\n","final_columns_updated = [col for col in ordered_columns_updated if col in summary_df_updated.columns]\n","summary_df_updated = summary_df_updated[final_columns_updated]\n","\n","\n","print(\"\\nUpdated Summary Table with Effect Sizes and Detailed Statistical Tests:\")\n","print(summary_df_updated.to_string())\n","\n","# Save the updated summary table\n","summary_df_updated.to_csv(f\"{output_dir_tables}/model_performance_summary_with_effect_sizes.csv\", index=False)\n","# Escape underscores for LaTeX\n","summary_df_updated.columns = summary_df_updated.columns.str.replace('_', '\\\\_')\n","summary_df_updated.to_latex(f\"{output_dir_tables}/model_performance_summary_with_effect_sizes.tex\", index=False, float_format=\"%.3f\", escape=False)"],"metadata":{"id":"lvWoW2UkFib1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Compose the last updated summary table either in csv or best format for readability or interpretation\n","\n","import pandas as pd\n","import numpy as np\n","# Generate the summary table from the collected data\n","summary_data = []\n","for model_name in models.keys():\n","    row = {'Model': model_name}\n","    # Add Mean and 95% CI for each metric\n","    for metric_name in metric_names:\n","        summary_info = metric_summary[model_name][metric_name]\n","        mean = summary_info['mean']\n","        ci_lower = summary_info['ci_lower']\n","        ci_upper = summary_info['ci_upper']\n","\n","        if not np.isnan(mean):\n","            # Format as \"Mean (Lower CI - Upper CI)\" for clarity\n","            row[f'{metric_name} Mean (95% CI)'] = f\"{mean:.3f} ({ci_lower:.3f} - {ci_upper:.3f})\"\n","        else:\n","            row[f'{metric_name} Mean (95% CI)'] = \"N/A\"\n","\n","    # Add Cohen's d vs LSTM-AE\n","    for metric_name in metric_names:\n","        cohen_d_key = f'{metric_name} Cohen\\'s d (vs LSTM-AE)'\n","        if cohen_d_key in summary_df_updated.columns: # Check if the column exists (will not exist for LSTM-AE row)\n","            cohen_d_value = summary_df_updated.loc[summary_df_updated['Model'] == model_name, cohen_d_key].iloc[0]\n","            row[cohen_d_key] = cohen_d_value\n","        else:\n","             row[cohen_d_key] = \"N/A\"\n","\n","\n","    # Add Statistical Test p-values and Significance flags vs LSTM-AE\n","    if model_name != 'LSTM-AE':\n","        vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n","        for test_name, p_value in vs_lstm_ae_tests.items():\n","            # Extract base metric name and test type\n","            parts = test_name.rsplit(' ', 3) # Split from right 3 times\n","            if len(parts) == 4: # Expected format \"Metric Name (Test Type p)\"\n","                 metric_base_name = parts[0]\n","                 test_type_part = f'{parts[1]} {parts[2]}' # e.g., \"(T-test p)\" or \"(Wilcoxon p)\"\n","                 test_column_name = f'{metric_base_name} {test_type_part} (vs LSTM-AE)'\n","                 sig_column_name = f'Sig ({metric_base_name} {test_type_part})'\n","\n","                 if test_column_name in summary_df_updated.columns:\n","                    row[test_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, test_column_name].iloc[0]\n","                    row[sig_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, sig_column_name].iloc[0]\n","            elif test_name.startswith('McNemar'): # Special case for McNemar\n","                test_column_name = 'McNemar p (vs LSTM-AE)'\n","                sig_column_name = 'Sig (McNemar p)'\n","                if test_column_name in summary_df_updated.columns:\n","                    row[test_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, test_column_name].iloc[0]\n","                    row[sig_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, sig_column_name].iloc[0]\n","\n","\n","    summary_data.append(row)\n","\n","\n","final_summary_df = pd.DataFrame(summary_data)\n","\n","# Reorder columns for the final table structure\n","# Ensure all potential columns from the previous calculations are included\n","potential_columns = ['Model']\n","for metric_name in metric_names:\n","    potential_columns.append(f'{metric_name} Mean (95% CI)')\n","    potential_columns.append(f'{metric_name} Cohen\\'s d (vs LSTM-AE)')\n","    potential_columns.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n","    potential_columns.append(f'Sig ({metric_name} (T-test p))')\n","    potential_columns.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n","    potential_columns.append(f'Sig ({metric_name} (Wilcoxon p))')\n","    potential_columns.append(f'{metric_name} (Mann-Whitney p) (vs LSTM-AE)')\n","    potential_columns.append(f'Sig ({metric_name} (Mann-Whitney p))') # Ensure Mann-Whitney is included\n","\n","potential_columns.append('McNemar p (vs LSTM-AE)')\n","potential_columns.append('Sig (McNemar p)')\n","\n","\n","# Filter columns to only include those present in the final_summary_df\n","final_columns_ordered = [col for col in potential_columns if col in final_summary_df.columns]\n","final_summary_df = final_summary_df[final_columns_ordered]\n","\n","\n","print(\"\\nFinal Comprehensive Summary Table:\")\n","# Display the table, preventing truncation\n","print(final_summary_df.to_string())\n","\n","# Save the final comprehensive table\n","final_summary_df.to_csv(f\"{output_dir_tables}/final_model_performance_summary.csv\", index=False)\n","\n","# Prepare for LaTeX: Escape special characters, especially underscores in column names\n","latex_df = final_summary_df.copy()\n","latex_df.columns = latex_df.columns.str.replace('_', '\\\\_')\n","latex_df.columns = latex_df.columns.str.replace('%', '\\\\%') # Escape % as well\n","latex_df.columns = latex_df.columns.str.replace('(', '{(').str.replace(')', ')}') # Handle parentheses in column names\n","latex_df.to_latex(f\"{output_dir_tables}/final_model_performance_summary.tex\", index=False, float_format=\"%.3f\", escape=False)\n","\n","print(f\"\\nSummary table saved to {output_dir_tables}/final_model_performance_summary.csv and .tex\")\n"],"metadata":{"id":"fuh2YmOjcT7a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: **Summary Table:**\n","#     *   Create a comprehensive **summary table** (pandas DataFrame) consolidating the results.\n","#     *   Include a row for each model.\n","#     *   Columns should include: Model Name, Mean  95% CI for ROC AUC, PR AUC, and F1-score (from bootstrap).\n","#     *   For models other than LSTM-AE, include columns showing the p-values from the paired t-test, Wilcoxon test, and McNemar's test when compared to the LSTM-AE.\n","#     *   Add a column or indicator (e.g., asterisk on p-values) to flag statistically significant differences (p < 0.05).\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","from typing import Dict, List, Any\n","from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n","from statsmodels.stats.contingency_tables import mcnemar # Correct import for mcnemar\n","\n","# Assuming models, metric_summary, and statistical_test_results dictionaries\n","# are populated from the previous code execution.\n","# Also assuming metric_distributions is available and contains the bootstrap distributions.\n","\n","# Define the metrics we want in the table\n","metric_names = ['ROC AUC', 'PR AUC', 'F1 Score']\n","\n","print(\"\\nGenerating Comprehensive Summary Table...\")\n","\n","summary_data = []\n","for model_name in models.keys():\n","    row: Dict[str, Any] = {'Model': model_name}\n","\n","    # Add Mean and 95% CI for each metric\n","    for metric_name in metric_names:\n","        # Ensure the metric exists for the model in metric_summary\n","        if model_name in metric_summary and metric_name in metric_summary[model_name]:\n","            summary_info = metric_summary[model_name][metric_name]\n","            mean = summary_info['mean']\n","            ci_lower = summary_info['ci_lower']\n","            ci_upper = summary_info['ci_upper']\n","\n","            if not np.isnan(mean):\n","                # Format as \"Mean (Lower CI - Upper CI)\"\n","                row[f'{metric_name} Mean (95% CI)'] = f\"{mean:.3f} ({ci_lower:.3f} - {ci_upper:.3f})\"\n","            else:\n","                row[f'{metric_name} Mean (95% CI)'] = \"N/A\"\n","        else:\n","            row[f'{metric_name} Mean (95% CI)'] = \"N/A\" # Metric info missing\n","\n","\n","    # Add Statistical Test p-values and Significance flags vs LSTM-AE\n","    if model_name != 'LSTM-AE':\n","        # Ensure test results exist for the model\n","        if 'vs LSTM-AE' in statistical_test_results and model_name in statistical_test_results['vs LSTM-AE']:\n","            vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n","\n","            # Define the specific tests to include and their order\n","            tests_to_include = {\n","                'ROC AUC (T-test p)': 'ROC AUC (T-test p) (vs LSTM-AE)',\n","                'ROC AUC (Wilcoxon p)': 'ROC AUC (Wilcoxon p) (vs LSTM-AE)',\n","                # 'ROC AUC (Mann-Whitney p)': 'ROC AUC (Mann-Whitney p) (vs LSTM-AE)', # Include if Mann-Whitney was used\n","                'PR AUC (T-test p)': 'PR AUC (T-test p) (vs LSTM-AE)',\n","                'PR AUC (Wilcoxon p)': 'PR AUC (Wilcoxon p) (vs LSTM-AE)',\n","                 # 'PR AUC (Mann-Whitney p)': 'PR AUC (Mann-Whitney p) (vs LSTM-AE)', # Include if Mann-Whitney was used\n","                'F1 Score (T-test p)': 'F1 Score (T-test p) (vs LSTM-AE)',\n","                'F1 Score (Wilcoxon p)': 'F1 Score (Wilcoxon p) (vs LSTM-AE)',\n","                # 'F1 Score (Mann-Whitney p)': 'F1 Score (Mann-Whitney p) (vs LSTM-AE)', # Include if Mann-Whitney was used\n","                'McNemar p': 'McNemar p (vs LSTM-AE)'\n","            }\n","\n","            for test_key, column_name in tests_to_include.items():\n","                p_value = vs_lstm_ae_tests.get(test_key) # Use .get to handle potentially missing keys\n","\n","                if p_value is not None and not np.isnan(p_value):\n","                    row[column_name] = f\"{p_value:.4f}\"\n","                    # Add significance flag (p < 0.05)\n","                    row[f'Significant? ({test_key})'] = '*' if p_value < 0.05 else ''\n","                else:\n","                    row[column_name] = \"N/A\"\n","                    row[f'Significant? ({test_key})'] = ''\n","        else:\n","            # Add N/A for all test columns if no test results are found for the model\n","            # This ensures consistent columns even if testing failed for a model\n","            tests_to_include = {\n","                'ROC AUC (T-test p)': 'ROC AUC (T-test p) (vs LSTM-AE)',\n","                'ROC AUC (Wilcoxon p)': 'ROC AUC (Wilcoxon p) (vs LSTM-AE)',\n","                # 'ROC AUC (Mann-Whitney p)': 'ROC AUC (Mann-Whitney p) (vs LSTM-AE)',\n","                'PR AUC (T-test p)': 'PR AUC (T-test p) (vs LSTM-AE)',\n","                'PR AUC (Wilcoxon p)': 'PR AUC (Wilcoxon p) (vs LSTM-AE)',\n","                # 'PR AUC (Mann-Whitney p)': 'PR AUC (Mann-Whitney p) (vs LSTM-AE)',\n","                'F1 Score (T-test p)': 'F1 Score (T-test p) (vs LSTM-AE)',\n","                'F1 Score (Wilcoxon p)': 'F1 Score (Wilcoxon p) (vs LSTM-AE)',\n","                # 'F1 Score (Mann-Whitney p)': 'F1 Score (Mann-Whitney p) (vs LSTM-AE)',\n","                'McNemar p': 'McNemar p (vs LSTM-AE)'\n","            }\n","            for test_key, column_name in tests_to_include.items():\n","                row[column_name] = \"N/A\"\n","                row[f'Significant? ({test_key})'] = ''\n","\n","\n","    summary_data.append(row)\n","\n","# Create the DataFrame from the collected data\n","summary_df = pd.DataFrame(summary_data)\n","\n","# Define the desired column order\n","# This ensures the table has a consistent structure\n","ordered_columns = ['Model']\n","for metric_name in metric_names:\n","    ordered_columns.append(f'{metric_name} Mean (95% CI)')\n","    # Add test columns related to this metric\n","    if metric_name == 'ROC AUC':\n","        ordered_columns.append('ROC AUC (T-test p) (vs LSTM-AE)')\n","        ordered_columns.append('Significant? (ROC AUC (T-test p))')\n","        ordered_columns.append('ROC AUC (Wilcoxon p) (vs LSTM-AE)')\n","        ordered_columns.append('Significant? (ROC AUC (Wilcoxon p))')\n","        # ordered_columns.append('ROC AUC (Mann-Whitney p) (vs LSTM-AE)') # Include if used\n","        # ordered_columns.append('Significant? (ROC AUC (Mann-Whitney p))')\n","    elif metric_name == 'PR AUC':\n","        ordered_columns.append('PR AUC (T-test p) (vs LSTM-AE)')\n","        ordered_columns.append('Significant? (PR AUC (T-test p))')\n","        ordered_columns.append('PR AUC (Wilcoxon p) (vs LSTM-AE)')\n","        ordered_columns.append('Significant? (PR AUC (Wilcoxon p))')\n","        # ordered_columns.append('PR AUC (Mann-Whitney p) (vs LSTM-AE)') # Include if used\n","        # ordered_columns.append('Significant? (PR AUC (Mann-Whitney p))')\n","    elif metric_name == 'F1 Score':\n","        ordered_columns.append('F1 Score (T-test p) (vs LSTM-AE)')\n","        ordered_columns.append('Significant? (F1 Score (T-test p))')\n","        ordered_columns.append('F1 Score (Wilcoxon p) (vs LSTM-AE)')\n","        ordered_columns.append('Significant? (F1 Score (Wilcoxon p))')\n","        # ordered_columns.append('F1 Score (Mann-Whitney p) (vs LSTM-AE)') # Include if used\n","        # ordered_columns.append('Significant? (F1 Score (Mann-Whitney p))')\n","\n","\n","# Add McNemar's test results\n","ordered_columns.append('McNemar p (vs LSTM-AE)')\n","ordered_columns.append('Significant? (McNemar p)')\n","\n","\n","# Filter columns to only include those that actually exist in the DataFrame\n","final_columns = [col for col in ordered_columns if col in summary_df.columns]\n","summary_df = summary_df[final_columns]\n","\n","\n","print(\"\\nSummary Table:\")\n","# Use to_string() to print the entire DataFrame without truncation\n","print(summary_df.to_string())\n","\n","# You might want to save this DataFrame to a file (e.g., CSV, Excel)\n","# Example: Save to CSV\n","# summary_df.to_csv(\"model_performance_summary.csv\", index=False)"],"metadata":{"id":"UFz071cwnPkd"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e02db5b3"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Create dummy data for illustration (replace with actual data if needed)\n","# Assume a single sample with a few time steps and features\n","sample_input = np.array([\n","    [0.1, 0.2, 0.8],\n","    [0.3, 0.4, 0.7],\n","    [0.5, 0.6, 0.6],\n","    [0.7, 0.8, 0.5]\n","]) # Shape (timesteps, features)\n","\n","# Simulate autoencoder reconstruction\n","# Add some small errors to the original input\n","sample_reconstruction = sample_input + np.random.randn(*sample_input.shape) * 0.05\n","sample_reconstruction = np.clip(sample_reconstruction, 0, 1) # Keep within a reasonable range\n","\n","\n","# Calculate absolute error\n","absolute_error = np.abs(sample_input - sample_reconstruction)\n","\n","# Calculate mean absolute error (MAE)\n","mean_absolute_error = np.mean(absolute_error)\n","\n","# --- Create the Visualization (Optimized for Small Size, High Res) ---\n","plt.figure(figsize=(6, 4), dpi=600) # Smaller figure size, higher DPI\n","\n","# Adjust font sizes for better readability in a smaller plot\n","title_fontsize = 10\n","label_fontsize = 8\n","tick_fontsize = 7\n","annotation_fontsize = 8\n","\n","# Plot 1: Original Input\n","plt.subplot(3, 1, 1)\n","plt.imshow(sample_input.T, cmap='viridis', aspect='auto', interpolation='nearest')\n","plt.title(\"Original Time Series Input\", fontsize=title_fontsize)\n","plt.xlabel(\"Time Steps\", fontsize=label_fontsize)\n","plt.ylabel(\"Features\", fontsize=label_fontsize)\n","plt.xticks(fontsize=tick_fontsize)\n","plt.yticks(fontsize=tick_fontsize)\n","plt.colorbar(label=\"Value\", fraction=0.046, pad=0.04) # Adjust colorbar size/position\n","\n","# Plot 2: Autoencoder Reconstruction\n","plt.subplot(3, 1, 2)\n","plt.imshow(sample_reconstruction.T, cmap='viridis', aspect='auto', interpolation='nearest')\n","plt.title(\"Autoencoder Reconstruction\", fontsize=title_fontsize)\n","plt.xlabel(\"Time Steps\", fontsize=label_fontsize)\n","plt.ylabel(\"Features\", fontsize=label_fontsize)\n","plt.xticks(fontsize=tick_fontsize)\n","plt.yticks(fontsize=tick_fontsize)\n","plt.colorbar(label=\"Value\", fraction=0.046, pad=0.04) # Adjust colorbar size/position\n","\n","# Plot 3: Absolute Error\n","plt.subplot(3, 1, 3)\n","plt.imshow(absolute_error.T, cmap='Reds', aspect='auto', interpolation='nearest')\n","plt.title(\"Absolute Error (|Original - Reconstruction|)\", fontsize=title_fontsize)\n","plt.xlabel(\"Time Steps\", fontsize=label_fontsize)\n","plt.ylabel(\"Features\", fontsize=label_fontsize)\n","plt.xticks(fontsize=tick_fontsize)\n","plt.yticks(fontsize=tick_fontsize)\n","plt.colorbar(label=\"Absolute Error\", fraction=0.046, pad=0.04) # Adjust colorbar size/position\n","\n","\n","# Add text annotations to explain the process\n","# Adjust position and font size of annotations\n","plt.figtext(0.5, 0.67, \"Autoencoder learns to reconstruct normal data\", ha=\"center\", fontsize=annotation_fontsize, bbox={\"boxstyle\":\"round\",\"fc\":\"aliceblue\",\"alpha\":0.8})\n","plt.figtext(0.5, 0.34, r\"$Error = |Original - Reconstruction|$\", ha=\"center\", fontsize=annotation_fontsize, bbox={\"boxstyle\":\"round\",\"fc\":\"aliceblue\",\"alpha\":0.8})\n","plt.figtext(0.5, 0.05, f\"Mean Absolute Error (MAE) = Average(Error) = {mean_absolute_error:.4f} (Anomaly Score)\", ha=\"center\", fontsize=annotation_fontsize, bbox={\"boxstyle\":\"round\",\"fc\":\"lightgreen\",\"alpha\":0.8})\n","\n","\n","plt.tight_layout(rect=[0, 0.08, 1, 1]) # Adjust layout to make space for the text\n","plt.suptitle(\"Visualization of Reconstruction Error Calculation\", y=1.02, fontsize=title_fontsize + 2) # Adjust suptitle size\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4db710c"},"source":["# Re-run the bootstrap analysis to populate metric_distributions\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","from typing import Dict, List, Any\n","import os\n","\n","# Assuming the following variables are available from previous steps:\n","# models (dictionary of trained models and their predictions/probabilities)\n","# X_test, y_test (test data)\n","# n_iterations (number of bootstrap iterations)\n","# alpha (significance level)\n","\n","# Initialize dictionaries to store bootstrap results\n","metric_distributions: Dict[str, Dict[str, List[float]]] = {\n","    model_name: {'ROC AUC': [], 'PR AUC': [], 'F1 Score': [], 'bootstrap_samples_for_plotting': []}\n","    for model_name in models.keys()\n","}\n","\n","# Perform bootstrap resampling\n","print(f\"Performing bootstrap resampling ({n_iterations} iterations)...\")\n","\n","for i in range(n_iterations):\n","    # Resample the test set with replacement\n","    bootstrap_indices = np.random.choice(len(y_test), size=len(y_test), replace=True)\n","    y_test_sample = y_test[bootstrap_indices]\n","\n","    for model_name, data in models.items():\n","        # Get probabilities and binary predictions for the bootstrap sample\n","        probs_sample = data['probs'][bootstrap_indices]\n","        preds_sample = data['preds'][bootstrap_indices] # Assuming binary predictions are available\n","\n","        # Calculate metrics on the bootstrap sample\n","        # ROC AUC\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","            roc_auc = roc_auc_score(y_test_sample, probs_sample)\n","            metric_distributions[model_name]['ROC AUC'].append(roc_auc)\n","        else:\n","            # Handle cases where resampling results in only one class or constant probabilities\n","            metric_distributions[model_name]['ROC AUC'].append(np.nan) # Append NaN for invalid samples\n","\n","        # PR AUC\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","             pr_auc = average_precision_score(y_test_sample, probs_sample)\n","             metric_distributions[model_name]['PR AUC'].append(pr_auc)\n","        else:\n","             metric_distributions[model_name]['PR AUC'].append(np.nan)\n","\n","\n","        # F1 Score (requires binary predictions)\n","        if len(np.unique(y_test_sample)) > 1: # F1 is undefined for single class\n","             f1 = f1_score(y_test_sample, preds_sample)\n","             metric_distributions[model_name]['F1 Score'].append(f1)\n","        else:\n","             metric_distributions[model_name]['F1 Score'].append(np.nan)\n","\n","\n","        # Store samples for plotting ROC/PR curves with bands\n","        if i < 100: # Store a subset for plotting efficiency\n","             metric_distributions[model_name]['bootstrap_samples_for_plotting'].append((y_test_sample, probs_sample))\n","\n","\n","print(\"Bootstrap resampling complete.\")\n","\n","# Calculate summary statistics (mean, CI) from bootstrap distributions\n","metric_summary: Dict[str, Dict[str, Dict[str, float]]] = {}\n","for model_name, metrics in metric_distributions.items():\n","    metric_summary[model_name] = {}\n","    for metric_name, distribution in metrics.items():\n","        if metric_name == 'bootstrap_samples_for_plotting':\n","            continue # Skip plotting samples\n","\n","        # Filter out NaN values before calculating mean and CI\n","        valid_distribution = np.array(distribution)[~np.isnan(distribution)]\n","\n","        if len(valid_distribution) > 0:\n","            mean = np.mean(valid_distribution)\n","            # Calculate confidence interval using percentile method\n","            lower_ci = np.percentile(valid_distribution, (alpha / 2) * 100)\n","            upper_ci = np.percentile(valid_distribution, 100 - (alpha / 2) * 100)\n","            metric_summary[model_name][metric_name] = {\n","                'mean': mean,\n","                'ci_lower': lower_ci,\n","                'ci_upper': ci_upper,\n","                'distribution': list(valid_distribution) # Store valid distribution for effect size calculation\n","            }\n","        else:\n","            metric_summary[model_name][metric_name] = {\n","                'mean': np.nan,\n","                'ci_lower': np.nan,\n","                'ci_upper': np.nan,\n","                'distribution': []\n","            }\n","\n","print(\"\\nSummary statistics calculated.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Run statistical Test on Ensemble Learners and tell which one performs best on statistical test\n","\n","import pandas as pd\n","import numpy as np\n","from scipy.stats import ttest_rel\n","from statsmodels.stats.contingency_tables import mcnemar\n","from typing import Dict, List, Any\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.base import clone\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import VotingClassifier\n","from xgboost import XGBClassifier\n","\n","# Assuming y_test, averaged_probs, ensemble_predictions, weighted_averaged_probs,\n","# weighted_ensemble_predictions, stacked_predictions_probs, stacked_predictions,\n","# stacked_predictions_probs_xgb, stacked_predictions_xgb,\n","# reconstruction_errors_test, anomaly_predictions,\n","# best_clf, mlp_model, ocsvm, knn, xgb, rf\n","# are available from the preceding code block.\n","\n","# Re-define models dictionary including the original and ensemble models\n","models = {\n","    'LSTM-AE': {'probs': reconstruction_errors_test, 'preds': anomaly_predictions}, # Assuming reconstruction_errors_test is the \"score\" for LSTM-AE\n","    'Logistic Regression': {'probs': best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': best_clf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'MLP': {'probs': mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten() > 0.5).astype(int)},\n","    'One-Class SVM': {'probs': -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten() < 0).astype(int)}, # Using negative scores for AUC/PR AUC, and thresholding at 0 for binary predictions\n","    'KNN': {'probs': knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': knn.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'XGBoost': {'probs': xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': xgb.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'Random Forest': {'probs': rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': rf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'Averaged Ensemble': {'probs': averaged_probs, 'preds': ensemble_predictions},\n","    'Weighted Ensemble': {'probs': weighted_averaged_probs, 'preds': weighted_ensemble_predictions},\n","    'Stacked (LR Meta)': {'probs': stacked_predictions_probs, 'preds': stacked_predictions},\n","    'Stacked (XGB Meta)': {'probs': stacked_predictions_probs_xgb, 'preds': stacked_predictions_xgb},\n","}\n","\n","# Ensure model names are ordered for consistent output\n","model_order = list(models.keys())\n","\n","# Define the ensemble models subset for pairwise comparison among themselves\n","# Assuming 'models' dictionary is already defined and contains ensemble models\n","ensemble_models = {\n","    model_name: data for model_name, data in models.items()\n","    if 'Ensemble' in model_name or 'Stacked' in model_name\n","}\n","\n","# Store statistical test results for ensemble comparisons\n","ensemble_statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n","\n","print(\"\\nPerforming Statistical Testing Among Ensemble Learners (Paired T-test and McNemar only)...\")\n","\n","ensemble_model_names = list(ensemble_models.keys())\n","\n","# Iterate through all unique pairs of ensemble models\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        model1_name = ensemble_model_names[i]\n","        model2_name = ensemble_model_names[j]\n","\n","        model1_data = ensemble_models[model1_name]\n","        model2_data = ensemble_models[model2_name]\n","\n","        # Retrieve bootstrap distributions\n","        model1_roc_auc_dist = np.array(metric_distributions[model1_name]['ROC AUC'], dtype=np.float64)\n","        model1_pr_auc_dist = np.array(metric_distributions[model1_name]['PR AUC'], dtype=np.float64)\n","        model1_f1_dist = np.array(metric_distributions[model1_name]['F1 Score'], dtype=np.float64)\n","\n","        model2_roc_auc_dist = np.array(metric_distributions[model2_name]['ROC AUC'], dtype=np.float64)\n","        model2_pr_auc_dist = np.array(metric_distributions[model2_name]['PR AUC'], dtype=np.float64)\n","        model2_f1_dist = np.array(metric_distributions[model2_name]['F1 Score'], dtype=np.float64)\n","\n","        # Retrieve original binary predictions for McNemar test\n","        model1_preds = model1_data['preds']\n","        model2_preds = model2_data['preds']\n","\n","\n","        comparison_key = f'{model1_name} vs {model2_name}'\n","        ensemble_statistical_test_results[comparison_key] = {}\n","\n","        # Perform Paired T-test on metric distributions (ROC AUC, PR AUC, F1 Score)\n","        for metric_name, dist1, dist2 in zip(\n","            ['ROC AUC', 'PR AUC', 'F1 Score'],\n","            [model1_roc_auc_dist, model1_pr_auc_dist, model1_f1_dist],\n","            [model2_roc_auc_dist, model2_pr_auc_dist, model2_f1_dist]\n","        ):\n","             min_len = min(len(dist1), len(dist2))\n","             if min_len >= 2:\n","                 # Paired t-test (handles NaNs)\n","                 t_stat, p_value_t = ttest_rel(dist1[:min_len], dist2[:min_len], nan_policy='omit')\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = p_value_t\n","             else:\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = np.nan\n","\n","\n","        # McNemar test on binary predictions (requires original predictions)\n","        try:\n","            # Ensure both prediction arrays have the same length as y_test\n","            if len(model1_preds) == len(y_test) and len(model2_preds) == len(y_test):\n","                # Create a contingency table for McNemar's test\n","                # (Model1 correct, Model2 incorrect), (Model1 incorrect, Model2 correct)\n","                n10 = np.sum((model1_preds == y_test) & (model2_preds != y_test))\n","                n01 = np.sum((model1_preds != y_test) & (model2_preds == y_test))\n","\n","                # The statsmodels mcnemar function uses the full 2x2 table\n","                contingency_table_mcnemar = pd.crosstab(model1_preds, model2_preds).reindex(\n","                    pd.MultiIndex.from_product([[0, 1], [0, 1]], names=[f'{model1_name} Pred', f'{model2_name} Pred']), fill_value=0)\n","\n","                table_array_mcnemar = np.array([\n","                    [contingency_table_mcnemar.loc[(0, 0)], contingency_table_mcnemar.loc[(0, 1)]],\n","                    [contingency_table_mcnemar.loc[(1, 0)], contingency_table_mcnemar.loc[(1, 1)]]\n","                ])\n","\n","                # Ensure there are discordant pairs for the test to be meaningful\n","                if n01 + n10 > 0:\n","                    mcnemar_result = mcnemar(table_array_mcnemar, exact=True) # Use exact=False for larger samples\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = mcnemar_result.pvalue\n","                else:\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = 1.0 # Perfect agreement on discordant pairs\n","\n","            else:\n","                 ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan # Length mismatch\n","\n","        except Exception as e:\n","             print(f\"Error performing McNemar test for {comparison_key}: {e}\")\n","             ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan\n","\n","\n","# Format the ensemble comparison results into a DataFrame\n","print(\"\\nStatistical Test Results Among Ensemble Learners (Paired T-test and McNemar only):\")\n","\n","ensemble_comparison_data = []\n","# Define the order of tests for consistent columns\n","test_keys_order = [\n","    'ROC AUC (T-test p)',\n","    'PR AUC (T-test p)',\n","    'F1 Score (T-test p)',\n","    'McNemar p'\n","]\n","\n","for comparison, tests in ensemble_statistical_test_results.items():\n","    row = {'Comparison': comparison}\n","    for test_name_key in test_keys_order:\n","        p_value = tests.get(test_name_key) # Use get to handle missing keys\n","\n","        if p_value is not None and not np.isnan(p_value):\n","            row[test_name_key] = f\"{p_value:.4f}\"\n","            row[f'Significant? ({test_name_key})'] = '*' if p_value < alpha else ''\n","        else:\n","            row[test_name_key] = \"N/A\"\n","            row[f'Significant? ({test_name_key})'] = ''\n","\n","    ensemble_comparison_data.append(row)\n","\n","ensemble_comparison_df = pd.DataFrame(ensemble_comparison_data)\n","\n","# Filter columns to only include those that were actually added\n","valid_ensemble_columns = ['Comparison']\n","for test_name_key in test_keys_order:\n","    if test_name_key in ensemble_comparison_df.columns:\n","        valid_ensemble_columns.append(test_name_key)\n","        valid_ensemble_columns.append(f'Significant? ({test_name_key})')\n","\n","ensemble_comparison_df = ensemble_comparison_df[valid_ensemble_columns]\n","\n","# Display the table\n","print(ensemble_comparison_df.to_string())\n","\n","# --- Visualization of Statistical Differences (Optional, but good) ---\n","\n","# You can visualize the p-values or significance flags.\n","# A heatmap of p-values could be insightful.\n","\n","# Prepare data for heatmap (using T-test p-values)\n","heatmap_data_p_roc = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","heatmap_data_p_pr = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","heatmap_data_p_f1 = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","heatmap_data_p_mcnemar = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","\n","# Fill heatmap dataframes\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        model1_name = ensemble_model_names[i]\n","        model2_name = ensemble_model_names[j]\n","        comparison_key1 = f'{model1_name} vs {model2_name}'\n","        comparison_key2 = f'{model2_name} vs {model1_name}' # Check both directions\n","\n","        comparison_results = None\n","        if comparison_key1 in ensemble_statistical_test_results:\n","             comparison_results = ensemble_statistical_test_results[comparison_key1]\n","        elif comparison_key2 in ensemble_statistical_test_results:\n","            # If key is reversed, the p-values are the same\n","             comparison_results = ensemble_statistical_test_results[comparison_key2]\n","        else:\n","            continue # Skip if no comparison results found\n","\n","        # Use T-test p-value for heatmap\n","        p_roc = comparison_results.get('ROC AUC (T-test p)', np.nan)\n","        p_pr = comparison_results.get('PR AUC (T-test p)', np.nan)\n","        p_f1 = comparison_results.get('F1 Score (T-test p)', np.nan)\n","        p_mcnemar = comparison_results.get('McNemar p', np.nan)\n","\n","\n","        heatmap_data_p_roc.loc[model1_name, model2_name] = p_roc\n","        heatmap_data_p_roc.loc[model2_name, model1_name] = p_roc # Symmetric\n","\n","        heatmap_data_p_pr.loc[model1_name, model2_name] = p_pr\n","        heatmap_data_p_pr.loc[model2_name, model1_name] = p_pr # Symmetric\n","\n","        heatmap_data_p_f1.loc[model1_name, model2_name] = p_f1\n","        heatmap_data_p_f1.loc[model2_name, model1_name] = p_f1 # Symmetric\n","\n","        heatmap_data_p_mcnemar.loc[model1_name, model2_name] = p_mcnemar\n","        heatmap_data_p_mcnemar.loc[model2_name, model1_name] = p_mcnemar # Symmetric\n","\n","\n","# Mask the upper triangle and diagonal since it's symmetric and we only need one half\n","mask = np.triu(np.ones_like(heatmap_data_p_roc, dtype=bool))\n","\n","# Plotting Heatmaps of p-values\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_roc, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise T-test p-values (ROC AUC) Among Ensemble Learners\")\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_pr, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise T-test p-values (PR AUC) Among Ensemble Learners\")\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_f1, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise T-test p-values (F1 Score) Among Ensemble Learners\")\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_mcnemar, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise McNemar Test p-values Among Ensemble Learners\")\n","plt.show()\n","\n","\n","# Interpretation of Heatmaps:\n","# - Small p-values (e.g., < 0.05, colored darker in a reverse viridis cmap) indicate a statistically significant difference in performance metric (for T-tests) or error patterns (for McNemar) between the two models being compared.\n","# - Large p-values indicate no statistically significant difference found at the chosen alpha level.\n","\n","# Combine statistical results with mean performance for a final conclusion\n","\n","print(\"\\nAnalyzing Ensemble Performance and Statistical Differences...\")\n","\n","ensemble_summary_data = []\n","for model_name in ensemble_model_names:\n","    summary_info_roc = metric_summary[model_name].get('ROC AUC', {'mean': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan})\n","    summary_info_pr = metric_summary[model_name].get('PR AUC', {'mean': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan})\n","    summary_info_f1 = metric_summary[model_name].get('F1 Score', {'mean': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan})\n","\n","    row = {\n","        'Model': model_name,\n","        'Mean ROC AUC': summary_info_roc['mean'],\n","        'Mean PR AUC': summary_info_pr['mean'],\n","        'Mean F1 Score': summary_info_f1['mean'],\n","        'ROC AUC CI': f\"({summary_info_roc['ci_lower']:.3f} - {summary_info_roc['ci_upper']:.3f})\" if not np.isnan(summary_info_roc['mean']) else \"N/A\",\n","        'PR AUC CI': f\"({summary_info_pr['ci_lower']:.3f} - {summary_info_pr['upper_ci']:.3f})\" if not np.isnan(summary_info_pr['mean']) else \"N/A\",\n","        'F1 Score CI': f\"({summary_info_f1['ci_lower']:.3f} - {summary_info_f1['ci_upper']:.3f})\" if not np.isnan(summary_info_f1['mean']) else \"N/A\",\n","        'Sig. Better Count (ROC AUC)': 0,\n","        'Sig. Worse Count (ROC AUC)': 0,\n","        'Sig. Better Count (PR AUC)': 0,\n","        'Sig. Worse Count (PR AUC)': 0,\n","        'Sig. Better Count (F1 Score)': 0,\n","        'Sig. Worse Count (F1 Score)': 0,\n","        'Sig. McNemar Count': 0 # Count how many comparisons show significant McNemar\n","    }\n","    ensemble_summary_data.append(row)\n","\n","ensemble_performance_df = pd.DataFrame(ensemble_summary_data)\n","\n","\n","# Update counts based on pairwise statistical tests (using T-test and McNemar)\n","for i in range(len(ensemble_performance_df)):\n","    model1_name = ensemble_performance_df.loc[i, 'Model']\n","    mean_roc1 = ensemble_performance_df.loc[i, 'Mean ROC AUC']\n","    mean_pr1 = ensemble_performance_df.loc[i, 'Mean PR AUC']\n","    mean_f1_1 = ensemble_performance_df.loc[i, 'Mean F1 Score']\n","\n","\n","    for j in range(i + 1, len(ensemble_performance_df)):\n","        model2_name = ensemble_performance_df.loc[j, 'Model']\n","        mean_roc2 = ensemble_performance_df.loc[j, 'Mean ROC AUC']\n","        mean_pr2 = ensemble_performance_df.loc[j, 'Mean PR AUC']\n","        mean_f1_2 = ensemble_performance_df.loc[j, 'Mean F1 Score']\n","\n","        comparison_key1 = f'{model1_name} vs {model2_name}'\n","        comparison_key2 = f'{model2_name} vs {model1_name}'\n","\n","        comparison_results = None\n","        if comparison_key1 in ensemble_statistical_test_results:\n","             comparison_results = ensemble_statistical_test_results[comparison_key1]\n","        elif comparison_key2 in ensemble_statistical_test_results:\n","             comparison_results = ensemble_statistical_test_results[comparison_key2]\n","\n","\n","        if comparison_results:\n","            # Check ROC AUC significance (using T-test)\n","            p_value_roc_t = comparison_results.get('ROC AUC (T-test p)')\n","            if p_value_roc_t is not None and not np.isnan(p_value_roc_t) and p_value_roc_t < alpha:\n","                if mean_roc1 > mean_roc2:\n","                    ensemble_performance_df.loc[i, 'Sig. Better Count (ROC AUC)'] += 1\n","                    ensemble_performance_df.loc[j, 'Sig. Worse Count (ROC AUC)'] += 1\n","                elif mean_roc2 > mean_roc1:\n","                    ensemble_performance_df.loc[j, 'Sig. Better Count (ROC AUC)'] += 1\n","                    ensemble_performance_df.loc[i, 'Sig. Worse Count (ROC AUC)'] += 1\n","\n","            # Check PR AUC significance (using T-test)\n","            p_value_pr_t = comparison_results.get('PR AUC (T-test p)')\n","            if p_value_pr_t is not None and not np.isnan(p_value_pr_t) and p_value_pr_t < alpha:\n","                 if mean_pr1 > mean_pr2:\n","                     ensemble_performance_df.loc[i, 'Sig. Better Count (PR AUC)'] += 1\n","                     ensemble_performance_df.loc[j, 'Sig. Worse Count (PR AUC)'] += 1\n","                 elif mean_pr2 > mean_pr1:\n","                     ensemble_performance_df.loc[j, 'Sig. Better Count (PR AUC)'] += 1\n","                     ensemble_performance_df.loc[i, 'Sig. Worse Count (PR AUC)'] += 1\n","\n","\n","            # Check F1 Score significance (using T-test)\n","            p_value_f1_t = comparison_results.get('F1 Score (T-test p)')\n","            if p_value_f1_t is not None and not np.isnan(p_value_f1_t) and p_value_f1_t < alpha:\n","                 if mean_f1_1 > mean_f1_2:\n","                     ensemble_performance_df.loc[i, 'Sig. Better Count (F1 Score)'] += 1\n","                     ensemble_performance_df.loc[j, 'Sig. Worse Count (F1 Score)'] += 1\n","                 elif mean_f1_2 > mean_f1_1:\n","                     ensemble_performance_df.loc[j, 'Sig. Better Count (F1 Score)'] += 1\n","                     ensemble_performance_df.loc[i, 'Sig. Worse Count (F1 Score)'] += 1\n","\n","\n","            # Check McNemar significance\n","            mcnemar_p = comparison_results.get('McNemar p')\n","            if mcnemar_p is not None and not np.isnan(mcnemar_p) and mcnemar_p < alpha:\n","                ensemble_performance_df.loc[i, 'Sig. McNemar Count'] += 1\n","                ensemble_performance_df.loc[j, 'Sig. McNemar Count'] += 1 # Count for both models in the pair\n","\n","# Sort the DataFrame by Mean ROC AUC (or other preferred metric)\n","ensemble_performance_df_sorted = ensemble_performance_df.sort_values(by='Mean ROC AUC', ascending=False).reset_index(drop=True)\n","\n","print(\"\\nEnsemble Model Performance Summary with Statistical Significance Counts (T-test and McNemar only):\")\n","print(ensemble_performance_df_sorted.to_string())\n","\n","# Final Recommendation based on the sorted table and counts:\n","# Look at the top rows of `ensemble_performance_df_sorted`.\n","# The model that has high mean values for ROC AUC and PR AUC (and F1 if relevant)\n","# and a high 'Sig. Better Count' and a low 'Sig. Worse Count' is likely the best performer.\n","# Consider models that are not significantly worse than any others.\n","\n","# Example Recommendation Logic:\n","best_model_candidate = ensemble_performance_df_sorted.loc[0, 'Model'] # Start with the highest mean ROC AUC\n","print(f\"\\nInitial Best Candidate (based on Mean ROC AUC): {best_model_candidate}\")\n","\n","# Check if this candidate is significantly worse than any other model (based on T-test for ROC AUC/PR AUC/F1)\n","is_significantly_worse = (ensemble_performance_df_sorted.loc[0, 'Sig. Worse Count (ROC AUC)'] > 0) or \\\n","                         (ensemble_performance_df_sorted.loc[0, 'Sig. Worse Count (PR AUC)'] > 0) or \\\n","                         (ensemble_performance_df_sorted.loc[0, 'Sig. Worse Count (F1 Score)'] > 0)\n","\n","\n","if not is_significantly_worse:\n","    print(f\"The initial candidate '{best_model_candidate}' was NOT found to be significantly worse than any other ensemble model (based on T-tests).\")\n","    print(f\"Final Recommendation: '{best_model_candidate}' based on highest mean performance and no statistically significant worse performance (based on T-tests).\")\n","else:\n","    print(f\"The initial candidate '{best_model_candidate}' WAS found to be significantly worse than at least one other ensemble model (based on T-tests).\")\n","    # Need a more sophisticated selection if the top model has significant weaknesses.\n","    # E.g., select the model with the highest mean ROC AUC among those with 0 'Sig. Worse Count'.\n","    candidates_not_sig_worse = ensemble_performance_df_sorted[\n","        (ensemble_performance_df_sorted['Sig. Worse Count (ROC AUC)'] == 0) &\n","        (ensemble_performance_df_sorted['Sig. Worse Count (PR AUC)'] == 0) &\n","        (ensemble_performance_df_sorted['Sig. Worse Count (F1 Score)'] == 0)\n","    ]\n","\n","    if not candidates_not_sig_worse.empty:\n","        best_stable_model = candidates_not_sig_worse.sort_values(by='Mean ROC AUC', ascending=False).iloc[0]\n","        print(f\"Considering models not significantly worse than others (based on T-tests).\")\n","        print(f\"Final Recommendation: '{best_stable_model['Model']}' based on highest mean ROC AUC among stable models (based on T-tests).\")\n","    else:\n","         print(\"All models were found to be significantly worse than at least one other model (based on T-tests).\")\n","         print(\"Recommend reviewing the pairwise comparison table and heatmaps for a nuanced understanding.\")\n","         print(\"The model with the highest overall 'Sig. Better Count' might be a good choice.\")\n","\n","# Save the detailed ensemble performance summary table\n","output_dir_tables = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n","os.makedirs(output_dir_tables, exist_ok=True)\n","ensemble_performance_df_sorted.to_csv(f\"{output_dir_tables}/ensemble_performance_summary_with_sig_counts_ttest_mcnemar.csv\", index=False)\n","\n","# Prepare for LaTeX: Escape special characters\n","latex_ensemble_perf_df = ensemble_performance_df_sorted.copy()\n","latex_ensemble_perf_df.columns = latex_ensemble_perf_df.columns.str.replace('_', '\\\\_')\n","latex_ensemble_perf_df.columns = latex_ensemble_perf_df.columns.str.replace('%', '\\\\%')\n","latex_ensemble_perf_df.columns = latex_ensemble_perf_df.columns.str.replace('(', '{(').str.replace(')', ')}')\n","\n","latex_ensemble_perf_df.to_latex(f\"{output_dir_tables}/ensemble_performance_summary_with_sig_counts_ttest_mcnemar.tex\", index=False, float_format=\"%.3f\", escape=False)\n","\n","print(f\"\\nEnsemble performance summary saved to {output_dir_tables}/ensemble_performance_summary_with_sig_counts_ttest_mcnemar.csv and .tex\")"],"metadata":{"id":"D-lnT6x2XF2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n","from statsmodels.stats.contingency_tables import mcnemar\n","from typing import Dict, List, Any\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, precision_recall_curve\n","import os\n","\n","# Define required variables (example values)\n","n_iterations = 1000  # Number of bootstrap resamples\n","alpha = 0.05         # Significance level for statistical tests (e.g., 0.05)\n","\n","# Re-define models dictionary including the original and ensemble models\n","models = {\n","    'LSTM-AE': {'probs': reconstruction_errors_test, 'preds': anomaly_predictions}, # Assuming reconstruction_errors_test is the \"score\" for LSTM-AE\n","    'Logistic Regression': {'probs': best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': best_clf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'MLP': {'probs': mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten() > 0.5).astype(int)},\n","    'One-Class SVM': {'probs': -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten() < 0).astype(int)}, # Using negative scores for AUC/PR AUC, and thresholding at 0 for binary predictions\n","    'KNN': {'probs': knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': knn.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'XGBoost': {'probs': xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': xgb.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'Random Forest': {'probs': rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': rf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'Averaged Ensemble': {'probs': averaged_probs, 'preds': ensemble_predictions},\n","    'Weighted Ensemble': {'probs': weighted_averaged_probs, 'preds': weighted_ensemble_predictions},\n","    'Stacked (LR Meta)': {'probs': stacked_predictions_probs, 'preds': stacked_predictions},\n","    'Stacked (XGB Meta)': {'probs': stacked_predictions_probs_xgb, 'preds': stacked_predictions_xgb},\n","}\n","\n","# Define the ensemble models subset for pairwise comparison among themselves\n","ensemble_models = {\n","    model_name: data for model_name, data in models.items()\n","    if 'Ensemble' in model_name or 'Stacked' in model_name\n","}\n","\n","# Initialize dictionaries to store bootstrap results\n","metric_distributions: Dict[str, Dict[str, List[float]]] = {\n","    model_name: {'ROC AUC': [], 'PR AUC': [], 'F1 Score': [], 'bootstrap_samples_for_plotting': []}\n","    for model_name in models.keys()\n","}\n","\n","# Use a fixed random state for reproducibility of bootstrap samples themselves\n","bootstrap_rng = np.random.default_rng(seed=42)\n","\n","# Perform bootstrap resampling\n","print(f\"Performing bootstrap resampling ({n_iterations} iterations)...\")\n","\n","for i in range(n_iterations):\n","    # Resample the test set with replacement\n","    bootstrap_indices = bootstrap_rng.choice(len(y_test), size=len(y_test), replace=True)\n","    y_test_sample = y_test[bootstrap_indices]\n","\n","    for model_name, data in models.items():\n","        # Get probabilities and binary predictions for the bootstrap sample\n","        probs_sample = data['probs'][bootstrap_indices]\n","        preds_sample = data['preds'][bootstrap_indices] # Assuming binary predictions are available\n","\n","        # Calculate metrics on the bootstrap sample\n","        # ROC AUC\n","        # Check for multiple classes in true labels and variation in scores/predictions\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","            roc_auc = roc_auc_score(y_test_sample, probs_sample)\n","            metric_distributions[model_name]['ROC AUC'].append(roc_auc)\n","        else:\n","            # Handle cases where resampling results in only one class or constant probabilities\n","            metric_distributions[model_name]['ROC AUC'].append(np.nan) # Append NaN for invalid samples\n","\n","        # PR AUC\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","             pr_auc = average_precision_score(y_test_sample, probs_sample)\n","             metric_distributions[model_name]['PR AUC'].append(pr_auc)\n","        else:\n","             metric_distributions[model_name]['PR AUC'].append(np.nan)\n","\n","\n","        # F1 Score (requires binary predictions)\n","        # Check for multiple classes in true labels and variation in predictions\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(preds_sample)) > 1: # F1 is undefined for single class or constant predictions\n","             f1 = f1_score(y_test_sample, preds_sample)\n","             metric_distributions[model_name]['F1 Score'].append(f1)\n","        else:\n","             metric_distributions[model_name]['F1 Score'].append(np.nan)\n","\n","\n","        # Store samples for plotting ROC/PR curves with bands\n","        # Store a fixed number of samples (e.g., up to 100) for plotting efficiency\n","        if len(metric_distributions[model_name]['bootstrap_samples_for_plotting']) < 100:\n","             metric_distributions[model_name]['bootstrap_samples_for_plotting'].append((y_test_sample, probs_sample))\n","\n","\n","print(\"Bootstrap resampling complete.\")\n","\n","# Calculate summary statistics (mean, CI) from bootstrap distributions\n","metric_summary: Dict[str, Dict[str, Dict[str, Any]]] = {}\n","for model_name, metrics in metric_distributions.items():\n","    metric_summary[model_name] = {}\n","    for metric_name, distribution in metrics.items():\n","        if metric_name == 'bootstrap_samples_for_plotting':\n","            continue # Skip plotting samples\n","\n","        # Filter out NaN values before calculating mean and CI\n","        valid_distribution = np.array(distribution)[~np.isnan(distribution)]\n","\n","        if len(valid_distribution) > 0:\n","            mean = np.mean(valid_distribution)\n","            # Calculate confidence interval using percentile method\n","            # Need at least 2 points for percentiles\n","            if len(valid_distribution) >= 2:\n","                lower_ci = np.percentile(valid_distribution, (alpha / 2) * 100)\n","                upper_ci = np.percentile(valid_distribution, 100 - (alpha / 2) * 100)\n","            else:\n","                lower_ci = np.nan\n","                upper_ci = np.nan\n","\n","            metric_summary[model_name][metric_name] = {\n","                'mean': mean,\n","                'ci_lower': lower_ci,\n","                'ci_upper': upper_ci,\n","                'distribution': list(valid_distribution) # Store valid distribution for effect size calculation\n","            }\n","        else:\n","            metric_summary[model_name][metric_name] = {\n","                'mean': np.nan,\n","                'ci_lower': np.nan,\n","                'ci_upper': np.nan,\n","                'distribution': []\n","            }\n","\n","print(\"\\nSummary statistics calculated.\")\n","\n","# Define the ensemble models subset for pairwise comparison among themselves\n","# Assuming 'models' dictionary is already defined and contains ensemble models\n","ensemble_models = {\n","    model_name: data for model_name, data in models.items()\n","    if 'Ensemble' in model_name or 'Stacked' in model_name\n","}\n","\n","# Store statistical test results for ensemble comparisons\n","ensemble_statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n","\n","print(\"\\nPerforming Statistical Testing Among Ensemble Learners...\")\n","\n","ensemble_model_names = list(ensemble_models.keys())\n","\n","# Iterate through all unique pairs of ensemble models\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        model1_name = ensemble_model_names[i]\n","        model2_name = ensemble_model_names[j]\n","\n","        model1_data = ensemble_models[model1_name]\n","        model2_data = ensemble_models[model2_name]\n","\n","        # Retrieve bootstrap distributions\n","        model1_roc_auc_dist = np.array(metric_distributions[model1_name]['ROC AUC'], dtype=np.float64)\n","        model1_pr_auc_dist = np.array(metric_distributions[model1_name]['PR AUC'], dtype=np.float64)\n","        model1_f1_dist = np.array(metric_distributions[model1_name]['F1 Score'], dtype=np.float64)\n","\n","        model2_roc_auc_dist = np.array(metric_distributions[model2_name]['ROC AUC'], dtype=np.float64)\n","        model2_pr_auc_dist = np.array(metric_distributions[model2_name]['PR AUC'], dtype=np.float64)\n","        model2_f1_dist = np.array(metric_distributions[model2_name]['F1 Score'], dtype=np.float64)\n","\n","        # Retrieve original binary predictions for McNemar test\n","        model1_preds = model1_data['preds']\n","        model2_preds = model2_data['preds']\n","\n","\n","        comparison_key = f'{model1_name} vs {model2_name}'\n","        ensemble_statistical_test_results[comparison_key] = {}\n","\n","        # Perform paired tests on metric distributions (ROC AUC, PR AUC, F1 Score)\n","        for metric_name, dist1, dist2 in zip(\n","            ['ROC AUC', 'PR AUC', 'F1 Score'],\n","            [model1_roc_auc_dist, model1_pr_auc_dist, model1_f1_dist],\n","            [model2_roc_auc_dist, model2_pr_auc_dist, model2_f1_dist]\n","        ):\n","             min_len = min(len(dist1), len(dist2))\n","             if min_len >= 2:\n","                 # Paired t-test (handles NaNs)\n","                 t_stat, p_value_t = ttest_rel(dist1[:min_len], dist2[:min_len], nan_policy='omit')\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = p_value_t\n","\n","                 # Wilcoxon test (filter NaNs, require variation)\n","                 dist1_valid = dist1[:min_len][~np.isnan(dist1[:min_len])]\n","                 dist2_valid = dist2[:min_len][~np.isnan(dist2[:min_len])]\n","                 if len(dist1_valid) >= 2 and len(dist2_valid) >= 2 and len(set(dist1_valid)) > 1 and len(set(dist2_valid)) > 1:\n","                     try:\n","                         w_stat, p_value_w = wilcoxon(dist1_valid, dist2_valid)\n","                         ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = p_value_w\n","                     except ValueError:\n","                         ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                 else:\n","                     ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","\n","                 # Mann-Whitney U test (filter NaNs, require sufficient data)\n","                 if len(dist1_valid) >= 2 and len(dist2_valid) >= 2:\n","                     u_stat, p_value_u = mannwhitneyu(dist1_valid, dist2_valid)\n","                     ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = p_value_u\n","                 else:\n","                      ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n","\n","             else:\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = np.nan\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n","\n","\n","        # McNemar test on binary predictions (requires original predictions)\n","        try:\n","            # Ensure both prediction arrays have the same length as y_test\n","            if len(model1_preds) == len(y_test) and len(model2_preds) == len(y_test):\n","                # Create a contingency table for McNemar's test\n","                # (Model1 correct, Model2 incorrect), (Model1 incorrect, Model2 correct)\n","                n10 = np.sum((model1_preds == y_test) & (model2_preds != y_test))\n","                n01 = np.sum((model1_preds != y_test) & (model2_preds == y_test))\n","\n","                # The statsmodels mcnemar function uses the full 2x2 table\n","                contingency_table_mcnemar = pd.crosstab(model1_preds, model2_preds).reindex(\n","                    pd.MultiIndex.from_product([[0, 1], [0, 1]], names=[f'{model1_name} Pred', f'{model2_name} Pred']), fill_value=0)\n","\n","                table_array_mcnemar = np.array([\n","                    [contingency_table_mcnemar.loc[(0, 0)], contingency_table_mcnemar.loc[(0, 1)]],\n","                    [contingency_table_mcnemar.loc[(1, 0)], contingency_table_mcnemar.loc[(1, 1)]]\n","                ])\n","\n","                # Ensure there are discordant pairs for the test to be meaningful\n","                if n01 + n10 > 0:\n","                    mcnemar_result = mcnemar(table_array_mcnemar, exact=True) # Use exact=False for larger samples\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = mcnemar_result.pvalue\n","                else:\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = 1.0 # Perfect agreement on discordant pairs\n","\n","            else:\n","                 ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan # Length mismatch\n","\n","        except Exception as e:\n","             print(f\"Error performing McNemar test for {comparison_key}: {e}\")\n","             ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan\n","\n","\n","# Format the ensemble comparison results into a DataFrame\n","print(\"\\nStatistical Test Results Among Ensemble Learners:\")\n","\n","ensemble_comparison_data = []\n","# Define the order of tests for consistent columns\n","test_keys_order = [\n","    'ROC AUC (T-test p)', 'ROC AUC (Wilcoxon p)', 'ROC AUC (Mann-Whitney p)',\n","    'PR AUC (T-test p)', 'PR AUC (Wilcoxon p)', 'PR AUC (Mann-Whitney p)',\n","    'F1 Score (T-test p)', 'F1 Score (Wilcoxon p)', 'F1 Score (Mann-Whitney p)',\n","    'McNemar p'\n","]\n","\n","for comparison, tests in ensemble_statistical_test_results.items():\n","    row = {'Comparison': comparison}\n","    for test_name_key in test_keys_order:\n","        p_value = tests.get(test_name_key) # Use get to handle missing keys\n","\n","        if p_value is not None and not np.isnan(p_value):\n","            row[test_name_key] = f\"{p_value:.4f}\"\n","            row[f'Significant? ({test_name_key})'] = '*' if p_value < alpha else ''\n","        else:\n","            row[test_name_key] = \"N/A\"\n","            row[f'Significant? ({test_name_key})'] = ''\n","\n","    ensemble_comparison_data.append(row)\n","\n","ensemble_comparison_df = pd.DataFrame(ensemble_comparison_data)\n","\n","# Filter columns to only include those that were actually added\n","valid_ensemble_columns = ['Comparison']\n","for test_name_key in test_keys_order:\n","    if test_name_key in ensemble_comparison_df.columns:\n","        valid_ensemble_columns.append(test_name_key)\n","        valid_ensemble_columns.append(f'Significant? ({test_name_key})')\n","\n","ensemble_comparison_df = ensemble_comparison_df[valid_ensemble_columns]\n","\n","# Display the table\n","print(ensemble_comparison_df.to_string())\n","\n","# --- Visualization of Statistical Differences (Optional, but good) ---\n","\n","# You can visualize the p-values or significance flags.\n","# A heatmap of p-values could be insightful.\n","\n","# Prepare data for heatmap\n","heatmap_data_p_roc = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","heatmap_data_p_pr = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","heatmap_data_p_f1 = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","heatmap_data_p_mcnemar = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","\n","# Fill heatmap dataframes\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        model1_name = ensemble_model_names[i]\n","        model2_name = ensemble_model_names[j]\n","        comparison_key1 = f'{model1_name} vs {model2_name}'\n","        comparison_key2 = f'{model2_name} vs {model1_name}' # Check both directions\n","\n","        if comparison_key1 in ensemble_statistical_test_results:\n","             comparison_results = ensemble_statistical_test_results[comparison_key1]\n","        elif comparison_key2 in ensemble_statistical_test_results:\n","            # If key is reversed, the p-values are the same\n","             comparison_results = ensemble_statistical_test_results[comparison_key2]\n","        else:\n","            continue # Skip if no comparison results found\n","\n","        # Use T-test p-value for heatmap (can choose Wilcoxon or Mann-Whitney)\n","        p_roc = comparison_results.get('ROC AUC (T-test p)', np.nan)\n","        p_pr = comparison_results.get('PR AUC (T-test p)', np.nan)\n","        p_f1 = comparison_results.get('F1 Score (T-test p)', np.nan)\n","        p_mcnemar = comparison_results.get('McNemar p', np.nan)\n","\n","\n","        heatmap_data_p_roc.loc[model1_name, model2_name] = p_roc\n","        heatmap_data_p_roc.loc[model2_name, model1_name] = p_roc # Symmetric\n","\n","        heatmap_data_p_pr.loc[model1_name, model2_name] = p_pr\n","        heatmap_data_p_pr.loc[model2_name, model1_name] = p_pr # Symmetric\n","\n","        heatmap_data_p_f1.loc[model1_name, model2_name] = p_f1\n","        heatmap_data_p_f1.loc[model2_name, model1_name] = p_f1 # Symmetric\n","\n","        heatmap_data_p_mcnemar.loc[model1_name, model2_name] = p_mcnemar\n","        heatmap_data_p_mcnemar.loc[model2_name, model1_name] = p_mcnemar # Symmetric\n","\n","\n","# Mask the upper triangle and diagonal since it's symmetric and we only need one half\n","mask = np.triu(np.ones_like(heatmap_data_p_roc, dtype=bool))\n","\n","# Plotting Heatmaps of p-values\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_roc, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise T-test p-values (ROC AUC) Among Ensemble Learners\")\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_pr, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise T-test p-values (PR AUC) Among Ensemble Learners\")\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_f1, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise T-test p-values (F1 Score) Among Ensemble Learners\")\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_p_mcnemar, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","plt.title(\"Pairwise McNemar Test p-values Among Ensemble Learners\")\n","plt.show()\n","\n","\n","# Interpretation of Heatmaps:\n","# - Small p-values (e.g., < 0.05, colored darker in a reverse viridis cmap) indicate a statistically significant difference in performance metric (for T-tests) or error patterns (for McNemar) between the two models being compared.\n","# - Large p-values indicate no statistically significant difference found at the chosen alpha level.\n","\n","# Combine statistical results with mean performance for a final conclusion\n","\n","print(\"\\nAnalyzing Ensemble Performance and Statistical Differences...\")\n","\n","ensemble_summary_data = []\n","for model_name in ensemble_model_names:\n","    summary_info_roc = metric_summary[model_name].get('ROC AUC', {'mean': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan})\n","    summary_info_pr = metric_summary[model_name].get('PR AUC', {'mean': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan})\n","    summary_info_f1 = metric_summary[model_name].get('F1 Score', {'mean': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan})\n","\n","    row = {\n","        'Model': model_name,\n","        'Mean ROC AUC': summary_info_roc['mean'],\n","        'Mean PR AUC': summary_info_pr['mean'],\n","        'Mean F1 Score': summary_info_f1['mean'],\n","        'ROC AUC CI': f\"({summary_info_roc['ci_lower']:.3f} - {summary_info_roc['ci_upper']:.3f})\" if not np.isnan(summary_info_roc['mean']) else \"N/A\",\n","        'PR AUC CI': f\"({summary_info_pr['ci_lower']:.3f} - {summary_info_pr['ci_upper']:.3f})\" if not np.isnan(summary_info_pr['mean']) else \"N/A\",\n","        'F1 Score CI': f\"({summary_info_f1['ci_lower']:.3f} - {summary_info_f1['ci_upper']:.3f})\" if not np.isnan(summary_info_f1['mean']) else \"N/A\",\n","        'Sig. Better Count (ROC AUC)': 0,\n","        'Sig. Worse Count (ROC AUC)': 0,\n","        'Sig. Better Count (PR AUC)': 0,\n","        'Sig. Worse Count (PR AUC)': 0,\n","        'Sig. Better Count (F1 Score)': 0,\n","        'Sig. Worse Count (F1 Score)': 0,\n","        'Sig. McNemar Count': 0 # Count how many comparisons show significant McNemar\n","    }\n","    ensemble_summary_data.append(row)\n","\n","ensemble_performance_df = pd.DataFrame(ensemble_summary_data)\n","\n","\n","# Update counts based on pairwise statistical tests (using T-test and McNemar)\n","for i in range(len(ensemble_performance_df)):\n","    model1_name = ensemble_performance_df.loc[i, 'Model']\n","    mean_roc1 = ensemble_performance_df.loc[i, 'Mean ROC AUC']\n","    mean_pr1 = ensemble_performance_df.loc[i, 'Mean PR AUC']\n","    mean_f1_1 = ensemble_performance_df.loc[i, 'Mean F1 Score']\n","\n","\n","    for j in range(i + 1, len(ensemble_performance_df)):\n","        model2_name = ensemble_performance_df.loc[j, 'Model']\n","        mean_roc2 = ensemble_performance_df.loc[j, 'Mean ROC AUC']\n","        mean_pr2 = ensemble_performance_df.loc[j, 'Mean PR AUC']\n","        mean_f1_2 = ensemble_performance_df.loc[j, 'Mean F1 Score']\n","\n","        comparison_key1 = f'{model1_name} vs {model2_name}'\n","        comparison_key2 = f'{model2_name} vs {model1_name}'\n","\n","        comparison_results = None\n","        if comparison_key1 in ensemble_statistical_test_results:\n","             comparison_results = ensemble_statistical_test_results[comparison_key1]\n","        elif comparison_key2 in ensemble_statistical_test_results:\n","             comparison_results = ensemble_statistical_test_results[comparison_key2]\n","\n","\n","        if comparison_results:\n","            # Check ROC AUC significance (using T-test)\n","            p_value_roc_t = comparison_results.get('ROC AUC (T-test p)')\n","            if p_value_roc_t is not None and not np.isnan(p_value_roc_t) and p_value_roc_t < alpha:\n","                if mean_roc1 > mean_roc2:\n","                    ensemble_performance_df.loc[i, 'Sig. Better Count (ROC AUC)'] += 1\n","                    ensemble_performance_df.loc[j, 'Sig. Worse Count (ROC AUC)'] += 1\n","                elif mean_roc2 > mean_roc1:\n","                    ensemble_performance_df.loc[j, 'Sig. Better Count (ROC AUC)'] += 1\n","                    ensemble_performance_df.loc[i, 'Sig. Worse Count (ROC AUC)'] += 1\n","\n","            # Check PR AUC significance (using T-test)\n","            p_value_pr_t = comparison_results.get('PR AUC (T-test p)')\n","            if p_value_pr_t is not None and not np.isnan(p_value_pr_t) and p_value_pr_t < alpha:\n","                 if mean_pr1 > mean_pr2:\n","                     ensemble_performance_df.loc[i, 'Sig. Better Count (PR AUC)'] += 1\n","                     ensemble_performance_df.loc[j, 'Sig. Worse Count (PR AUC)'] += 1\n","                 elif mean_pr2 > mean_pr1:\n","                     ensemble_performance_df.loc[j, 'Sig. Better Count (PR AUC)'] += 1\n","                     ensemble_performance_df.loc[i, 'Sig. Worse Count (PR AUC)'] += 1\n","\n","\n","            # Check F1 Score significance (using T-test)\n","            p_value_f1_t = comparison_results.get('F1 Score (T-test p)')\n","            if p_value_f1_t is not None and not np.isnan(p_value_f1_t) and p_value_f1_t < alpha:\n","                 if mean_f1_1 > mean_f1_2:\n","                     ensemble_performance_df.loc[i, 'Sig. Better Count (F1 Score)'] += 1\n","                     ensemble_performance_df.loc[j, 'Sig. Worse Count (F1 Score)'] += 1\n","                 elif mean_f1_2 > mean_f1_1:\n","                     ensemble_performance_df.loc[j, 'Sig. Better Count (F1 Score)'] += 1\n","                     ensemble_performance_df.loc[i, 'Sig. Worse Count (F1 Score)'] += 1\n","\n","\n","            # Check McNemar significance\n","            mcnemar_p = comparison_results.get('McNemar p')\n","            if mcnemar_p is not None and not np.isnan(mcnemar_p) and mcnemar_p < alpha:\n","                ensemble_performance_df.loc[i, 'Sig. McNemar Count'] += 1\n","                ensemble_performance_df.loc[j, 'Sig. McNemar Count'] += 1 # Count for both models in the pair\n","\n","# Sort the DataFrame by Mean ROC AUC (or other preferred metric)\n","ensemble_performance_df_sorted = ensemble_performance_df.sort_values(by='Mean ROC AUC', ascending=False).reset_index(drop=True)\n","\n","print(\"\\nEnsemble Model Performance Summary with Statistical Significance Counts (T-test and McNemar only):\")\n","print(ensemble_performance_df_sorted.to_string())\n","\n","# Final Recommendation based on the sorted table and counts:\n","# Look at the top rows of `ensemble_performance_df_sorted`.\n","# The model that has high mean values for ROC AUC and PR AUC (and F1 if relevant)\n","# and a high 'Sig. Better Count' and a low 'Sig. Worse Count' is likely the best performer.\n","# Consider models that are not significantly worse than any others.\n","\n","# Example Recommendation Logic:\n","best_model_candidate = ensemble_performance_df_sorted.loc[0, 'Model'] # Start with the highest mean ROC AUC\n","print(f\"\\nInitial Best Candidate (based on Mean ROC AUC): {best_model_candidate}\")\n","\n","# Check if this candidate is significantly worse than any other model (based on T-test for ROC AUC/PR AUC/F1)\n","is_significantly_worse = (ensemble_performance_df_sorted.loc[0, 'Sig. Worse Count (ROC AUC)'] > 0) or \\\n","                         (ensemble_performance_df_sorted.loc[0, 'Sig. Worse Count (PR AUC)'] > 0) or \\\n","                         (ensemble_performance_df_sorted.loc[0, 'Sig. Worse Count (F1 Score)'] > 0)\n","\n","\n","if not is_significantly_worse:\n","    print(f\"The initial candidate '{best_model_candidate}' was NOT found to be significantly worse than any other ensemble model (based on T-tests).\")\n","    print(f\"Final Recommendation: '{best_model_candidate}' based on highest mean performance and no statistically significant worse performance (based on T-tests).\")\n","else:\n","    print(f\"The initial candidate '{best_model_candidate}' WAS found to be significantly worse than at least one other ensemble model (based on T-tests).\")\n","    # Need a more sophisticated selection if the top model has significant weaknesses.\n","    # E.g., select the model with the highest mean ROC AUC among those with 0 'Sig. Worse Count'.\n","    candidates_not_sig_worse = ensemble_performance_df_sorted[\n","        (ensemble_performance_df_sorted['Sig. Worse Count (ROC AUC)'] == 0) &\n","        (ensemble_performance_df_sorted['Sig. Worse Count (PR AUC)'] == 0) &\n","        (ensemble_performance_df_sorted['Sig. Worse Count (F1 Score)'] == 0)\n","    ]\n","\n","    if not candidates_not_sig_worse.empty:\n","        best_stable_model = candidates_not_sig_worse.sort_values(by='Mean ROC AUC', ascending=False).iloc[0]\n","        print(f\"Considering models not significantly worse than others (based on T-tests).\")\n","        print(f\"Final Recommendation: '{best_stable_model['Model']}' based on highest mean ROC AUC among stable models (based on T-tests).\")\n","    else:\n","         print(\"All models were found to be significantly worse than at least one other model (based on T-tests).\")\n","         print(\"Recommend reviewing the pairwise comparison table and heatmaps for a nuanced understanding.\")\n","         print(\"The model with the highest overall 'Sig. Better Count' might be a good choice.\")\n","\n","# Save the detailed ensemble performance summary table\n","output_dir_tables = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n","os.makedirs(output_dir_tables, exist_ok=True)\n","ensemble_performance_df_sorted.to_csv(f\"{output_dir_tables}/ensemble_performance_summary_with_sig_counts_ttest_mcnemar.csv\", index=False)\n","\n","# Prepare for LaTeX: Escape special characters\n","latex_ensemble_perf_df = ensemble_performance_df_sorted.copy()\n","latex_ensemble_perf_df.columns = latex_ensemble_perf_df.columns.str.replace('_', '\\\\_')\n","latex_ensemble_perf_df.columns = latex_ensemble_perf_df.columns.str.replace('%', '\\\\%')\n","latex_ensemble_perf_df.columns = latex_ensemble_perf_df.columns.str.replace('(', '{(', regex=False).str.replace(')', ')}')\n","\n","latex_ensemble_perf_df.to_latex(f\"{output_dir_tables}/ensemble_performance_summary_with_sig_counts_ttest_mcnemar.tex\", index=False, float_format=\"%.3f\", escape=False)\n","\n","print(f\"\\nEnsemble performance summary saved to {output_dir_tables}/ensemble_performance_summary_with_sig_counts_ttest_mcnemar.csv and .tex\")"],"metadata":{"id":"Lpcctr7tY_Is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define required variables (example values)\n","n_iterations = 1000  # Number of bootstrap resamples\n","alpha = 0.05         # Significance level for statistical tests (e.g., 0.05)\n","\n","# Ensure the following are already defined in your environment:\n","# models: Dict[str, Dict[str, np.ndarray]]  # Each model with 'probs' and 'preds'\n","# y_test: np.ndarray                         # Ground truth labels for test set\n"],"metadata":{"id":"fpwH_em4lMhn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define significance level and number of bootstrap iterations\n","n_iterations = 1000\n","alpha = 0.05\n","\n","# Example: Dummy check to ensure required variables exist\n","assert 'models' in globals(), \"Define 'models' dictionary with keys: 'probs', 'preds'\"\n","assert 'y_test' in globals(), \"Define 'y_test' numpy array\"\n"],"metadata":{"id":"WRCdF4j6mC_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import ttest_rel\n","from typing import Dict, List, Any\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","# --- Assume the following variables are already defined ---\n","# models, y_test, n_iterations, alpha (e.g., 0.05)\n","\n","# Step 1: Bootstrap Metric Distributions\n","metric_distributions: Dict[str, Dict[str, List[float]]] = {\n","    model_name: {'ROC AUC': [], 'PR AUC': [], 'F1 Score': []}\n","    for model_name in models.keys()\n","}\n","\n","bootstrap_rng = np.random.default_rng(seed=42)\n","print(f\"Performing bootstrap resampling ({n_iterations} iterations)...\")\n","\n","for i in range(n_iterations):\n","    indices = bootstrap_rng.choice(len(y_test), size=len(y_test), replace=True)\n","    y_sample = y_test[indices]\n","\n","    for model_name, data in models.items():\n","        probs = data['probs'][indices]\n","        preds = data['preds'][indices]\n","\n","        # ROC AUC\n","        if len(np.unique(y_sample)) > 1 and len(np.unique(probs)) > 1:\n","            metric_distributions[model_name]['ROC AUC'].append(roc_auc_score(y_sample, probs))\n","        else:\n","            metric_distributions[model_name]['ROC AUC'].append(np.nan)\n","\n","        # PR AUC\n","        if len(np.unique(y_sample)) > 1 and len(np.unique(probs)) > 1:\n","            metric_distributions[model_name]['PR AUC'].append(average_precision_score(y_sample, probs))\n","        else:\n","            metric_distributions[model_name]['PR AUC'].append(np.nan)\n","\n","        # F1 Score\n","        if len(np.unique(y_sample)) > 1 and len(np.unique(preds)) > 1:\n","            metric_distributions[model_name]['F1 Score'].append(f1_score(y_sample, preds))\n","        else:\n","            metric_distributions[model_name]['F1 Score'].append(np.nan)\n","\n","print(\"Bootstrap resampling complete.\")\n","\n","# Step 2: Filter Ensemble Models\n","ensemble_models = {\n","    name: data for name, data in models.items()\n","    if 'Ensemble' in name or 'Stacked' in name\n","}\n","ensemble_model_names = list(ensemble_models.keys())\n","\n","# Step 3: Perform Paired T-Tests\n","print(\"\\nPerforming Paired T-Tests Among Ensemble Learners...\")\n","ensemble_statistical_test_results: Dict[str, Dict[str, float]] = {}\n","\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        m1, m2 = ensemble_model_names[i], ensemble_model_names[j]\n","        key = f\"{m1} vs {m2}\"\n","        ensemble_statistical_test_results[key] = {}\n","\n","        for metric in ['ROC AUC', 'PR AUC', 'F1 Score']:\n","            dist1 = np.array(metric_distributions[m1][metric], dtype=np.float64)\n","            dist2 = np.array(metric_distributions[m2][metric], dtype=np.float64)\n","            min_len = min(len(dist1), len(dist2))\n","\n","            if min_len >= 2:\n","                t_stat, p_val = ttest_rel(dist1[:min_len], dist2[:min_len], nan_policy='omit')\n","                ensemble_statistical_test_results[key][f'{metric} (T-test p)'] = p_val\n","            else:\n","                ensemble_statistical_test_results[key][f'{metric} (T-test p)'] = np.nan\n","\n","# Step 4: Create Results Table\n","print(\"\\nT-test Results Among Ensemble Learners:\")\n","ensemble_comparison_data = []\n","for comp, tests in ensemble_statistical_test_results.items():\n","    row = {'Comparison': comp}\n","    for metric_key, p_val in tests.items():\n","        row[metric_key] = f\"{p_val:.4f}\" if not np.isnan(p_val) else \"N/A\"\n","        row[f'Significant? ({metric_key})'] = '*' if not np.isnan(p_val) and p_val < alpha else ''\n","    ensemble_comparison_data.append(row)\n","\n","ensemble_comparison_df = pd.DataFrame(ensemble_comparison_data)\n","print(ensemble_comparison_df.to_string())\n","\n","# Step 5: Heatmap Visualization of Paired T-test p-values\n","print(\"\\nVisualizing Paired T-test p-values...\")\n","for metric in ['ROC AUC', 'PR AUC', 'F1 Score']:\n","    df_p = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","\n","    for i in range(len(ensemble_model_names)):\n","        for j in range(i + 1, len(ensemble_model_names)):\n","            m1, m2 = ensemble_model_names[i], ensemble_model_names[j]\n","            key1 = f\"{m1} vs {m2}\"\n","            key2 = f\"{m2} vs {m1}\"\n","\n","            p_val = ensemble_statistical_test_results.get(key1, {}).get(f'{metric} (T-test p)',\n","                     ensemble_statistical_test_results.get(key2, {}).get(f'{metric} (T-test p)', np.nan))\n","\n","            df_p.loc[m1, m2] = p_val\n","            df_p.loc[m2, m1] = p_val\n","\n","    # Mask upper triangle\n","    mask = np.triu(np.ones_like(df_p, dtype=bool))\n","\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(df_p, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","    plt.title(f\"Pairwise T-test p-values ({metric}) Among Ensemble Learners\")\n","    plt.show()\n","\n","# Optional: Save CSV and LaTeX\n","output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","ensemble_comparison_df.to_csv(f\"{output_dir}/ensemble_ttest_comparison.csv\", index=False)\n","\n","latex_df = ensemble_comparison_df.copy()\n","latex_df.columns = latex_df.columns.str.replace('_', '\\\\_').str.replace('%', '\\\\%')\n","latex_df.columns = latex_df.columns.str.replace('(', '{(').str.replace(')', ')}')\n","\n","latex_df.to_latex(f\"{output_dir}/ensemble_ttest_comparison.tex\", index=False, float_format=\"%.4f\", escape=False)\n","\n","print(f\"\\nSaved: {output_dir}/ensemble_ttest_comparison.csv and .tex\")\n"],"metadata":{"id":"sm12Lc54k6mH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import ttest_rel\n","from typing import Dict, List, Any\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","\n","# -----------------------------------\n","# --- Required Inputs (Define First) ---\n","# -----------------------------------\n","\n","n_iterations = 1000  # Number of bootstrap iterations\n","alpha = 0.05         # Significance level (e.g., 0.05)\n","\n","# Ensure `models` is a dictionary:\n","# models = {'ModelA': {'probs': ..., 'preds': ...}, ...}\n","# y_test = ground truth labels for test set (numpy array)\n","\n","# -----------------------------------\n","# --- Step 1: Bootstrap Metric Calculation ---\n","# -----------------------------------\n","\n","metric_distributions: Dict[str, Dict[str, List[float]]] = {\n","    model_name: {'ROC AUC': [], 'PR AUC': [], 'F1 Score': []}\n","    for model_name in models.keys()\n","}\n","\n","bootstrap_rng = np.random.default_rng(seed=42)\n","print(f\"Performing bootstrap resampling ({n_iterations} iterations)...\")\n","\n","for _ in range(n_iterations):\n","    indices = bootstrap_rng.choice(len(y_test), size=len(y_test), replace=True)\n","    y_sample = y_test[indices]\n","\n","    for model_name, data in models.items():\n","        probs = data['probs'][indices]\n","        preds = data['preds'][indices]\n","\n","        # ROC AUC\n","        if len(np.unique(y_sample)) > 1 and len(np.unique(probs)) > 1:\n","            metric_distributions[model_name]['ROC AUC'].append(roc_auc_score(y_sample, probs))\n","        else:\n","            metric_distributions[model_name]['ROC AUC'].append(np.nan)\n","\n","        # PR AUC\n","        if len(np.unique(y_sample)) > 1 and len(np.unique(probs)) > 1:\n","            metric_distributions[model_name]['PR AUC'].append(average_precision_score(y_sample, probs))\n","        else:\n","            metric_distributions[model_name]['PR AUC'].append(np.nan)\n","\n","        # F1 Score\n","        if len(np.unique(y_sample)) > 1 and len(np.unique(preds)) > 1:\n","            metric_distributions[model_name]['F1 Score'].append(f1_score(y_sample, preds))\n","        else:\n","            metric_distributions[model_name]['F1 Score'].append(np.nan)\n","\n","print(\"Bootstrap resampling complete.\")\n","\n","# -----------------------------------\n","# --- Step 2: Summary Statistics (Mean + CI) ---\n","# -----------------------------------\n","\n","metric_summary = {}\n","for model_name, metrics in metric_distributions.items():\n","    metric_summary[model_name] = {}\n","    for metric_name, distribution in metrics.items():\n","        dist = np.array(distribution)\n","        dist = dist[~np.isnan(dist)]\n","\n","        if len(dist) >= 2:\n","            mean = np.mean(dist)\n","            lower_ci = np.percentile(dist, alpha / 2 * 100)\n","            upper_ci = np.percentile(dist, (1 - alpha / 2) * 100)\n","        else:\n","            mean = lower_ci = upper_ci = np.nan\n","\n","        metric_summary[model_name][metric_name] = {\n","            'mean': mean,\n","            'ci_lower': lower_ci,\n","            'ci_upper': upper_ci,\n","            'distribution': dist\n","        }\n","\n","# -----------------------------------\n","# --- Step 3: Paired T-Tests (Only Ensemble Models) ---\n","# -----------------------------------\n","\n","ensemble_models = {\n","    name: data for name, data in models.items()\n","    if 'Ensemble' in name or 'Stacked' in name\n","}\n","ensemble_model_names = list(ensemble_models.keys())\n","\n","ensemble_statistical_test_results = {}\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        m1, m2 = ensemble_model_names[i], ensemble_model_names[j]\n","        key = f\"{m1} vs {m2}\"\n","        ensemble_statistical_test_results[key] = {}\n","\n","        for metric in ['ROC AUC', 'PR AUC', 'F1 Score']:\n","            dist1 = metric_summary[m1][metric]['distribution']\n","            dist2 = metric_summary[m2][metric]['distribution']\n","            min_len = min(len(dist1), len(dist2))\n","\n","            if min_len >= 2:\n","                t_stat, p_val = ttest_rel(dist1[:min_len], dist2[:min_len], nan_policy='omit')\n","                ensemble_statistical_test_results[key][f'{metric} (T-test p)'] = p_val\n","            else:\n","                ensemble_statistical_test_results[key][f'{metric} (T-test p)'] = np.nan\n","\n","# -----------------------------------\n","# --- Step 4: Compile Table with Summary + T-tests ---\n","# -----------------------------------\n","\n","summary_rows = []\n","\n","for model in ensemble_model_names:\n","    row = {\n","        'Model': model,\n","        'Mean ROC AUC': f\"{metric_summary[model]['ROC AUC']['mean']:.4f}\",\n","        'ROC AUC CI': f\"({metric_summary[model]['ROC AUC']['ci_lower']:.4f} - {metric_summary[model]['ROC AUC']['ci_upper']:.4f})\",\n","        'Mean PR AUC': f\"{metric_summary[model]['PR AUC']['mean']:.4f}\",\n","        'PR AUC CI': f\"({metric_summary[model]['PR AUC']['ci_lower']:.4f} - {metric_summary[model]['PR AUC']['ci_upper']:.4f})\",\n","        'Mean F1 Score': f\"{metric_summary[model]['F1 Score']['mean']:.4f}\",\n","        'F1 Score CI': f\"({metric_summary[model]['F1 Score']['ci_lower']:.4f} - {metric_summary[model]['F1 Score']['ci_upper']:.4f})\",\n","    }\n","    summary_rows.append(row)\n","\n","summary_df = pd.DataFrame(summary_rows)\n","\n","print(\"\\nEnsemble Learner Performance Summary:\")\n","print(summary_df.to_string(index=False))\n","\n","# -----------------------------------\n","# --- Step 5: Heatmap of p-values ---\n","# -----------------------------------\n","\n","for metric in ['ROC AUC', 'PR AUC', 'F1 Score']:\n","    heatmap_df = pd.DataFrame(index=ensemble_model_names, columns=ensemble_model_names, dtype=float)\n","\n","    for i in range(len(ensemble_model_names)):\n","        for j in range(i + 1, len(ensemble_model_names)):\n","            m1, m2 = ensemble_model_names[i], ensemble_model_names[j]\n","            key1 = f\"{m1} vs {m2}\"\n","            key2 = f\"{m2} vs {m1}\"\n","\n","            p_val = ensemble_statistical_test_results.get(key1, {}).get(f'{metric} (T-test p)',\n","                     ensemble_statistical_test_results.get(key2, {}).get(f'{metric} (T-test p)', np.nan))\n","\n","            heatmap_df.loc[m1, m2] = p_val\n","            heatmap_df.loc[m2, m1] = p_val\n","\n","    # Mask upper triangle\n","    mask = np.triu(np.ones_like(heatmap_df, dtype=bool))\n","\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(heatmap_df, annot=True, cmap=\"viridis_r\", fmt=\".3f\", mask=mask, linewidths=.5)\n","    plt.title(f\"Paired T-test p-values for {metric}\")\n","    plt.show()\n","\n","# -----------------------------------\n","# --- Optional Save ---\n","# -----------------------------------\n","\n","output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","summary_df.to_csv(f\"{output_dir}/ensemble_performance_summary.csv\", index=False)\n","\n","# Save LaTeX version\n","latex_df = summary_df.copy()\n","latex_df.columns = latex_df.columns.str.replace('_', '\\\\_').str.replace('%', '\\\\%')\n","latex_df.columns = latex_df.columns.str.replace('(', '{(').str.replace(')', ')}')\n","\n","latex_df.to_latex(f\"{output_dir}/ensemble_performance_summary.tex\", index=False, float_format=\"%.4f\", escape=False)\n","\n","print(f\"\\nSaved to: {output_dir}/ensemble_performance_summary.csv and .tex\")\n"],"metadata":{"id":"qaUEoeu8m5JV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy.stats import ttest_rel\n","import os\n","\n","# --- Configuration ---\n","alpha = 0.05  # Significance level\n","output_dir = \"./results\"  # Change path as needed\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# --- Assume metric_distributions is precomputed and loaded ---\n","# Format: {model_name: {'ROC AUC': [...], 'PR AUC': [...], 'F1 Score': [...]}}\n","\n","# Dummy data for illustration only (Replace with actual bootstrap results)\n","metric_distributions = {\n","    'EnsembleA': {\n","        'ROC AUC': np.random.normal(0.85, 0.01, 100),\n","        'PR AUC': np.random.normal(0.80, 0.02, 100),\n","        'F1 Score': np.random.normal(0.78, 0.03, 100),\n","    },\n","    'EnsembleB': {\n","        'ROC AUC': np.random.normal(0.83, 0.01, 100),\n","        'PR AUC': np.random.normal(0.77, 0.02, 100),\n","        'F1 Score': np.random.normal(0.76, 0.03, 100),\n","    },\n","    'EnsembleC': {\n","        'ROC AUC': np.random.normal(0.84, 0.015, 100),\n","        'PR AUC': np.random.normal(0.78, 0.015, 100),\n","        'F1 Score': np.random.normal(0.77, 0.02, 100),\n","    },\n","}\n","\n","# --- Step 1: Generate Summary Table (Mean and CI) ---\n","summary_stats = []\n","for model, metrics in metric_distributions.items():\n","    row = {'Model': model}\n","    for metric, values in metrics.items():\n","        values = np.array(values)\n","        values = values[~np.isnan(values)]\n","        mean = np.mean(values)\n","        ci_lower = np.percentile(values, alpha / 2 * 100)\n","        ci_upper = np.percentile(values, 100 - alpha / 2 * 100)\n","        row[f'Mean {metric}'] = mean\n","        row[f'{metric} CI'] = f\"({ci_lower:.3f} - {ci_upper:.3f})\"\n","    summary_stats.append(row)\n","\n","summary_df = pd.DataFrame(summary_stats)\n","print(\"\\n--- Ensemble Summary Statistics ---\")\n","print(summary_df.to_string(index=False))\n","summary_df.to_csv(f\"{output_dir}/ensemble_summary_statistics.csv\", index=False)\n","\n","# --- Step 2: Perform Pairwise Paired T-Tests ---\n","model_names = list(metric_distributions.keys())\n","comparison_results = []\n","\n","for i in range(len(model_names)):\n","    for j in range(i + 1, len(model_names)):\n","        model1 = model_names[i]\n","        model2 = model_names[j]\n","        row = {'Comparison': f\"{model1} vs {model2}\"}\n","\n","        for metric in ['ROC AUC', 'PR AUC', 'F1 Score']:\n","            dist1 = np.array(metric_distributions[model1][metric])\n","            dist2 = np.array(metric_distributions[model2][metric])\n","\n","            min_len = min(len(dist1), len(dist2))\n","            dist1 = dist1[:min_len]\n","            dist2 = dist2[:min_len]\n","\n","            t_stat, p_val = ttest_rel(dist1, dist2, nan_policy='omit')\n","            row[f\"{metric} (T-test p)\"] = f\"{p_val:.4f}\" if not np.isnan(p_val) else \"N/A\"\n","            row[f\"Significant ({metric})\"] = \"*\" if not np.isnan(p_val) and p_val < alpha else \"\"\n","\n","        comparison_results.append(row)\n","\n","pval_df = pd.DataFrame(comparison_results)\n","\n","print(\"\\n--- Pairwise Paired T-Test P-Values ---\")\n","print(pval_df.to_string(index=False))\n","\n","# --- Step 3: Save Results ---\n","pval_df.to_csv(f\"{output_dir}/ensemble_pairwise_ttest_pvalues.csv\", index=False)\n","pval_df.to_latex(f\"{output_dir}/ensemble_pairwise_ttest_pvalues.tex\", index=False, float_format=\"%.4f\", escape=False)\n","\n","print(f\"\\nResults saved to {output_dir}\")\n"],"metadata":{"id":"Gw7JG0Mon7Du"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GkDKWI932jDA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6ac5a1a"},"source":["# Task\n","Rerun the bootstrap resampling and pairwise statistical tests (T-test, Wilcoxon, Mann-Whitney, McNemar) for ensemble models, apply Holm-Bonferroni correction to all generated p-values, and update the ensemble comparison summary tables with corrected p-values and significance flags to report the statistically significant results."]},{"cell_type":"markdown","metadata":{"id":"73bc2d2a"},"source":["## Rerun Bootstrap and Ensemble Statistical Tests\n","\n","### Subtask:\n","Rerun the bootstrap resampling and pairwise statistical tests (T-test, Wilcoxon, Mann-Whitney, McNemar) among ensemble models to ensure all p-values are correctly generated and stored.\n"]},{"cell_type":"markdown","metadata":{"id":"d3f3ce92"},"source":["**Reasoning**:\n","First, I will ensure all necessary variables, including the `models` dictionary with predictions and probabilities, `y_test`, and `n_iterations`, are properly defined before proceeding with the bootstrap resampling. This step is crucial for accurate statistical analysis of ensemble models.\n","\n"]},{"cell_type":"code","metadata":{"id":"40d2e7a5"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","from typing import Dict, List, Any\n","import os\n","from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n","from statsmodels.stats.contingency_tables import mcnemar\n","\n","# --- Configuration Variables ---\n","n_iterations = 1000  # Number of bootstrap resamples\n","alpha = 0.05         # Significance level for statistical tests (e.g., 0.05)\n","\n","# --- Define models and their predictions/probabilities ---\n","# These variables are assumed to be available from previous cells in the notebook.\n","# If any are missing, replace them with appropriate loaded/computed values.\n","models = {\n","    'LSTM-AE': {'probs': reconstruction_errors_test, 'preds': anomaly_predictions},\n","    'Logistic Regression': {'probs': best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': best_clf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'MLP': {'probs': mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten() > 0.5).astype(int)},\n","    'One-Class SVM': {'probs': -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten() < 0).astype(int)},\n","    'KNN': {'probs': knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': knn.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'XGBoost': {'probs': xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': xgb.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'Random Forest': {'probs': rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': rf.predict(reconstruction_errors_test.reshape(-1, 1))},\n","    'Averaged Ensemble': {'probs': averaged_probs, 'preds': ensemble_predictions},\n","    'Weighted Ensemble': {'probs': weighted_averaged_probs, 'preds': weighted_ensemble_predictions},\n","    'Stacked (LR Meta)': {'probs': stacked_predictions_probs, 'preds': stacked_predictions},\n","    'Stacked (XGB Meta)': {'probs': stacked_predictions_probs_xgb, 'preds': stacked_predictions_xgb},\n","}\n","\n","# Ensure y_test is a numpy array\n","y_test = np.array(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"115fc437"},"source":["**Reasoning**:\n","Now that the `models` dictionary and `y_test` are correctly defined, I will perform the bootstrap resampling process to generate the metric distributions for all models. Then, I will execute all specified pairwise statistical tests (Paired T-test, Wilcoxon test, Mann-Whitney U test, and McNemar test) between ensemble models and store their p-values in the `ensemble_statistical_test_results` dictionary.\n","\n"]},{"cell_type":"code","metadata":{"id":"131024aa"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","from typing import Dict, List, Any\n","import os\n","from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n","from statsmodels.stats.contingency_tables import mcnemar\n","\n","# Initialize dictionaries to store bootstrap results\n","metric_distributions: Dict[str, Dict[str, List[float]]] = {\n","    model_name: {'ROC AUC': [], 'PR AUC': [], 'F1 Score': [], 'bootstrap_samples_for_plotting': []}\n","    for model_name in models.keys()\n","}\n","\n","# Use a fixed random state for reproducibility of bootstrap samples themselves\n","bootstrap_rng = np.random.default_rng(seed=42)\n","\n","# Perform bootstrap resampling\n","print(f\"Performing bootstrap resampling ({n_iterations} iterations)...\")\n","\n","for i in range(n_iterations):\n","    # Resample the test set with replacement\n","    bootstrap_indices = bootstrap_rng.choice(len(y_test), size=len(y_test), replace=True)\n","    y_test_sample = y_test[bootstrap_indices]\n","\n","    for model_name, data in models.items():\n","        # Get probabilities and binary predictions for the bootstrap sample\n","        probs_sample = data['probs'][bootstrap_indices]\n","        preds_sample = data['preds'][bootstrap_indices] # Assuming binary predictions are available\n","\n","        # Calculate metrics on the bootstrap sample\n","        # ROC AUC\n","        # Check for multiple classes in true labels and variation in scores/predictions\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","            roc_auc = roc_auc_score(y_test_sample, probs_sample)\n","            metric_distributions[model_name]['ROC AUC'].append(roc_auc)\n","        else:\n","            # Handle cases where resampling results in only one class or constant probabilities\n","            metric_distributions[model_name]['ROC AUC'].append(np.nan) # Append NaN for invalid samples\n","\n","        # PR AUC\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","             pr_auc = average_precision_score(y_test_sample, probs_sample)\n","             metric_distributions[model_name]['PR AUC'].append(pr_auc)\n","        else:\n","             metric_distributions[model_name]['PR AUC'].append(np.nan)\n","\n","\n","        # F1 Score (requires binary predictions)\n","        # Check for multiple classes in true labels and variation in predictions\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(preds_sample)) > 1: # F1 is undefined for single class or constant predictions\n","             f1 = f1_score(y_test_sample, preds_sample)\n","             metric_distributions[model_name]['F1 Score'].append(f1)\n","        else:\n","             metric_distributions[model_name]['F1 Score'].append(np.nan)\n","\n","\n","        # Store samples for plotting ROC/PR curves with bands\n","        # Store a fixed number of samples (e.g., up to 100) for plotting efficiency\n","        if len(metric_distributions[model_name]['bootstrap_samples_for_plotting']) < 100:\n","             metric_distributions[model_name]['bootstrap_samples_for_plotting'].append((y_test_sample, probs_sample))\n","\n","\n","print(\"Bootstrap resampling complete.\")\n","\n","# Calculate summary statistics (mean, CI) from bootstrap distributions\n","metric_summary: Dict[str, Dict[str, Dict[str, Any]]] = {}\n","for model_name, metrics in metric_distributions.items():\n","    metric_summary[model_name] = {}\n","    for metric_name, distribution in metrics.items():\n","        if metric_name == 'bootstrap_samples_for_plotting':\n","            continue # Skip plotting samples\n","\n","        # Filter out NaN values before calculating mean and CI\n","        valid_distribution = np.array(distribution)[~np.isnan(distribution)]\n","\n","        if len(valid_distribution) > 0:\n","            mean = np.mean(valid_distribution)\n","            # Calculate confidence interval using percentile method\n","            # Need at least 2 points for percentiles\n","            if len(valid_distribution) >= 2:\n","                lower_ci = np.percentile(valid_distribution, (alpha / 2) * 100)\n","                upper_ci = np.percentile(valid_distribution, 100 - (alpha / 2) * 100)\n","            else:\n","                lower_ci = np.nan\n","                upper_ci = np.nan\n","\n","            metric_summary[model_name][metric_name] = {\n","                'mean': mean,\n","                'ci_lower': lower_ci,\n","                'ci_upper': upper_ci,\n","                'distribution': list(valid_distribution) # Store valid distribution for effect size calculation\n","            }\n","        else:\n","            metric_summary[model_name][metric_name] = {\n","                'mean': np.nan,\n","                'ci_lower': np.nan,\n","                'ci_upper': np.nan,\n","                'distribution': []\n","            }\n","\n","print(\"\\nSummary statistics calculated.\")\n","\n","# Define the ensemble models subset for pairwise comparison among themselves\n","# Assuming 'models' dictionary is already defined and contains ensemble models\n","ensemble_models = {\n","    model_name: data for model_name, data in models.items()\n","    if 'Ensemble' in model_name or 'Stacked' in model_name\n","}\n","\n","# Store statistical test results for ensemble comparisons\n","ensemble_statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n","\n","print(\"\\nPerforming Statistical Testing Among Ensemble Learners...\")\n","\n","ensemble_model_names = list(ensemble_models.keys())\n","\n","# Iterate through all unique pairs of ensemble models\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        model1_name = ensemble_model_names[i]\n","        model2_name = ensemble_model_names[j]\n","\n","        model1_data = ensemble_models[model1_name]\n","        model2_data = ensemble_models[model2_name]\n","\n","        # Retrieve bootstrap distributions\n","        model1_roc_auc_dist = np.array(metric_distributions[model1_name]['ROC AUC'], dtype=np.float64)\n","        model1_pr_auc_dist = np.array(metric_distributions[model1_name]['PR AUC'], dtype=np.float64)\n","        model1_f1_dist = np.array(metric_distributions[model1_name]['F1 Score'], dtype=np.float64)\n","\n","        model2_roc_auc_dist = np.array(metric_distributions[model2_name]['ROC AUC'], dtype=np.float64)\n","        model2_pr_auc_dist = np.array(metric_distributions[model2_name]['PR AUC'], dtype=np.float64)\n","        model2_f1_dist = np.array(metric_distributions[model2_name]['F1 Score'], dtype=np.float64)\n","\n","        # Retrieve original binary predictions for McNemar test\n","        model1_preds = model1_data['preds']\n","        model2_preds = model2_data['preds']\n","\n","\n","        comparison_key = f'{model1_name} vs {model2_name}'\n","        ensemble_statistical_test_results[comparison_key] = {}\n","\n","        # Perform paired tests on metric distributions (ROC AUC, PR AUC, F1 Score)\n","        for metric_name, dist1, dist2 in zip(\n","            ['ROC AUC', 'PR AUC', 'F1 Score'],\n","            [model1_roc_auc_dist, model1_pr_auc_dist, model1_f1_dist],\n","            [model2_roc_auc_dist, model2_pr_auc_dist, model2_f1_dist]\n","        ):\n","             min_len = min(len(dist1), len(dist2))\n","             if min_len >= 2:\n","                 # Paired t-test (handles NaNs)\n","                 t_stat, p_value_t = ttest_rel(dist1[:min_len], dist2[:min_len], nan_policy='omit')\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = p_value_t\n","\n","                 # Wilcoxon test (filter NaNs, require variation)\n","                 dist1_valid = dist1[:min_len][~np.isnan(dist1[:min_len])]\n","                 dist2_valid = dist2[:min_len][~np.isnan(dist2[:min_len])]\n","                 if len(dist1_valid) >= 2 and len(dist2_valid) >= 2 and len(set(dist1_valid)) > 1 and len(set(dist2_valid)) > 1:\n","                     try:\n","                         w_stat, p_value_w = wilcoxon(dist1_valid, dist2_valid)\n","                         ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = p_value_w\n","                     except ValueError:\n","                         ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                 else:\n","                     ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","\n","                 # Mann-Whitney U test (filter NaNs, require sufficient data)\n","                 if len(dist1_valid) >= 2 and len(dist2_valid) >= 2:\n","                     u_stat, p_value_u = mannwhitneyu(dist1_valid, dist2_valid)\n","                     ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = p_value_u\n","                 else:\n","                      ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n","\n","             else:\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = np.nan\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n","\n","\n","        # McNemar test on binary predictions (requires original predictions)\n","        try:\n","            # Ensure both prediction arrays have the same length as y_test\n","            if len(model1_preds) == len(y_test) and len(model2_preds) == len(y_test):\n","                # Create a contingency table for McNemar's test\n","                # (Model1 correct, Model2 incorrect), (Model1 incorrect, Model2 correct)\n","                n10 = np.sum((model1_preds == y_test) & (model2_preds != y_test))\n","                n01 = np.sum((model1_preds != y_test) & (model2_preds == y_test))\n","\n","                # The statsmodels mcnemar function uses the full 2x2 table\n","                contingency_table_mcnemar = pd.crosstab(model1_preds, model2_preds).reindex(\n","                    pd.MultiIndex.from_product([[0, 1], [0, 1]], names=[f'{model1_name} Pred', f'{model2_name} Pred']), fill_value=0)\n","\n","                table_array_mcnemar = np.array([\n","                    [contingency_table_mcnemar.loc[(0, 0)], contingency_table_mcnemar.loc[(0, 1)]],\n","                    [contingency_table_mcnemar.loc[(1, 0)], contingency_table_mcnemar.loc[(1, 1)]]\n","                ])\n","\n","                # Ensure there are discordant pairs for the test to be meaningful\n","                if n01 + n10 > 0:\n","                    mcnemar_result = mcnemar(table_array_mcnemar, exact=True) # Use exact=False for larger samples\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = mcnemar_result.pvalue\n","                else:\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = 1.0 # Perfect agreement on discordant pairs\n","\n","            else:\n","                 ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan # Length mismatch\n","\n","        except Exception as e:\n","             print(f\"Error performing McNemar test for {comparison_key}: {e}\")\n","             ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"20a45ae1"},"source":["**Reasoning**:\n","The previous attempt at McNemar's test produced an error due to improper construction of the contingency table. I will refine the McNemar test by explicitly calculating the counts for all four cells of the 2x2 contingency table using boolean logic on `model_preds` and `y_test`. This direct approach ensures that `statsmodels.stats.contingency_tables.mcnemar` receives a correctly formatted NumPy array of scalar counts, resolving the `TypeError` and allowing the test to run successfully.\n","\n"]},{"cell_type":"code","metadata":{"id":"5fd7d495"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","from typing import Dict, List, Any\n","import os\n","from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n","from statsmodels.stats.contingency_tables import mcnemar\n","\n","# Initialize dictionaries to store bootstrap results\n","metric_distributions: Dict[str, Dict[str, List[float]]] = {\n","    model_name: {'ROC AUC': [], 'PR AUC': [], 'F1 Score': [], 'bootstrap_samples_for_plotting': []}\n","    for model_name in models.keys()\n","}\n","\n","# Use a fixed random state for reproducibility of bootstrap samples themselves\n","bootstrap_rng = np.random.default_rng(seed=42)\n","\n","# Perform bootstrap resampling\n","print(f\"Performing bootstrap resampling ({n_iterations} iterations)...\")\n","\n","for i in range(n_iterations):\n","    # Resample the test set with replacement\n","    bootstrap_indices = bootstrap_rng.choice(len(y_test), size=len(y_test), replace=True)\n","    y_test_sample = y_test[bootstrap_indices]\n","\n","    for model_name, data in models.items():\n","        # Get probabilities and binary predictions for the bootstrap sample\n","        probs_sample = data['probs'][bootstrap_indices]\n","        preds_sample = data['preds'][bootstrap_indices] # Assuming binary predictions are available\n","\n","        # Calculate metrics on the bootstrap sample\n","        # ROC AUC\n","        # Check for multiple classes in true labels and variation in scores/predictions\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","            roc_auc = roc_auc_score(y_test_sample, probs_sample)\n","            metric_distributions[model_name]['ROC AUC'].append(roc_auc)\n","        else:\n","            # Handle cases where resampling results in only one class or constant probabilities\n","            metric_distributions[model_name]['ROC AUC'].append(np.nan) # Append NaN for invalid samples\n","\n","        # PR AUC\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n","             pr_auc = average_precision_score(y_test_sample, probs_sample)\n","             metric_distributions[model_name]['PR AUC'].append(pr_auc)\n","        else:\n","             metric_distributions[model_name]['PR AUC'].append(np.nan)\n","\n","\n","        # F1 Score (requires binary predictions)\n","        # Check for multiple classes in true labels and variation in predictions\n","        if len(np.unique(y_test_sample)) > 1 and len(np.unique(preds_sample)) > 1: # F1 is undefined for single class or constant predictions\n","             f1 = f1_score(y_test_sample, preds_sample)\n","             metric_distributions[model_name]['F1 Score'].append(f1)\n","        else:\n","             metric_distributions[model_name]['F1 Score'].append(np.nan)\n","\n","\n","        # Store samples for plotting ROC/PR curves with bands\n","        # Store a fixed number of samples (e.g., up to 100) for plotting efficiency\n","        if len(metric_distributions[model_name]['bootstrap_samples_for_plotting']) < 100:\n","             metric_distributions[model_name]['bootstrap_samples_for_plotting'].append((y_test_sample, probs_sample))\n","\n","\n","print(\"Bootstrap resampling complete.\")\n","\n","# Calculate summary statistics (mean, CI) from bootstrap distributions\n","metric_summary: Dict[str, Dict[str, Dict[str, Any]]] = {}\n","for model_name, metrics in metric_distributions.items():\n","    metric_summary[model_name] = {}\n","    for metric_name, distribution in metrics.items():\n","        if metric_name == 'bootstrap_samples_for_plotting':\n","            continue # Skip plotting samples\n","\n","        # Filter out NaN values before calculating mean and CI\n","        valid_distribution = np.array(distribution)[~np.isnan(distribution)]\n","\n","        if len(valid_distribution) > 0:\n","            mean = np.mean(valid_distribution)\n","            # Calculate confidence interval using percentile method\n","            # Need at least 2 points for percentiles\n","            if len(valid_distribution) >= 2:\n","                lower_ci = np.percentile(valid_distribution, (alpha / 2) * 100)\n","                upper_ci = np.percentile(valid_distribution, 100 - (alpha / 2) * 100)\n","            else:\n","                lower_ci = np.nan\n","                upper_ci = np.nan\n","\n","            metric_summary[model_name][metric_name] = {\n","                'mean': mean,\n","                'ci_lower': lower_ci,\n","                'ci_upper': upper_ci,\n","                'distribution': list(valid_distribution) # Store valid distribution for effect size calculation\n","            }\n","        else:\n","            metric_summary[model_name][metric_name] = {\n","                'mean': np.nan,\n","                'ci_lower': np.nan,\n","                'ci_upper': np.nan,\n","                'distribution': []\n","            }\n","\n","print(\"\\nSummary statistics calculated.\")\n","\n","# Define the ensemble models subset for pairwise comparison among themselves\n","# Assuming 'models' dictionary is already defined and contains ensemble models\n","ensemble_models = {\n","    model_name: data for model_name, data in models.items()\n","    if 'Ensemble' in model_name or 'Stacked' in model_name\n","}\n","\n","# Store statistical test results for ensemble comparisons\n","ensemble_statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n","\n","print(\"\\nPerforming Statistical Testing Among Ensemble Learners...\")\n","\n","ensemble_model_names = list(ensemble_models.keys())\n","\n","# Iterate through all unique pairs of ensemble models\n","for i in range(len(ensemble_model_names)):\n","    for j in range(i + 1, len(ensemble_model_names)):\n","        model1_name = ensemble_model_names[i]\n","        model2_name = ensemble_model_names[j]\n","\n","        model1_data = ensemble_models[model1_name]\n","        model2_data = ensemble_models[model2_name]\n","\n","        # Retrieve bootstrap distributions\n","        model1_roc_auc_dist = np.array(metric_distributions[model1_name]['ROC AUC'], dtype=np.float64)\n","        model1_pr_auc_dist = np.array(metric_distributions[model1_name]['PR AUC'], dtype=np.float64)\n","        model1_f1_dist = np.array(metric_distributions[model1_name]['F1 Score'], dtype=np.float64)\n","\n","        model2_roc_auc_dist = np.array(metric_distributions[model2_name]['ROC AUC'], dtype=np.float64)\n","        model2_pr_auc_dist = np.array(metric_distributions[model2_name]['PR AUC'], dtype=np.float64)\n","        model2_f1_dist = np.array(metric_distributions[model2_name]['F1 Score'], dtype=np.float64)\n","\n","        # Retrieve original binary predictions for McNemar test\n","        model1_preds = model1_data['preds']\n","        model2_preds = model2_data['preds']\n","\n","\n","        comparison_key = f'{model1_name} vs {model2_name}'\n","        ensemble_statistical_test_results[comparison_key] = {}\n","\n","        # Perform paired tests on metric distributions (ROC AUC, PR AUC, F1 Score)\n","        for metric_name, dist1, dist2 in zip(\n","            ['ROC AUC', 'PR AUC', 'F1 Score'],\n","            [model1_roc_auc_dist, model1_pr_auc_dist, model1_f1_dist],\n","            [model2_roc_auc_dist, model2_pr_auc_dist, model2_f1_dist]\n","        ):\n","             min_len = min(len(dist1), len(dist2))\n","             if min_len >= 2:\n","                 # Paired t-test (handles NaNs)\n","                 t_stat, p_value_t = ttest_rel(dist1[:min_len], dist2[:min_len], nan_policy='omit')\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = p_value_t\n","\n","                 # Wilcoxon test (filter NaNs, require variation)\n","                 dist1_valid = dist1[:min_len][~np.isnan(dist1[:min_len])]\n","                 dist2_valid = dist2[:min_len][~np.isnan(dist2[:min_len])]\n","                 if len(dist1_valid) >= 2 and len(dist2_valid) >= 2 and len(set(dist1_valid)) > 1 and len(set(dist2_valid)) > 1:\n","                     try:\n","                         w_stat, p_value_w = wilcoxon(dist1_valid, dist2_valid)\n","                         ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = p_value_w\n","                     except ValueError:\n","                         ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                 else:\n","                     ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","\n","                 # Mann-Whitney U test (filter NaNs, require sufficient data)\n","                 if len(dist1_valid) >= 2 and len(dist2_valid) >= 2:\n","                     u_stat, p_value_u = mannwhitneyu(dist1_valid, dist2_valid)\n","                     ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = p_value_u\n","                 else:\n","                      ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n","\n","             else:\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = np.nan\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n","                 ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n","\n","\n","        # McNemar test on binary predictions (requires original predictions)\n","        try:\n","            # Ensure both prediction arrays have the same length as y_test\n","            if len(model1_preds) == len(y_test) and len(model2_preds) == len(y_test):\n","                correct_m1 = (model1_preds == y_test)\n","                correct_m2 = (model2_preds == y_test)\n","\n","                n_both_correct = np.sum(correct_m1 & correct_m2)\n","                n_m1_correct_m2_incorrect = np.sum(correct_m1 & ~correct_m2)\n","                n_m1_incorrect_m2_correct = np.sum(~correct_m1 & correct_m2)\n","                n_both_incorrect = np.sum(~correct_m1 & ~correct_m2)\n","\n","                mcnemar_table = np.array([\n","                    [n_both_correct, n_m1_correct_m2_incorrect],\n","                    [n_m1_incorrect_m2_correct, n_both_incorrect]\n","                ])\n","\n","                # Ensure there are discordant pairs for the test to be meaningful\n","                if n_m1_correct_m2_incorrect + n_m1_incorrect_m2_correct > 0:\n","                    mcnemar_result = mcnemar(mcnemar_table, exact=True)\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = mcnemar_result.pvalue\n","                else:\n","                    ensemble_statistical_test_results[comparison_key]['McNemar p'] = 1.0 # Perfect agreement on discordant pairs\n","\n","            else:\n","                 ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan # Length mismatch\n","\n","        except Exception as e:\n","             print(f\"Error performing McNemar test for {comparison_key}: {e}\")\n","             ensemble_statistical_test_results[comparison_key]['McNemar p'] = np.nan"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90524006"},"source":["**Reasoning**:\n","I need to apply Holm-Bonferroni correction to all generated p-values from the ensemble statistical tests. This requires extracting all p-values, applying the correction using `statsmodels.stats.multitest.multipletests`, and then updating the `ensemble_statistical_test_results` dictionary with these corrected values. Finally, I will display the updated comparison table with corrected p-values and significance flags, and save it in CSV and LaTeX formats.\n","\n"]},{"cell_type":"code","metadata":{"id":"be649f40"},"source":["import numpy as np\n","import pandas as pd\n","from statsmodels.stats.multitest import multipletests\n","\n","# --- Step 1: Extract all p-values for correction ---\n","all_p_values = []\n","p_value_keys = []\n","\n","for comparison_key, tests in ensemble_statistical_test_results.items():\n","    for test_name, p_value in tests.items():\n","        if p_value is not None and not np.isnan(p_value):\n","            all_p_values.append(p_value)\n","            p_value_keys.append((comparison_key, test_name))\n","\n","# --- Step 2: Apply Holm-Bonferroni correction ---\n","if all_p_values:\n","    corrected_p_values_tuple = multipletests(all_p_values, alpha=alpha, method='holm')\n","    corrected_p_values = corrected_p_values_tuple[1] # p-value adjusted\n","    corrected_significant = corrected_p_values_tuple[0] # reject_null\n","else:\n","    corrected_p_values = []\n","    corrected_significant = []\n","\n","# --- Step 3: Update ensemble_statistical_test_results with corrected p-values ---\n","corrected_p_value_map = {}\n","for i, (comparison_key, test_name) in enumerate(p_value_keys):\n","    corrected_p_value_map[(comparison_key, test_name)] = {\n","        'p_value_corrected': corrected_p_values[i],\n","        'significant_corrected': corrected_significant[i]\n","    }\n","\n","# Recreate the comparison data with corrected p-values\n","print(\"\\nStatistical Test Results Among Ensemble Learners (Holm-Bonferroni Corrected):\")\n","\n","ensemble_comparison_data_corrected = []\n","# Define the order of tests for consistent columns\n","test_keys_order = [\n","    'ROC AUC (T-test p)', 'ROC AUC (Wilcoxon p)', 'ROC AUC (Mann-Whitney p)',\n","    'PR AUC (T-test p)', 'PR AUC (Wilcoxon p)', 'PR AUC (Mann-Whitney p)',\n","    'F1 Score (T-test p)', 'F1 Score (Wilcoxon p)', 'F1 Score (Mann-Whitney p)',\n","    'McNemar p'\n","]\n","\n","for comparison, tests in ensemble_statistical_test_results.items():\n","    row = {'Comparison': comparison}\n","    for test_name_key in test_keys_order:\n","        # Get original p-value\n","        original_p_value = tests.get(test_name_key)\n","\n","        # Get corrected p-value and significance\n","        corrected_info = corrected_p_value_map.get((comparison, test_name_key))\n","\n","        if corrected_info:\n","            p_value_corrected = corrected_info['p_value_corrected']\n","            significant_corrected = corrected_info['significant_corrected']\n","            row[test_name_key] = f\"{p_value_corrected:.4f}\"\n","            row[f'Significant? ({test_name_key})'] = '*' if significant_corrected else ''\n","        elif original_p_value is not None and not np.isnan(original_p_value):\n","            row[test_name_key] = \"N/A\" # No corrected info, but original existed\n","            row[f'Significant? ({test_name_key})'] = ''\n","        else:\n","            row[test_name_key] = \"N/A\" # Original p-value was also N/A or NaN\n","            row[f'Significant? ({test_name_key})'] = ''\n","\n","    ensemble_comparison_data_corrected.append(row)\n","\n","ensemble_comparison_df_corrected = pd.DataFrame(ensemble_comparison_data_corrected)\n","\n","# Filter columns to only include those that were actually added\n","valid_ensemble_columns_corrected = ['Comparison']\n","for test_name_key in test_keys_order:\n","    if test_name_key in ensemble_comparison_df_corrected.columns:\n","        valid_ensemble_columns_corrected.append(test_name_key)\n","        valid_ensemble_columns_corrected.append(f'Significant? ({test_name_key})')\n","\n","ensemble_comparison_df_corrected = ensemble_comparison_df_corrected[valid_ensemble_columns_corrected]\n","\n","# Display the table\n","print(ensemble_comparison_df_corrected.to_string())\n","\n","# Save the detailed ensemble performance summary table\n","output_dir_tables = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n","os.makedirs(output_dir_tables, exist_ok=True)\n","ensemble_comparison_df_corrected.to_csv(f\"{output_dir_tables}/ensemble_statistical_comparison_corrected.csv\", index=False)\n","\n","# Prepare for LaTeX: Escape special characters\n","latex_ensemble_df_corrected = ensemble_comparison_df_corrected.copy()\n","latex_ensemble_df_corrected.columns = latex_ensemble_df_corrected.columns.str.replace('_', '\\\\_')\n","latex_ensemble_df_corrected.columns = latex_ensemble_df_corrected.columns.str.replace('%', '\\\\%')\n","latex_ensemble_df_corrected.columns = latex_ensemble_df_corrected.columns.str.replace('(', '{(', regex=False).str.replace(')', ')}')\n","\n","latex_ensemble_df_corrected.to_latex(f\"{output_dir_tables}/ensemble_statistical_comparison_corrected.tex\", index=False, float_format=\"%.4f\", escape=False)\n","\n","print(f\"\\nEnsemble statistical comparison corrected table saved to {output_dir_tables}/ensemble_statistical_comparison_corrected.csv and .tex\")"],"execution_count":null,"outputs":[]}]}